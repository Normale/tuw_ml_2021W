30 operators have been imported by TPOT.
Optimization Progress:   0%|          | 0/2550 [00:00<?, ?pipeline/s]Optimization Progress:   0%|          | 4/2550 [00:17<3:10:08,  4.48s/pipeline]Optimization Progress:   1%|          | 20/2550 [00:34<2:24:59,  3.44s/pipeline]Optimization Progress:   1%|▏         | 36/2550 [00:49<1:53:05,  2.70s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..
Optimization Progress:   2%|▏         | 50/2550 [00:51<1:52:28,  2.70s/pipeline]Optimization Progress:   2%|▏         | 50/2550 [00:51<1:20:04,  1.92s/pipeline]Optimization Progress:   2%|▏         | 51/2550 [00:53<1:20:43,  1.94s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:   2%|▏         | 51/2550 [00:53<1:20:43,  1.94s/pipeline]Optimization Progress:   2%|▏         | 53/2550 [01:23<4:06:04,  5.91s/pipeline]Optimization Progress:   3%|▎         | 69/2550 [01:42<3:05:41,  4.49s/pipeline]Optimization Progress:   3%|▎         | 85/2550 [02:01<2:24:13,  3.51s/pipeline]                                                                                
Generation 1 - Current Pareto front scores:
Optimization Progress:   4%|▍         | 100/2550 [02:01<2:23:21,  3.51s/pipeline]                                                                                 
-1	-0.5757905293022872	KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=28, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)
Optimization Progress:   4%|▍         | 100/2550 [02:01<2:23:21,  3.51s/pipeline]                                                                                 
-2	-0.5741355582130674	RandomForestRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.7000000000000001, RandomForestRegressor__min_samples_leaf=8, RandomForestRegressor__min_samples_split=2, RandomForestRegressor__n_estimators=100)
Optimization Progress:   4%|▍         | 100/2550 [02:01<2:23:21,  3.51s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:   4%|▍         | 100/2550 [02:02<2:23:21,  3.51s/pipeline]Optimization Progress:   4%|▍         | 100/2550 [02:02<1:41:06,  2.48s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:   4%|▍         | 100/2550 [02:02<1:41:06,  2.48s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.
Optimization Progress:   4%|▍         | 100/2550 [02:03<1:41:06,  2.48s/pipeline]Optimization Progress:   4%|▍         | 102/2550 [02:07<1:39:50,  2.45s/pipeline]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:   4%|▍         | 102/2550 [02:07<1:39:50,  2.45s/pipeline]Optimization Progress:   4%|▍         | 104/2550 [02:29<3:22:38,  4.97s/pipeline]Optimization Progress:   5%|▍         | 120/2550 [03:51<3:23:03,  5.01s/pipeline]Optimization Progress:   5%|▌         | 136/2550 [04:16<2:40:03,  3.98s/pipeline]                                                                                 
Generation 2 - Current Pareto front scores:
Optimization Progress:   6%|▌         | 150/2550 [04:16<2:39:07,  3.98s/pipeline]                                                                                 
-1	-0.5736142451336099	KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=64, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)
Optimization Progress:   6%|▌         | 150/2550 [04:16<2:39:07,  3.98s/pipeline]                                                                                 
-2	-0.5724037733233002	LassoLarsCV(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=11, ExtraTreesRegressor__n_estimators=100), LassoLarsCV__normalize=False)
Optimization Progress:   6%|▌         | 150/2550 [04:16<2:39:07,  3.98s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:   6%|▌         | 150/2550 [04:17<2:39:07,  3.98s/pipeline]Optimization Progress:   6%|▌         | 150/2550 [04:17<1:53:04,  2.83s/pipeline]Optimization Progress:   6%|▌         | 151/2550 [04:22<2:11:22,  3.29s/pipeline]Optimization Progress:   6%|▌         | 152/2550 [05:54<19:54:55, 29.90s/pipeline]Optimization Progress:   7%|▋         | 168/2550 [06:26<14:14:38, 21.53s/pipeline]Optimization Progress:   7%|▋         | 184/2550 [06:57<10:17:37, 15.66s/pipeline]Optimization Progress:   8%|▊         | 200/2550 [06:59<7:10:52, 11.00s/pipeline]                                                                                  
Generation 3 - Current Pareto front scores:
Optimization Progress:   8%|▊         | 200/2550 [06:59<7:10:52, 11.00s/pipeline]                                                                                 
-1	-0.5736142451336099	KNeighborsRegressor(input_matrix, KNeighborsRegressor__n_neighbors=64, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)
Optimization Progress:   8%|▊         | 200/2550 [06:59<7:10:52, 11.00s/pipeline]                                                                                 
-2	-0.5510396212269953	KNeighborsRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=47, KNeighborsRegressor__p=1, KNeighborsRegressor__weights=distance)
Optimization Progress:   8%|▊         | 200/2550 [06:59<7:10:52, 11.00s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:   8%|▊         | 200/2550 [07:01<7:10:52, 11.00s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.
Optimization Progress:   8%|▊         | 200/2550 [07:03<7:10:52, 11.00s/pipeline]Optimization Progress:   8%|▊         | 201/2550 [08:40<24:47:28, 37.99s/pipeline]Optimization Progress:   9%|▊         | 217/2550 [09:43<17:59:33, 27.76s/pipeline]Optimization Progress:   9%|▉         | 233/2550 [11:36<13:52:42, 21.56s/pipeline]Optimization Progress:  10%|▉         | 249/2550 [11:47<9:46:53, 15.30s/pipeline]                                                                                  
Generation 4 - Current Pareto front scores:
Optimization Progress:  10%|▉         | 250/2550 [11:47<9:46:38, 15.30s/pipeline]                                                                                 
-1	-0.5633930000769449	ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  10%|▉         | 250/2550 [11:47<9:46:38, 15.30s/pipeline]                                                                                 
-2	-0.550023639246366	KNeighborsRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=47, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=distance)
Optimization Progress:  10%|▉         | 250/2550 [11:47<9:46:38, 15.30s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:  10%|▉         | 250/2550 [11:50<9:46:38, 15.30s/pipeline]Optimization Progress:  10%|▉         | 250/2550 [11:50<7:22:08, 11.53s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  10%|▉         | 250/2550 [11:50<7:22:08, 11.53s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.
Optimization Progress:  10%|▉         | 250/2550 [11:53<7:22:08, 11.53s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress:  10%|▉         | 250/2550 [11:53<7:22:08, 11.53s/pipeline]Optimization Progress:  10%|▉         | 252/2550 [11:53<5:26:53,  8.54s/pipeline]Optimization Progress:  10%|▉         | 253/2550 [12:30<10:52:07, 17.03s/pipeline]Optimization Progress:  11%|█         | 269/2550 [13:00<7:54:57, 12.49s/pipeline] Optimization Progress:  11%|█         | 285/2550 [14:40<6:40:25, 10.61s/pipeline]                                                                                 
Generation 5 - Current Pareto front scores:
Optimization Progress:  12%|█▏        | 300/2550 [14:40<6:37:46, 10.61s/pipeline]                                                                                 
-1	-0.5532535523702826	ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  12%|█▏        | 300/2550 [14:40<6:37:46, 10.61s/pipeline]                                                                                 
-2	-0.550023639246366	KNeighborsRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=47, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=distance)
Optimization Progress:  12%|█▏        | 300/2550 [14:40<6:37:46, 10.61s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  12%|█▏        | 300/2550 [14:43<6:37:46, 10.61s/pipeline]Optimization Progress:  12%|█▏        | 300/2550 [14:43<4:40:44,  7.49s/pipeline]Optimization Progress:  12%|█▏        | 302/2550 [14:47<3:39:03,  5.85s/pipeline]Optimization Progress:  12%|█▏        | 303/2550 [16:31<22:03:50, 35.35s/pipeline]Optimization Progress:  13%|█▎        | 319/2550 [17:41<16:08:34, 26.05s/pipeline]Optimization Progress:  13%|█▎        | 335/2550 [18:20<11:40:19, 18.97s/pipeline]                                                                                  
Generation 6 - Current Pareto front scores:
Optimization Progress:  14%|█▎        | 350/2550 [18:20<11:35:34, 18.97s/pipeline]                                                                                  
-1	-0.5532535523702826	ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  14%|█▎        | 350/2550 [18:20<11:35:34, 18.97s/pipeline]                                                                                  
-2	-0.550023639246366	KNeighborsRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=47, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=distance)
Optimization Progress:  14%|█▎        | 350/2550 [18:20<11:35:34, 18.97s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  14%|█▎        | 350/2550 [18:24<11:35:34, 18.97s/pipeline]Optimization Progress:  14%|█▎        | 350/2550 [18:24<8:10:04, 13.37s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.
Optimization Progress:  14%|█▎        | 350/2550 [18:26<8:10:04, 13.37s/pipeline]Optimization Progress:  14%|█▍        | 351/2550 [18:26<6:06:21, 10.00s/pipeline]Optimization Progress:  14%|█▍        | 352/2550 [19:35<16:47:51, 27.51s/pipeline]Optimization Progress:  14%|█▍        | 368/2550 [19:58<11:56:19, 19.70s/pipeline]Optimization Progress:  15%|█▌        | 384/2550 [20:23<8:34:56, 14.26s/pipeline] Optimization Progress:  16%|█▌        | 400/2550 [20:28<6:00:44, 10.07s/pipeline]                                                                                 
Generation 7 - Current Pareto front scores:
Optimization Progress:  16%|█▌        | 400/2550 [20:28<6:00:44, 10.07s/pipeline]                                                                                 
-1	-0.5532535523702826	ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  16%|█▌        | 400/2550 [20:28<6:00:44, 10.07s/pipeline]                                                                                 
-2	-0.5481438987948792	ExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  16%|█▌        | 400/2550 [20:28<6:00:44, 10.07s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:  16%|█▌        | 400/2550 [20:29<6:00:44, 10.07s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  16%|█▌        | 400/2550 [20:30<6:00:44, 10.07s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.
Optimization Progress:  16%|█▌        | 400/2550 [20:32<6:00:44, 10.07s/pipeline]Optimization Progress:  16%|█▌        | 401/2550 [20:33<5:06:14,  8.55s/pipeline]Optimization Progress:  16%|█▌        | 402/2550 [20:49<6:26:55, 10.81s/pipeline]Optimization Progress:  16%|█▋        | 418/2550 [21:11<4:43:46,  7.99s/pipeline]Optimization Progress:  17%|█▋        | 434/2550 [21:42<3:37:24,  6.16s/pipeline]Optimization Progress:  18%|█▊        | 450/2550 [21:51<2:36:37,  4.48s/pipeline]                                                                                 
Generation 8 - Current Pareto front scores:
Optimization Progress:  18%|█▊        | 450/2550 [21:51<2:36:37,  4.48s/pipeline]                                                                                 
-1	-0.5532123180921728	ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  18%|█▊        | 450/2550 [21:51<2:36:37,  4.48s/pipeline]                                                                                 
-2	-0.5481438987948792	ExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  18%|█▊        | 450/2550 [21:51<2:36:37,  4.48s/pipeline]Optimization Progress:  18%|█▊        | 451/2550 [21:55<2:39:15,  4.55s/pipeline]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  18%|█▊        | 451/2550 [21:55<2:39:15,  4.55s/pipeline]Optimization Progress:  18%|█▊        | 453/2550 [22:39<5:41:04,  9.76s/pipeline]Optimization Progress:  18%|█▊        | 469/2550 [23:01<4:11:11,  7.24s/pipeline]Optimization Progress:  19%|█▉        | 485/2550 [23:19<3:06:22,  5.42s/pipeline]                                                                                 
Generation 9 - Current Pareto front scores:
Optimization Progress:  20%|█▉        | 500/2550 [23:19<3:05:01,  5.42s/pipeline]                                                                                 
-1	-0.5532123180921728	ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  20%|█▉        | 500/2550 [23:19<3:05:01,  5.42s/pipeline]                                                                                 
-2	-0.5475946594199389	KNeighborsRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=77, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=distance)
Optimization Progress:  20%|█▉        | 500/2550 [23:19<3:05:01,  5.42s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.
Optimization Progress:  20%|█▉        | 500/2550 [23:20<3:05:01,  5.42s/pipeline]Optimization Progress:  20%|█▉        | 500/2550 [23:20<2:10:01,  3.81s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  20%|█▉        | 500/2550 [23:21<2:10:01,  3.81s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.
Optimization Progress:  20%|█▉        | 500/2550 [23:22<2:10:01,  3.81s/pipeline]Optimization Progress:  20%|█▉        | 501/2550 [23:46<6:00:34, 10.56s/pipeline]Optimization Progress:  20%|██        | 517/2550 [24:09<4:24:56,  7.82s/pipeline]Optimization Progress:  21%|██        | 533/2550 [24:36<3:20:36,  5.97s/pipeline]Optimization Progress:  22%|██▏       | 549/2550 [24:40<2:21:59,  4.26s/pipeline]                                                                                 
Generation 10 - Current Pareto front scores:
Optimization Progress:  22%|██▏       | 550/2550 [24:40<2:21:55,  4.26s/pipeline]                                                                                 
-1	-0.5532123180921728	ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.9500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  22%|██▏       | 550/2550 [24:40<2:21:55,  4.26s/pipeline]                                                                                 
-2	-0.5475946594199389	KNeighborsRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=77, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=distance)
Optimization Progress:  22%|██▏       | 550/2550 [24:40<2:21:55,  4.26s/pipeline]Optimization Progress:  22%|██▏       | 550/2550 [24:45<2:29:34,  4.49s/pipeline]Optimization Progress:  22%|██▏       | 551/2550 [25:27<8:41:16, 15.65s/pipeline]Optimization Progress:  22%|██▏       | 567/2550 [25:49<6:16:05, 11.38s/pipeline]Optimization Progress:  23%|██▎       | 583/2550 [26:12<4:34:50,  8.38s/pipeline]Optimization Progress:  23%|██▎       | 599/2550 [26:18<3:14:47,  5.99s/pipeline]                                                                                 
Generation 11 - Current Pareto front scores:
Optimization Progress:  24%|██▎       | 600/2550 [26:18<3:14:41,  5.99s/pipeline]                                                                                 
-1	-0.5530279047542866	ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  24%|██▎       | 600/2550 [26:18<3:14:41,  5.99s/pipeline]                                                                                 
-2	-0.5475946594199389	KNeighborsRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=77, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=distance)
Optimization Progress:  24%|██▎       | 600/2550 [26:18<3:14:41,  5.99s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.
Optimization Progress:  24%|██▎       | 600/2550 [26:19<3:14:41,  5.99s/pipeline]Optimization Progress:  24%|██▎       | 600/2550 [26:19<2:19:31,  4.29s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 57.
Optimization Progress:  24%|██▎       | 600/2550 [26:20<2:19:31,  4.29s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  24%|██▎       | 600/2550 [26:20<2:19:31,  4.29s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.
Optimization Progress:  24%|██▎       | 600/2550 [26:22<2:19:31,  4.29s/pipeline]Optimization Progress:  24%|██▎       | 601/2550 [26:23<2:21:52,  4.37s/pipeline]Optimization Progress:  24%|██▎       | 602/2550 [26:39<4:18:26,  7.96s/pipeline]Optimization Progress:  24%|██▍       | 618/2550 [27:01<3:12:20,  5.97s/pipeline]Optimization Progress:  25%|██▍       | 634/2550 [27:28<2:29:47,  4.69s/pipeline]Optimization Progress:  25%|██▌       | 650/2550 [27:33<1:46:44,  3.37s/pipeline]                                                                                 
Generation 12 - Current Pareto front scores:
Optimization Progress:  25%|██▌       | 650/2550 [27:33<1:46:44,  3.37s/pipeline]                                                                                 
-1	-0.5530279047542866	ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  25%|██▌       | 650/2550 [27:33<1:46:44,  3.37s/pipeline]                                                                                 
-2	-0.5475946594199389	KNeighborsRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=77, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=distance)
Optimization Progress:  25%|██▌       | 650/2550 [27:33<1:46:44,  3.37s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.
Optimization Progress:  25%|██▌       | 650/2550 [27:34<1:46:44,  3.37s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  25%|██▌       | 650/2550 [27:36<1:46:44,  3.37s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  25%|██▌       | 650/2550 [27:38<1:46:44,  3.37s/pipeline]Optimization Progress:  26%|██▌       | 651/2550 [27:38<2:03:18,  3.90s/pipeline]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  26%|██▌       | 651/2550 [27:38<2:03:18,  3.90s/pipeline]Optimization Progress:  26%|██▌       | 653/2550 [28:01<3:15:11,  6.17s/pipeline]Optimization Progress:  26%|██▌       | 669/2550 [28:31<2:33:27,  4.89s/pipeline]Optimization Progress:  27%|██▋       | 685/2550 [28:52<1:58:38,  3.82s/pipeline]                                                                                 
Generation 13 - Current Pareto front scores:
Optimization Progress:  27%|██▋       | 700/2550 [28:52<1:57:41,  3.82s/pipeline]                                                                                 
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  27%|██▋       | 700/2550 [28:52<1:57:41,  3.82s/pipeline]                                                                                 
-2	-0.5475946594199389	KNeighborsRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=77, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=distance)
Optimization Progress:  27%|██▋       | 700/2550 [28:52<1:57:41,  3.82s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  27%|██▋       | 700/2550 [28:54<1:57:41,  3.82s/pipeline]Optimization Progress:  27%|██▋       | 700/2550 [28:54<1:23:35,  2.71s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  27%|██▋       | 700/2550 [28:57<1:23:35,  2.71s/pipeline]Optimization Progress:  28%|██▊       | 703/2550 [28:57<1:07:26,  2.19s/pipeline]Optimization Progress:  28%|██▊       | 704/2550 [29:40<7:26:58, 14.53s/pipeline]Optimization Progress:  28%|██▊       | 720/2550 [29:55<5:18:34, 10.45s/pipeline]Optimization Progress:  29%|██▉       | 736/2550 [30:22<3:56:23,  7.82s/pipeline]                                                                                 
Generation 14 - Current Pareto front scores:
Optimization Progress:  29%|██▉       | 750/2550 [30:22<3:54:33,  7.82s/pipeline]                                                                                 
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  29%|██▉       | 750/2550 [30:22<3:54:33,  7.82s/pipeline]                                                                                 
-2	-0.5475946594199389	KNeighborsRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=77, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=distance)
Optimization Progress:  29%|██▉       | 750/2550 [30:22<3:54:33,  7.82s/pipeline]Optimization Progress:  29%|██▉       | 750/2550 [30:27<2:47:31,  5.58s/pipeline]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  29%|██▉       | 750/2550 [30:27<2:47:31,  5.58s/pipeline]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  29%|██▉       | 751/2550 [30:27<2:47:26,  5.58s/pipeline]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  29%|██▉       | 752/2550 [30:27<2:47:20,  5.58s/pipeline]Optimization Progress:  30%|██▉       | 754/2550 [30:49<2:44:37,  5.50s/pipeline]Optimization Progress:  30%|███       | 770/2550 [31:21<2:12:15,  4.46s/pipeline]Optimization Progress:  31%|███       | 786/2550 [31:44<1:44:34,  3.56s/pipeline]                                                                                 
Generation 15 - Current Pareto front scores:
Optimization Progress:  31%|███▏      | 800/2550 [31:44<1:43:45,  3.56s/pipeline]                                                                                 
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  31%|███▏      | 800/2550 [31:44<1:43:45,  3.56s/pipeline]                                                                                 
-2	-0.5475946594199389	KNeighborsRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=4, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100), KNeighborsRegressor__n_neighbors=77, KNeighborsRegressor__p=2, KNeighborsRegressor__weights=distance)
Optimization Progress:  31%|███▏      | 800/2550 [31:44<1:43:45,  3.56s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  31%|███▏      | 800/2550 [31:45<1:43:45,  3.56s/pipeline]Optimization Progress:  31%|███▏      | 800/2550 [31:45<1:12:49,  2.50s/pipeline]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  31%|███▏      | 800/2550 [31:49<1:12:49,  2.50s/pipeline]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  31%|███▏      | 801/2550 [31:49<1:12:46,  2.50s/pipeline]Optimization Progress:  31%|███▏      | 802/2550 [31:49<1:10:41,  2.43s/pipeline]Optimization Progress:  31%|███▏      | 802/2550 [32:00<1:10:41,  2.43s/pipeline]Optimization Progress:  31%|███▏      | 803/2550 [32:12<4:11:30,  8.64s/pipeline]Optimization Progress:  32%|███▏      | 819/2550 [32:30<3:03:55,  6.38s/pipeline]Optimization Progress:  33%|███▎      | 835/2550 [32:53<2:20:04,  4.90s/pipeline]                                                                                 
Generation 16 - Current Pareto front scores:
Optimization Progress:  33%|███▎      | 850/2550 [32:53<2:18:50,  4.90s/pipeline]                                                                                 
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  33%|███▎      | 850/2550 [32:53<2:18:50,  4.90s/pipeline]                                                                                 
-2	-0.5468623068654151	ExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  33%|███▎      | 850/2550 [32:53<2:18:50,  4.90s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  33%|███▎      | 850/2550 [32:53<2:18:50,  4.90s/pipeline]Optimization Progress:  33%|███▎      | 850/2550 [32:53<1:37:18,  3.43s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  33%|███▎      | 850/2550 [32:55<1:37:18,  3.43s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:  33%|███▎      | 850/2550 [32:56<1:37:18,  3.43s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.
Optimization Progress:  33%|███▎      | 850/2550 [32:56<1:37:18,  3.43s/pipeline]Optimization Progress:  33%|███▎      | 850/2550 [33:10<1:37:18,  3.43s/pipeline]Optimization Progress:  33%|███▎      | 851/2550 [33:18<4:38:43,  9.84s/pipeline]Optimization Progress:  34%|███▍      | 867/2550 [33:36<3:22:38,  7.22s/pipeline]Optimization Progress:  35%|███▍      | 883/2550 [34:36<2:51:38,  6.18s/pipeline]Optimization Progress:  35%|███▌      | 899/2550 [34:44<2:03:09,  4.48s/pipeline]                                                                                 
Generation 17 - Current Pareto front scores:
Optimization Progress:  35%|███▌      | 900/2550 [34:44<2:03:04,  4.48s/pipeline]                                                                                 
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  35%|███▌      | 900/2550 [34:44<2:03:04,  4.48s/pipeline]                                                                                 
-2	-0.5468623068654151	ExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  35%|███▌      | 900/2550 [34:44<2:03:04,  4.48s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:  35%|███▌      | 900/2550 [34:44<2:03:04,  4.48s/pipeline]Optimization Progress:  35%|███▌      | 900/2550 [34:44<1:31:42,  3.33s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95.
Optimization Progress:  35%|███▌      | 900/2550 [34:45<1:31:42,  3.33s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  35%|███▌      | 900/2550 [34:45<1:31:42,  3.33s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  35%|███▌      | 900/2550 [34:45<1:31:42,  3.33s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.
Optimization Progress:  35%|███▌      | 900/2550 [34:47<1:31:42,  3.33s/pipeline]Optimization Progress:  35%|███▌      | 901/2550 [34:48<1:32:12,  3.36s/pipeline]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  35%|███▌      | 901/2550 [34:48<1:32:12,  3.36s/pipeline]Optimization Progress:  35%|███▌      | 903/2550 [35:18<3:10:29,  6.94s/pipeline]Optimization Progress:  36%|███▌      | 919/2550 [35:43<2:24:48,  5.33s/pipeline]Optimization Progress:  37%|███▋      | 935/2550 [36:09<1:53:12,  4.21s/pipeline]                                                                                 
Generation 18 - Current Pareto front scores:
Optimization Progress:  37%|███▋      | 950/2550 [36:09<1:52:09,  4.21s/pipeline]                                                                                 
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  37%|███▋      | 950/2550 [36:09<1:52:09,  4.21s/pipeline]                                                                                 
-2	-0.5468623068654151	ExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  37%|███▋      | 950/2550 [36:09<1:52:09,  4.21s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  37%|███▋      | 950/2550 [36:09<1:52:09,  4.21s/pipeline]Optimization Progress:  37%|███▋      | 950/2550 [36:09<1:18:34,  2.95s/pipeline]Optimization Progress:  37%|███▋      | 950/2550 [36:20<1:18:34,  2.95s/pipeline]Optimization Progress:  37%|███▋      | 951/2550 [36:44<5:35:27, 12.59s/pipeline]Optimization Progress:  38%|███▊      | 967/2550 [37:00<4:00:20,  9.11s/pipeline]Optimization Progress:  39%|███▊      | 983/2550 [37:25<2:58:37,  6.84s/pipeline]Optimization Progress:  39%|███▉      | 999/2550 [37:29<2:06:02,  4.88s/pipeline]                                                                                 
Generation 19 - Current Pareto front scores:
Optimization Progress:  39%|███▉      | 1000/2550 [37:29<2:05:57,  4.88s/pipeline]                                                                                  
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  39%|███▉      | 1000/2550 [37:29<2:05:57,  4.88s/pipeline]                                                                                  
-2	-0.5467240471066791	ExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  39%|███▉      | 1000/2550 [37:29<2:05:57,  4.88s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  39%|███▉      | 1000/2550 [37:30<2:05:57,  4.88s/pipeline]Optimization Progress:  39%|███▉      | 1000/2550 [37:30<1:31:52,  3.56s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress:  39%|███▉      | 1000/2550 [37:31<1:31:52,  3.56s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  39%|███▉      | 1000/2550 [37:32<1:31:52,  3.56s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  39%|███▉      | 1000/2550 [37:32<1:31:52,  3.56s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100.
Optimization Progress:  39%|███▉      | 1000/2550 [37:33<1:31:52,  3.56s/pipeline]Optimization Progress:  39%|███▉      | 1001/2550 [37:34<1:32:15,  3.57s/pipeline]Optimization Progress:  39%|███▉      | 1002/2550 [38:04<4:59:45, 11.62s/pipeline]Optimization Progress:  40%|███▉      | 1018/2550 [38:31<3:40:25,  8.63s/pipeline]Optimization Progress:  41%|████      | 1034/2550 [38:50<2:41:41,  6.40s/pipeline]Optimization Progress:  41%|████      | 1050/2550 [38:52<1:53:15,  4.53s/pipeline]                                                                                  
Generation 20 - Current Pareto front scores:
Optimization Progress:  41%|████      | 1050/2550 [38:52<1:53:15,  4.53s/pipeline]                                                                                  
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  41%|████      | 1050/2550 [38:52<1:53:15,  4.53s/pipeline]                                                                                  
-2	-0.5467240471066791	ExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  41%|████      | 1050/2550 [38:52<1:53:15,  4.53s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.
Optimization Progress:  41%|████      | 1050/2550 [38:53<1:53:15,  4.53s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  41%|████      | 1050/2550 [38:54<1:53:15,  4.53s/pipeline]Optimization Progress:  41%|████▏     | 1052/2550 [38:57<1:35:28,  3.82s/pipeline]Optimization Progress:  41%|████▏     | 1053/2550 [39:34<5:46:34, 13.89s/pipeline]Optimization Progress:  42%|████▏     | 1069/2550 [39:54<4:09:02, 10.09s/pipeline]Optimization Progress:  43%|████▎     | 1085/2550 [40:14<3:01:56,  7.45s/pipeline]                                                                                  
Generation 21 - Current Pareto front scores:
Optimization Progress:  43%|████▎     | 1100/2550 [40:14<3:00:04,  7.45s/pipeline]                                                                                  
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  43%|████▎     | 1100/2550 [40:14<3:00:04,  7.45s/pipeline]                                                                                  
-2	-0.546250754418818	ExtraTreesRegressor(AdaBoostRegressor(CombineDFs(input_matrix, input_matrix), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=square, AdaBoostRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  43%|████▎     | 1100/2550 [40:14<3:00:04,  7.45s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  43%|████▎     | 1100/2550 [40:14<3:00:04,  7.45s/pipeline]Optimization Progress:  43%|████▎     | 1100/2550 [40:14<2:06:07,  5.22s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  43%|████▎     | 1100/2550 [40:15<2:06:07,  5.22s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.
Optimization Progress:  43%|████▎     | 1100/2550 [40:18<2:06:07,  5.22s/pipeline]Optimization Progress:  43%|████▎     | 1101/2550 [40:30<2:06:02,  5.22s/pipeline]Optimization Progress:  43%|████▎     | 1102/2550 [40:46<3:23:20,  8.43s/pipeline]Optimization Progress:  44%|████▍     | 1118/2550 [41:08<2:30:22,  6.30s/pipeline]Optimization Progress:  44%|████▍     | 1134/2550 [41:36<1:56:37,  4.94s/pipeline]Optimization Progress:  45%|████▌     | 1150/2550 [41:40<1:22:35,  3.54s/pipeline]                                                                                  
Generation 22 - Current Pareto front scores:
Optimization Progress:  45%|████▌     | 1150/2550 [41:40<1:22:35,  3.54s/pipeline]                                                                                  
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  45%|████▌     | 1150/2550 [41:40<1:22:35,  3.54s/pipeline]                                                                                  
-2	-0.5462460615959913	ExtraTreesRegressor(CombineDFs(input_matrix, RidgeCV(input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  45%|████▌     | 1150/2550 [41:40<1:22:35,  3.54s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.
Optimization Progress:  45%|████▌     | 1150/2550 [41:43<1:22:35,  3.54s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  45%|████▌     | 1150/2550 [41:45<1:22:35,  3.54s/pipeline]Optimization Progress:  45%|████▌     | 1151/2550 [42:16<5:05:02, 13.08s/pipeline]Optimization Progress:  46%|████▌     | 1167/2550 [42:46<3:44:23,  9.73s/pipeline]Optimization Progress:  46%|████▋     | 1183/2550 [43:17<2:48:07,  7.38s/pipeline]Optimization Progress:  47%|████▋     | 1199/2550 [43:24<1:59:23,  5.30s/pipeline]                                                                                  
Generation 23 - Current Pareto front scores:
Optimization Progress:  47%|████▋     | 1200/2550 [43:24<1:59:18,  5.30s/pipeline]                                                                                  
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  47%|████▋     | 1200/2550 [43:24<1:59:18,  5.30s/pipeline]                                                                                  
-2	-0.5462460615959913	ExtraTreesRegressor(CombineDFs(input_matrix, RidgeCV(input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  47%|████▋     | 1200/2550 [43:24<1:59:18,  5.30s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64.
Optimization Progress:  47%|████▋     | 1200/2550 [43:27<1:59:18,  5.30s/pipeline]Optimization Progress:  47%|████▋     | 1200/2550 [43:27<1:44:03,  4.62s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  47%|████▋     | 1200/2550 [43:27<1:44:03,  4.62s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.
Optimization Progress:  47%|████▋     | 1200/2550 [43:28<1:44:03,  4.62s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  47%|████▋     | 1200/2550 [43:29<1:44:03,  4.62s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  47%|████▋     | 1200/2550 [43:30<1:44:03,  4.62s/pipeline]Optimization Progress:  47%|████▋     | 1201/2550 [43:30<1:30:46,  4.04s/pipeline]Optimization Progress:  47%|████▋     | 1202/2550 [43:52<3:32:32,  9.46s/pipeline]Optimization Progress:  48%|████▊     | 1218/2550 [44:19<2:38:10,  7.13s/pipeline]Optimization Progress:  48%|████▊     | 1234/2550 [44:51<2:02:40,  5.59s/pipeline]Optimization Progress:  49%|████▉     | 1250/2550 [44:58<1:27:56,  4.06s/pipeline]                                                                                  
Generation 24 - Current Pareto front scores:
Optimization Progress:  49%|████▉     | 1250/2550 [44:58<1:27:56,  4.06s/pipeline]                                                                                  
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  49%|████▉     | 1250/2550 [44:58<1:27:56,  4.06s/pipeline]                                                                                  
-2	-0.5462460615959913	ExtraTreesRegressor(CombineDFs(input_matrix, RidgeCV(input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  49%|████▉     | 1250/2550 [44:58<1:27:56,  4.06s/pipeline]Optimization Progress:  49%|████▉     | 1252/2550 [45:03<1:15:48,  3.50s/pipeline]Optimization Progress:  49%|████▉     | 1253/2550 [45:25<3:13:43,  8.96s/pipeline]Optimization Progress:  50%|████▉     | 1269/2550 [45:40<2:20:15,  6.57s/pipeline]Optimization Progress:  50%|█████     | 1285/2550 [46:09<1:48:28,  5.14s/pipeline]                                                                                  
Generation 25 - Current Pareto front scores:
Optimization Progress:  51%|█████     | 1300/2550 [46:10<1:47:10,  5.14s/pipeline]                                                                                  
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  51%|█████     | 1300/2550 [46:10<1:47:10,  5.14s/pipeline]                                                                                  
-2	-0.5462460615959913	ExtraTreesRegressor(CombineDFs(input_matrix, RidgeCV(input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  51%|█████     | 1300/2550 [46:10<1:47:10,  5.14s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.
Optimization Progress:  51%|█████     | 1300/2550 [46:10<1:47:10,  5.14s/pipeline]Optimization Progress:  51%|█████     | 1300/2550 [46:15<1:17:18,  3.71s/pipeline]Optimization Progress:  51%|█████     | 1302/2550 [46:41<2:14:08,  6.45s/pipeline]Optimization Progress:  52%|█████▏    | 1318/2550 [47:06<1:42:19,  4.98s/pipeline]Optimization Progress:  52%|█████▏    | 1334/2550 [47:27<1:18:47,  3.89s/pipeline]Optimization Progress:  53%|█████▎    | 1350/2550 [47:46<1:01:42,  3.09s/pipeline]                                                                                  
Generation 26 - Current Pareto front scores:
Optimization Progress:  53%|█████▎    | 1350/2550 [47:46<1:01:42,  3.09s/pipeline]                                                                                  
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  53%|█████▎    | 1350/2550 [47:46<1:01:42,  3.09s/pipeline]                                                                                  
-2	-0.5462460615959913	ExtraTreesRegressor(CombineDFs(input_matrix, RidgeCV(input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  53%|█████▎    | 1350/2550 [47:46<1:01:42,  3.09s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  53%|█████▎    | 1350/2550 [47:47<1:01:42,  3.09s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  53%|█████▎    | 1350/2550 [47:52<1:01:42,  3.09s/pipeline]Optimization Progress:  53%|█████▎    | 1351/2550 [47:52<1:14:46,  3.74s/pipeline]Optimization Progress:  53%|█████▎    | 1352/2550 [48:28<4:30:35, 13.55s/pipeline]Optimization Progress:  54%|█████▎    | 1368/2550 [49:34<3:31:08, 10.72s/pipeline]Optimization Progress:  54%|█████▍    | 1384/2550 [50:01<2:35:34,  8.01s/pipeline]Optimization Progress:  55%|█████▍    | 1400/2550 [50:01<1:47:29,  5.61s/pipeline]                                                                                  
Generation 27 - Current Pareto front scores:
Optimization Progress:  55%|█████▍    | 1400/2550 [50:01<1:47:29,  5.61s/pipeline]                                                                                  
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  55%|█████▍    | 1400/2550 [50:01<1:47:29,  5.61s/pipeline]                                                                                  
-2	-0.5462460615959913	ExtraTreesRegressor(CombineDFs(input_matrix, RidgeCV(input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  55%|█████▍    | 1400/2550 [50:01<1:47:29,  5.61s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  55%|█████▍    | 1400/2550 [50:01<1:47:29,  5.61s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:  55%|█████▍    | 1400/2550 [50:03<1:47:29,  5.61s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  55%|█████▍    | 1400/2550 [50:03<1:47:29,  5.61s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  55%|█████▍    | 1400/2550 [50:04<1:47:29,  5.61s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.05000.
Optimization Progress:  55%|█████▍    | 1400/2550 [50:04<1:47:29,  5.61s/pipeline]Optimization Progress:  55%|█████▍    | 1401/2550 [50:20<1:47:23,  5.61s/pipeline]Optimization Progress:  55%|█████▍    | 1402/2550 [50:29<2:35:02,  8.10s/pipeline]Optimization Progress:  56%|█████▌    | 1418/2550 [50:47<1:53:34,  6.02s/pipeline]Optimization Progress:  56%|█████▌    | 1434/2550 [51:24<1:31:06,  4.90s/pipeline]Optimization Progress:  57%|█████▋    | 1450/2550 [51:26<1:03:46,  3.48s/pipeline]                                                                                  
Generation 28 - Current Pareto front scores:
Optimization Progress:  57%|█████▋    | 1450/2550 [51:26<1:03:46,  3.48s/pipeline]                                                                                  
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  57%|█████▋    | 1450/2550 [51:26<1:03:46,  3.48s/pipeline]                                                                                  
-2	-0.5462460615959913	ExtraTreesRegressor(CombineDFs(input_matrix, RidgeCV(input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  57%|█████▋    | 1450/2550 [51:26<1:03:46,  3.48s/pipeline]Optimization Progress:  57%|█████▋    | 1451/2550 [52:11<4:49:15, 15.79s/pipeline]Optimization Progress:  58%|█████▊    | 1467/2550 [52:42<3:30:05, 11.64s/pipeline]Optimization Progress:  58%|█████▊    | 1483/2550 [53:05<2:32:25,  8.57s/pipeline]Optimization Progress:  59%|█████▉    | 1499/2550 [53:17<1:49:16,  6.24s/pipeline]                                                                                  
Generation 29 - Current Pareto front scores:
Optimization Progress:  59%|█████▉    | 1500/2550 [53:17<1:49:10,  6.24s/pipeline]                                                                                  
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  59%|█████▉    | 1500/2550 [53:17<1:49:10,  6.24s/pipeline]                                                                                  
-2	-0.5462460615959913	ExtraTreesRegressor(CombineDFs(input_matrix, RidgeCV(input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  59%|█████▉    | 1500/2550 [53:17<1:49:10,  6.24s/pipeline]                                                                                  
-3	-0.5440183999446969	ExtraTreesRegressor(LassoLarsCV(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LassoLarsCV__normalize=True), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  59%|█████▉    | 1500/2550 [53:17<1:49:10,  6.24s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  59%|█████▉    | 1500/2550 [53:17<1:49:10,  6.24s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  59%|█████▉    | 1500/2550 [53:20<1:49:10,  6.24s/pipeline]Optimization Progress:  59%|█████▉    | 1500/2550 [53:20<1:30:46,  5.19s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  59%|█████▉    | 1500/2550 [53:20<1:30:46,  5.19s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  59%|█████▉    | 1500/2550 [53:21<1:30:46,  5.19s/pipeline]Optimization Progress:  59%|█████▉    | 1501/2550 [53:23<1:18:19,  4.48s/pipeline]Optimization Progress:  59%|█████▉    | 1502/2550 [53:47<3:00:54, 10.36s/pipeline]Optimization Progress:  60%|█████▉    | 1518/2550 [54:17<2:14:19,  7.81s/pipeline]Optimization Progress:  60%|██████    | 1534/2550 [54:59<1:45:53,  6.25s/pipeline]Optimization Progress:  61%|██████    | 1550/2550 [55:08<1:15:44,  4.54s/pipeline]                                                                                  
Generation 30 - Current Pareto front scores:
Optimization Progress:  61%|██████    | 1550/2550 [55:08<1:15:44,  4.54s/pipeline]                                                                                  
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  61%|██████    | 1550/2550 [55:08<1:15:44,  4.54s/pipeline]                                                                                  
-2	-0.5462460615959913	ExtraTreesRegressor(CombineDFs(input_matrix, RidgeCV(input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  61%|██████    | 1550/2550 [55:08<1:15:44,  4.54s/pipeline]                                                                                  
-3	-0.5440183999446969	ExtraTreesRegressor(LassoLarsCV(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LassoLarsCV__normalize=True), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  61%|██████    | 1550/2550 [55:08<1:15:44,  4.54s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  61%|██████    | 1550/2550 [55:13<1:15:44,  4.54s/pipeline]Optimization Progress:  61%|██████    | 1551/2550 [55:13<1:18:40,  4.72s/pipeline]Optimization Progress:  61%|██████    | 1552/2550 [55:36<2:48:11, 10.11s/pipeline]Optimization Progress:  61%|██████▏   | 1568/2550 [56:21<2:09:53,  7.94s/pipeline]Optimization Progress:  62%|██████▏   | 1584/2550 [56:49<1:37:44,  6.07s/pipeline]Optimization Progress:  63%|██████▎   | 1600/2550 [56:53<1:08:26,  4.32s/pipeline]                                                                                  
Generation 31 - Current Pareto front scores:
Optimization Progress:  63%|██████▎   | 1600/2550 [56:53<1:08:26,  4.32s/pipeline]                                                                                  
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  63%|██████▎   | 1600/2550 [56:53<1:08:26,  4.32s/pipeline]                                                                                  
-2	-0.5462460615959913	ExtraTreesRegressor(CombineDFs(input_matrix, RidgeCV(input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  63%|██████▎   | 1600/2550 [56:53<1:08:26,  4.32s/pipeline]                                                                                  
-3	-0.5440183999446969	ExtraTreesRegressor(LassoLarsCV(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LassoLarsCV__normalize=True), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  63%|██████▎   | 1600/2550 [56:53<1:08:26,  4.32s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:  63%|██████▎   | 1600/2550 [56:53<1:08:26,  4.32s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  63%|██████▎   | 1600/2550 [56:54<1:08:26,  4.32s/pipeline]Optimization Progress:  63%|██████▎   | 1601/2550 [57:19<2:52:02, 10.88s/pipeline]Optimization Progress:  63%|██████▎   | 1617/2550 [57:42<2:05:00,  8.04s/pipeline]Optimization Progress:  64%|██████▍   | 1633/2550 [58:03<1:32:10,  6.03s/pipeline]Optimization Progress:  65%|██████▍   | 1649/2550 [58:07<1:04:33,  4.30s/pipeline]                                                                                  
Generation 32 - Current Pareto front scores:
Optimization Progress:  65%|██████▍   | 1650/2550 [58:07<1:04:28,  4.30s/pipeline]                                                                                  
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  65%|██████▍   | 1650/2550 [58:07<1:04:28,  4.30s/pipeline]                                                                                  
-2	-0.5462460615959913	ExtraTreesRegressor(CombineDFs(input_matrix, RidgeCV(input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  65%|██████▍   | 1650/2550 [58:07<1:04:28,  4.30s/pipeline]                                                                                  
-3	-0.5440183999446969	ExtraTreesRegressor(LassoLarsCV(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LassoLarsCV__normalize=True), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  65%|██████▍   | 1650/2550 [58:07<1:04:28,  4.30s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  65%|██████▍   | 1650/2550 [58:07<1:04:28,  4.30s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  65%|██████▍   | 1650/2550 [58:11<1:04:28,  4.30s/pipeline]Optimization Progress:  65%|██████▍   | 1650/2550 [58:11<1:03:52,  4.26s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  65%|██████▍   | 1650/2550 [58:12<1:03:52,  4.26s/pipeline]Optimization Progress:  65%|██████▍   | 1651/2550 [58:12<46:46,  3.12s/pipeline]  Optimization Progress:  65%|██████▍   | 1652/2550 [58:29<1:47:39,  7.19s/pipeline]Optimization Progress:  65%|██████▌   | 1668/2550 [58:52<1:20:31,  5.48s/pipeline]Optimization Progress:  66%|██████▌   | 1684/2550 [59:51<1:11:14,  4.94s/pipeline]Optimization Progress:  67%|██████▋   | 1700/2550 [59:54<49:45,  3.51s/pipeline]                                                                                  
Generation 33 - Current Pareto front scores:
Optimization Progress:  67%|██████▋   | 1700/2550 [59:54<49:45,  3.51s/pipeline]                                                                                
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  67%|██████▋   | 1700/2550 [59:54<49:45,  3.51s/pipeline]                                                                                
-2	-0.5462460615959913	ExtraTreesRegressor(CombineDFs(input_matrix, RidgeCV(input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  67%|██████▋   | 1700/2550 [59:54<49:45,  3.51s/pipeline]                                                                                
-3	-0.5440183999446969	ExtraTreesRegressor(LassoLarsCV(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LassoLarsCV__normalize=True), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  67%|██████▋   | 1700/2550 [59:54<49:45,  3.51s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress:  67%|██████▋   | 1700/2550 [59:56<49:45,  3.51s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  67%|██████▋   | 1700/2550 [59:58<49:45,  3.51s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  67%|██████▋   | 1700/2550 [59:58<49:45,  3.51s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  67%|██████▋   | 1700/2550 [1:00:00<49:45,  3.51s/pipeline]Optimization Progress:  67%|██████▋   | 1701/2550 [1:00:00<59:51,  4.23s/pipeline]Optimization Progress:  67%|██████▋   | 1702/2550 [1:00:22<2:17:09,  9.70s/pipeline]Optimization Progress:  67%|██████▋   | 1718/2550 [1:00:46<1:40:16,  7.23s/pipeline]Optimization Progress:  68%|██████▊   | 1734/2550 [1:01:07<1:14:13,  5.46s/pipeline]Optimization Progress:  69%|██████▊   | 1750/2550 [1:01:16<53:18,  4.00s/pipeline]                                                                                    
Generation 34 - Current Pareto front scores:
Optimization Progress:  69%|██████▊   | 1750/2550 [1:01:16<53:18,  4.00s/pipeline]                                                                                  
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  69%|██████▊   | 1750/2550 [1:01:16<53:18,  4.00s/pipeline]                                                                                  
-2	-0.5462460615959913	ExtraTreesRegressor(CombineDFs(input_matrix, RidgeCV(input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  69%|██████▊   | 1750/2550 [1:01:16<53:18,  4.00s/pipeline]                                                                                  
-3	-0.5440183999446969	ExtraTreesRegressor(LassoLarsCV(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LassoLarsCV__normalize=True), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  69%|██████▊   | 1750/2550 [1:01:16<53:18,  4.00s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.
Optimization Progress:  69%|██████▊   | 1750/2550 [1:01:16<53:18,  4.00s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:  69%|██████▊   | 1750/2550 [1:01:18<53:18,  4.00s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99.
Optimization Progress:  69%|██████▊   | 1750/2550 [1:01:20<53:18,  4.00s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.
Optimization Progress:  69%|██████▊   | 1750/2550 [1:01:20<53:18,  4.00s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  69%|██████▊   | 1750/2550 [1:01:21<53:18,  4.00s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  69%|██████▊   | 1750/2550 [1:01:22<53:18,  4.00s/pipeline]Optimization Progress:  69%|██████▊   | 1751/2550 [1:01:23<1:04:42,  4.86s/pipeline]Optimization Progress:  69%|██████▊   | 1752/2550 [1:02:00<3:12:53, 14.50s/pipeline]Optimization Progress:  69%|██████▉   | 1768/2550 [1:02:28<2:19:05, 10.67s/pipeline]Optimization Progress:  70%|██████▉   | 1784/2550 [1:02:56<1:42:04,  7.99s/pipeline]Optimization Progress:  71%|███████   | 1800/2550 [1:03:00<1:10:54,  5.67s/pipeline]                                                                                    
Generation 35 - Current Pareto front scores:
Optimization Progress:  71%|███████   | 1800/2550 [1:03:00<1:10:54,  5.67s/pipeline]                                                                                    
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  71%|███████   | 1800/2550 [1:03:00<1:10:54,  5.67s/pipeline]                                                                                    
-2	-0.5462460615959913	ExtraTreesRegressor(CombineDFs(input_matrix, RidgeCV(input_matrix)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  71%|███████   | 1800/2550 [1:03:00<1:10:54,  5.67s/pipeline]                                                                                    
-3	-0.5440183999446969	ExtraTreesRegressor(LassoLarsCV(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LassoLarsCV__normalize=True), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  71%|███████   | 1800/2550 [1:03:00<1:10:54,  5.67s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.
Optimization Progress:  71%|███████   | 1800/2550 [1:03:00<1:10:54,  5.67s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  71%|███████   | 1800/2550 [1:03:04<1:10:54,  5.67s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.
Optimization Progress:  71%|███████   | 1800/2550 [1:03:04<1:10:54,  5.67s/pipeline]Optimization Progress:  71%|███████   | 1801/2550 [1:03:37<3:09:14, 15.16s/pipeline]Optimization Progress:  71%|███████▏  | 1817/2550 [1:04:07<2:16:23, 11.16s/pipeline]Optimization Progress:  72%|███████▏  | 1833/2550 [1:04:30<1:38:41,  8.26s/pipeline]Optimization Progress:  73%|███████▎  | 1849/2550 [1:04:34<1:08:22,  5.85s/pipeline]                                                                                    
Generation 36 - Current Pareto front scores:
Optimization Progress:  73%|███████▎  | 1850/2550 [1:04:34<1:08:16,  5.85s/pipeline]                                                                                    
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  73%|███████▎  | 1850/2550 [1:04:34<1:08:16,  5.85s/pipeline]                                                                                    
-2	-0.544239796751251	ExtraTreesRegressor(AdaBoostRegressor(CombineDFs(input_matrix, input_matrix), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  73%|███████▎  | 1850/2550 [1:04:34<1:08:16,  5.85s/pipeline]                                                                                    
-3	-0.5440183999446969	ExtraTreesRegressor(LassoLarsCV(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LassoLarsCV__normalize=True), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  73%|███████▎  | 1850/2550 [1:04:34<1:08:16,  5.85s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  73%|███████▎  | 1850/2550 [1:04:35<1:08:16,  5.85s/pipeline]Optimization Progress:  73%|███████▎  | 1850/2550 [1:04:35<51:21,  4.40s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress:  73%|███████▎  | 1850/2550 [1:04:37<51:21,  4.40s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  73%|███████▎  | 1850/2550 [1:04:38<51:21,  4.40s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  73%|███████▎  | 1850/2550 [1:04:38<51:21,  4.40s/pipeline]Optimization Progress:  73%|███████▎  | 1851/2550 [1:04:40<51:34,  4.43s/pipeline]Optimization Progress:  73%|███████▎  | 1852/2550 [1:05:49<4:39:23, 24.02s/pipeline]Optimization Progress:  73%|███████▎  | 1868/2550 [1:06:09<3:15:19, 17.18s/pipeline]Optimization Progress:  74%|███████▍  | 1884/2550 [1:06:36<2:19:00, 12.52s/pipeline]Optimization Progress:  75%|███████▍  | 1900/2550 [1:06:39<1:35:41,  8.83s/pipeline]                                                                                    
Generation 37 - Current Pareto front scores:
Optimization Progress:  75%|███████▍  | 1900/2550 [1:06:39<1:35:41,  8.83s/pipeline]                                                                                    
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  75%|███████▍  | 1900/2550 [1:06:39<1:35:41,  8.83s/pipeline]                                                                                    
-2	-0.544239796751251	ExtraTreesRegressor(AdaBoostRegressor(CombineDFs(input_matrix, input_matrix), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  75%|███████▍  | 1900/2550 [1:06:39<1:35:41,  8.83s/pipeline]                                                                                    
-3	-0.5440183999446969	ExtraTreesRegressor(LassoLarsCV(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LassoLarsCV__normalize=True), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  75%|███████▍  | 1900/2550 [1:06:39<1:35:41,  8.83s/pipeline]Optimization Progress:  75%|███████▍  | 1901/2550 [1:07:05<2:30:06, 13.88s/pipeline]Optimization Progress:  75%|███████▌  | 1917/2550 [1:07:33<1:47:59, 10.24s/pipeline]Optimization Progress:  76%|███████▌  | 1933/2550 [1:07:58<1:18:37,  7.65s/pipeline]Optimization Progress:  76%|███████▋  | 1949/2550 [1:08:02<54:21,  5.43s/pipeline]                                                                                    
Generation 38 - Current Pareto front scores:
Optimization Progress:  76%|███████▋  | 1950/2550 [1:08:02<54:15,  5.43s/pipeline]                                                                                  
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  76%|███████▋  | 1950/2550 [1:08:02<54:15,  5.43s/pipeline]                                                                                  
-2	-0.544239796751251	ExtraTreesRegressor(AdaBoostRegressor(CombineDFs(input_matrix, input_matrix), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  76%|███████▋  | 1950/2550 [1:08:02<54:15,  5.43s/pipeline]                                                                                  
-3	-0.5440183999446969	ExtraTreesRegressor(LassoLarsCV(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LassoLarsCV__normalize=True), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  76%|███████▋  | 1950/2550 [1:08:02<54:15,  5.43s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  76%|███████▋  | 1950/2550 [1:08:04<54:15,  5.43s/pipeline]Optimization Progress:  76%|███████▋  | 1950/2550 [1:08:04<41:56,  4.19s/pipeline]Optimization Progress:  77%|███████▋  | 1951/2550 [1:08:26<1:36:11,  9.64s/pipeline]Optimization Progress:  77%|███████▋  | 1967/2550 [1:09:12<1:13:52,  7.60s/pipeline]Optimization Progress:  78%|███████▊  | 1983/2550 [1:09:36<54:40,  5.79s/pipeline]  Optimization Progress:  78%|███████▊  | 1999/2550 [1:09:41<38:01,  4.14s/pipeline]                                                                                  
Generation 39 - Current Pareto front scores:
Optimization Progress:  78%|███████▊  | 2000/2550 [1:09:41<37:56,  4.14s/pipeline]                                                                                  
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  78%|███████▊  | 2000/2550 [1:09:41<37:56,  4.14s/pipeline]                                                                                  
-2	-0.544239796751251	ExtraTreesRegressor(AdaBoostRegressor(CombineDFs(input_matrix, input_matrix), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  78%|███████▊  | 2000/2550 [1:09:41<37:56,  4.14s/pipeline]                                                                                  
-3	-0.5440183999446969	ExtraTreesRegressor(LassoLarsCV(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LassoLarsCV__normalize=True), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  78%|███████▊  | 2000/2550 [1:09:41<37:56,  4.14s/pipeline]Optimization Progress:  78%|███████▊  | 2000/2550 [1:09:46<40:56,  4.47s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  78%|███████▊  | 2000/2550 [1:09:46<40:56,  4.47s/pipeline]Optimization Progress:  79%|███████▊  | 2002/2550 [1:10:16<1:09:40,  7.63s/pipeline]Optimization Progress:  79%|███████▉  | 2018/2550 [1:10:46<52:17,  5.90s/pipeline]  Optimization Progress:  80%|███████▉  | 2034/2550 [1:11:12<39:36,  4.61s/pipeline]Optimization Progress:  80%|████████  | 2050/2550 [1:11:12<26:59,  3.24s/pipeline]                                                                                  
Generation 40 - Current Pareto front scores:
Optimization Progress:  80%|████████  | 2050/2550 [1:11:12<26:59,  3.24s/pipeline]                                                                                  
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  80%|████████  | 2050/2550 [1:11:12<26:59,  3.24s/pipeline]                                                                                  
-2	-0.544239796751251	ExtraTreesRegressor(AdaBoostRegressor(CombineDFs(input_matrix, input_matrix), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  80%|████████  | 2050/2550 [1:11:12<26:59,  3.24s/pipeline]                                                                                  
-3	-0.5440183999446969	ExtraTreesRegressor(LassoLarsCV(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LassoLarsCV__normalize=True), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  80%|████████  | 2050/2550 [1:11:12<26:59,  3.24s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  80%|████████  | 2050/2550 [1:11:15<26:59,  3.24s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:  80%|████████  | 2050/2550 [1:11:15<26:59,  3.24s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:  80%|████████  | 2050/2550 [1:11:17<26:59,  3.24s/pipeline]Optimization Progress:  80%|████████  | 2051/2550 [1:12:05<2:29:08, 17.93s/pipeline]Optimization Progress:  81%|████████  | 2067/2550 [1:12:31<1:45:04, 13.05s/pipeline]Optimization Progress:  82%|████████▏ | 2083/2550 [1:12:53<1:14:16,  9.54s/pipeline]Optimization Progress:  82%|████████▏ | 2099/2550 [1:13:08<52:23,  6.97s/pipeline]                                                                                    
Generation 41 - Current Pareto front scores:
Optimization Progress:  82%|████████▏ | 2100/2550 [1:13:08<52:16,  6.97s/pipeline]                                                                                  
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  82%|████████▏ | 2100/2550 [1:13:08<52:16,  6.97s/pipeline]                                                                                  
-2	-0.544239796751251	ExtraTreesRegressor(AdaBoostRegressor(CombineDFs(input_matrix, input_matrix), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  82%|████████▏ | 2100/2550 [1:13:08<52:16,  6.97s/pipeline]                                                                                  
-3	-0.5440183999446969	ExtraTreesRegressor(LassoLarsCV(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LassoLarsCV__normalize=True), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  82%|████████▏ | 2100/2550 [1:13:08<52:16,  6.97s/pipeline]                                                                                  
-4	-0.5439765929687682	ExtraTreesRegressor(LassoLarsCV(PolynomialFeatures(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LassoLarsCV__normalize=True), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  82%|████████▏ | 2100/2550 [1:13:08<52:16,  6.97s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.
Optimization Progress:  82%|████████▏ | 2100/2550 [1:13:11<52:16,  6.97s/pipeline]Optimization Progress:  82%|████████▏ | 2100/2550 [1:13:11<41:26,  5.53s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  82%|████████▏ | 2100/2550 [1:13:11<41:26,  5.53s/pipeline]                                                                                  Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  82%|████████▏ | 2100/2550 [1:13:14<41:26,  5.53s/pipeline]Optimization Progress:  82%|████████▏ | 2101/2550 [1:13:14<36:03,  4.82s/pipeline]Optimization Progress:  82%|████████▏ | 2102/2550 [1:14:04<2:17:55, 18.47s/pipeline]Optimization Progress:  83%|████████▎ | 2118/2550 [1:14:44<1:38:27, 13.67s/pipeline]Optimization Progress:  84%|████████▎ | 2134/2550 [1:15:28<1:12:09, 10.41s/pipeline]Optimization Progress:  84%|████████▍ | 2150/2550 [1:15:32<49:03,  7.36s/pipeline]                                                                                    
Generation 42 - Current Pareto front scores:
Optimization Progress:  84%|████████▍ | 2150/2550 [1:15:32<49:03,  7.36s/pipeline]                                                                                  
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  84%|████████▍ | 2150/2550 [1:15:32<49:03,  7.36s/pipeline]                                                                                  
-2	-0.544239796751251	ExtraTreesRegressor(AdaBoostRegressor(CombineDFs(input_matrix, input_matrix), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  84%|████████▍ | 2150/2550 [1:15:32<49:03,  7.36s/pipeline]                                                                                  
-3	-0.5440183999446969	ExtraTreesRegressor(LassoLarsCV(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LassoLarsCV__normalize=True), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  84%|████████▍ | 2150/2550 [1:15:32<49:03,  7.36s/pipeline]                                                                                  
-4	-0.5439765929687682	ExtraTreesRegressor(LassoLarsCV(PolynomialFeatures(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LassoLarsCV__normalize=True), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  84%|████████▍ | 2150/2550 [1:15:32<49:03,  7.36s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.
Optimization Progress:  84%|████████▍ | 2150/2550 [1:15:35<49:03,  7.36s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  84%|████████▍ | 2150/2550 [1:15:37<49:03,  7.36s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  84%|████████▍ | 2150/2550 [1:15:39<49:03,  7.36s/pipeline]Optimization Progress:  84%|████████▍ | 2151/2550 [1:15:39<47:21,  7.12s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  84%|████████▍ | 2151/2550 [1:15:39<47:21,  7.12s/pipeline]Optimization Progress:  84%|████████▍ | 2153/2550 [1:20:39<5:30:45, 49.99s/pipeline]                                                                                    Skipped pipeline #2154 due to time out. Continuing to the next pipeline.
Optimization Progress:  84%|████████▍ | 2154/2550 [1:20:39<5:29:55, 49.99s/pipeline]Optimization Progress:  85%|████████▌ | 2170/2550 [1:21:07<3:44:45, 35.49s/pipeline]Optimization Progress:  86%|████████▌ | 2186/2550 [1:22:09<2:37:46, 26.01s/pipeline]                                                                                    
Generation 43 - Current Pareto front scores:
Optimization Progress:  86%|████████▋ | 2201/2550 [1:22:09<2:31:16, 26.01s/pipeline]                                                                                    
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  86%|████████▋ | 2201/2550 [1:22:09<2:31:16, 26.01s/pipeline]                                                                                    
-2	-0.544239796751251	ExtraTreesRegressor(AdaBoostRegressor(CombineDFs(input_matrix, input_matrix), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  86%|████████▋ | 2201/2550 [1:22:09<2:31:16, 26.01s/pipeline]                                                                                    
-3	-0.5440183999446969	ExtraTreesRegressor(LassoLarsCV(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LassoLarsCV__normalize=True), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  86%|████████▋ | 2201/2550 [1:22:09<2:31:16, 26.01s/pipeline]                                                                                    
-4	-0.5439765929687682	ExtraTreesRegressor(LassoLarsCV(PolynomialFeatures(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LassoLarsCV__normalize=True), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  86%|████████▋ | 2201/2550 [1:22:09<2:31:16, 26.01s/pipeline]                                                                                    
-5	-0.5437350257346557	ExtraTreesRegressor(LassoLarsCV(OneHotEncoder(MinMaxScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LassoLarsCV__normalize=False), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  86%|████████▋ | 2201/2550 [1:22:09<2:31:16, 26.01s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..
Optimization Progress:  86%|████████▋ | 2201/2550 [1:22:10<2:31:16, 26.01s/pipeline]Optimization Progress:  86%|████████▋ | 2201/2550 [1:22:10<1:45:57, 18.22s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  86%|████████▋ | 2201/2550 [1:22:10<1:45:57, 18.22s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  86%|████████▋ | 2201/2550 [1:22:10<1:45:57, 18.22s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  86%|████████▋ | 2201/2550 [1:22:10<1:45:57, 18.22s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  86%|████████▋ | 2201/2550 [1:22:11<1:45:57, 18.22s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress:  86%|████████▋ | 2201/2550 [1:22:12<1:45:57, 18.22s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.
Optimization Progress:  86%|████████▋ | 2201/2550 [1:22:12<1:45:57, 18.22s/pipeline]Optimization Progress:  86%|████████▋ | 2202/2550 [1:22:43<2:12:21, 22.82s/pipeline]Optimization Progress:  87%|████████▋ | 2218/2550 [1:23:16<1:31:46, 16.59s/pipeline]Optimization Progress:  88%|████████▊ | 2234/2550 [1:23:50<1:04:32, 12.26s/pipeline]Optimization Progress:  88%|████████▊ | 2250/2550 [1:24:05<44:16,  8.86s/pipeline]                                                                                    
Generation 44 - Current Pareto front scores:
Optimization Progress:  88%|████████▊ | 2251/2550 [1:24:05<44:07,  8.86s/pipeline]                                                                                  
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  88%|████████▊ | 2251/2550 [1:24:05<44:07,  8.86s/pipeline]                                                                                  
-2	-0.544239796751251	ExtraTreesRegressor(AdaBoostRegressor(CombineDFs(input_matrix, input_matrix), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  88%|████████▊ | 2251/2550 [1:24:05<44:07,  8.86s/pipeline]                                                                                  
-3	-0.5439765929687682	ExtraTreesRegressor(LassoLarsCV(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LassoLarsCV__normalize=True), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  88%|████████▊ | 2251/2550 [1:24:05<44:07,  8.86s/pipeline]                                                                                  
-5	-0.5437350257346557	ExtraTreesRegressor(LassoLarsCV(OneHotEncoder(MinMaxScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LassoLarsCV__normalize=False), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  88%|████████▊ | 2251/2550 [1:24:05<44:07,  8.86s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  88%|████████▊ | 2251/2550 [1:24:05<44:07,  8.86s/pipeline]Optimization Progress:  88%|████████▊ | 2251/2550 [1:24:05<31:40,  6.35s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.
Optimization Progress:  88%|████████▊ | 2251/2550 [1:24:05<31:40,  6.35s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  88%|████████▊ | 2251/2550 [1:24:06<31:40,  6.35s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.
Optimization Progress:  88%|████████▊ | 2251/2550 [1:24:07<31:40,  6.35s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  88%|████████▊ | 2251/2550 [1:24:08<31:40,  6.35s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.05000.
Optimization Progress:  88%|████████▊ | 2251/2550 [1:24:09<31:40,  6.35s/pipeline]Optimization Progress:  88%|████████▊ | 2252/2550 [1:24:11<30:27,  6.13s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  88%|████████▊ | 2252/2550 [1:24:11<30:27,  6.13s/pipeline]Optimization Progress:  88%|████████▊ | 2254/2550 [1:24:56<54:10, 10.98s/pipeline]Optimization Progress:  89%|████████▉ | 2270/2550 [1:26:06<41:59,  9.00s/pipeline]Optimization Progress:  90%|████████▉ | 2286/2550 [1:26:51<31:28,  7.15s/pipeline]                                                                                  
Generation 45 - Current Pareto front scores:
Optimization Progress:  90%|█████████ | 2301/2550 [1:26:51<29:41,  7.15s/pipeline]                                                                                  
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  90%|█████████ | 2301/2550 [1:26:51<29:41,  7.15s/pipeline]                                                                                  
-2	-0.544239796751251	ExtraTreesRegressor(AdaBoostRegressor(CombineDFs(input_matrix, input_matrix), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  90%|█████████ | 2301/2550 [1:26:51<29:41,  7.15s/pipeline]                                                                                  
-3	-0.5439765929687682	ExtraTreesRegressor(LassoLarsCV(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LassoLarsCV__normalize=True), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  90%|█████████ | 2301/2550 [1:26:51<29:41,  7.15s/pipeline]                                                                                  
-5	-0.5437350257346557	ExtraTreesRegressor(LassoLarsCV(OneHotEncoder(MinMaxScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LassoLarsCV__normalize=False), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  90%|█████████ | 2301/2550 [1:26:51<29:41,  7.15s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  90%|█████████ | 2301/2550 [1:26:52<29:41,  7.15s/pipeline]Optimization Progress:  90%|█████████ | 2301/2550 [1:26:52<20:50,  5.02s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  90%|█████████ | 2301/2550 [1:26:53<20:50,  5.02s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  90%|█████████ | 2301/2550 [1:26:56<20:50,  5.02s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.
Optimization Progress:  90%|█████████ | 2301/2550 [1:26:56<20:50,  5.02s/pipeline]                                                                                  Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  90%|█████████ | 2301/2550 [1:26:58<20:50,  5.02s/pipeline]Optimization Progress:  90%|█████████ | 2302/2550 [1:26:58<21:33,  5.22s/pipeline]Optimization Progress:  90%|█████████ | 2303/2550 [1:28:00<1:31:42, 22.28s/pipeline]Optimization Progress:  91%|█████████ | 2319/2550 [1:28:26<1:01:55, 16.08s/pipeline]Optimization Progress:  92%|█████████▏| 2335/2550 [1:28:56<42:22, 11.83s/pipeline]  Optimization Progress:  92%|█████████▏| 2351/2550 [1:28:59<27:39,  8.34s/pipeline]                                                                                  
Generation 46 - Current Pareto front scores:
Optimization Progress:  92%|█████████▏| 2351/2550 [1:28:59<27:39,  8.34s/pipeline]                                                                                  
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  92%|█████████▏| 2351/2550 [1:28:59<27:39,  8.34s/pipeline]                                                                                  
-2	-0.544239796751251	ExtraTreesRegressor(AdaBoostRegressor(CombineDFs(input_matrix, input_matrix), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  92%|█████████▏| 2351/2550 [1:28:59<27:39,  8.34s/pipeline]                                                                                  
-3	-0.5438867188837971	ExtraTreesRegressor(LassoLarsCV(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LassoLarsCV__normalize=True), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  92%|█████████▏| 2351/2550 [1:28:59<27:39,  8.34s/pipeline]                                                                                  
-4	-0.5437350257346557	ExtraTreesRegressor(LassoLarsCV(MinMaxScaler(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False)), LassoLarsCV__normalize=False), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  92%|█████████▏| 2351/2550 [1:28:59<27:39,  8.34s/pipeline]                                                                                  Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  92%|█████████▏| 2351/2550 [1:29:07<27:39,  8.34s/pipeline]Optimization Progress:  92%|█████████▏| 2352/2550 [1:29:07<26:43,  8.10s/pipeline]                                                                                  Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  92%|█████████▏| 2352/2550 [1:29:07<26:43,  8.10s/pipeline]Optimization Progress:  92%|█████████▏| 2354/2550 [1:30:01<44:54, 13.75s/pipeline]Optimization Progress:  93%|█████████▎| 2370/2550 [1:35:07<46:06, 15.37s/pipeline]                                                                                  Skipped pipeline #2379 due to time out. Continuing to the next pipeline.
Optimization Progress:  93%|█████████▎| 2379/2550 [1:35:07<43:48, 15.37s/pipeline]Optimization Progress:  94%|█████████▎| 2387/2550 [1:35:31<30:21, 11.18s/pipeline]                                                                                  
Generation 47 - Current Pareto front scores:
Optimization Progress:  94%|█████████▍| 2402/2550 [1:35:31<27:34, 11.18s/pipeline]                                                                                  
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  94%|█████████▍| 2402/2550 [1:35:31<27:34, 11.18s/pipeline]                                                                                  
-2	-0.544239796751251	ExtraTreesRegressor(AdaBoostRegressor(CombineDFs(input_matrix, input_matrix), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  94%|█████████▍| 2402/2550 [1:35:31<27:34, 11.18s/pipeline]                                                                                  
-3	-0.5438867188837971	ExtraTreesRegressor(LassoLarsCV(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LassoLarsCV__normalize=True), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  94%|█████████▍| 2402/2550 [1:35:31<27:34, 11.18s/pipeline]                                                                                  
-4	-0.5434419907762302	ExtraTreesRegressor(LinearSVR(LassoLarsCV(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LassoLarsCV__normalize=True), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  94%|█████████▍| 2402/2550 [1:35:31<27:34, 11.18s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  94%|█████████▍| 2402/2550 [1:35:31<27:34, 11.18s/pipeline]Optimization Progress:  94%|█████████▍| 2402/2550 [1:35:31<19:18,  7.83s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  94%|█████████▍| 2402/2550 [1:35:31<19:18,  7.83s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  94%|█████████▍| 2402/2550 [1:35:31<19:18,  7.83s/pipeline]                                                                                  Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  94%|█████████▍| 2402/2550 [1:35:39<19:18,  7.83s/pipeline]Optimization Progress:  94%|█████████▍| 2403/2550 [1:35:50<19:10,  7.83s/pipeline]Optimization Progress:  94%|█████████▍| 2404/2550 [1:36:03<24:58, 10.27s/pipeline]Optimization Progress:  95%|█████████▍| 2420/2550 [1:41:17<28:19, 13.07s/pipeline]                                                                                  Skipped pipeline #2431 due to time out. Continuing to the next pipeline.
Optimization Progress:  95%|█████████▌| 2431/2550 [1:41:17<25:55, 13.07s/pipeline]Optimization Progress:  96%|█████████▌| 2437/2550 [1:41:55<18:30,  9.82s/pipeline]Optimization Progress:  96%|█████████▌| 2453/2550 [1:41:59<11:13,  6.94s/pipeline]                                                                                  
Generation 48 - Current Pareto front scores:
Optimization Progress:  96%|█████████▌| 2453/2550 [1:41:59<11:13,  6.94s/pipeline]                                                                                  
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  96%|█████████▌| 2453/2550 [1:41:59<11:13,  6.94s/pipeline]                                                                                  
-2	-0.544239796751251	ExtraTreesRegressor(AdaBoostRegressor(CombineDFs(input_matrix, input_matrix), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  96%|█████████▌| 2453/2550 [1:41:59<11:13,  6.94s/pipeline]                                                                                  
-3	-0.5438867188837971	ExtraTreesRegressor(LassoLarsCV(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LassoLarsCV__normalize=True), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  96%|█████████▌| 2453/2550 [1:41:59<11:13,  6.94s/pipeline]                                                                                  
-4	-0.5434419907762302	ExtraTreesRegressor(LinearSVR(LassoLarsCV(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LassoLarsCV__normalize=True), LinearSVR__C=10.0, LinearSVR__dual=True, LinearSVR__epsilon=1.0, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=1e-05), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  96%|█████████▌| 2453/2550 [1:41:59<11:13,  6.94s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  96%|█████████▌| 2453/2550 [1:42:03<11:13,  6.94s/pipeline]Optimization Progress:  96%|█████████▌| 2454/2550 [1:42:06<11:24,  7.13s/pipeline]                                                                                  Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  96%|█████████▌| 2454/2550 [1:42:06<11:24,  7.13s/pipeline]Optimization Progress:  96%|█████████▋| 2456/2550 [1:43:17<24:30, 15.64s/pipeline]Optimization Progress:  97%|█████████▋| 2472/2550 [1:43:54<15:08, 11.64s/pipeline]Optimization Progress:  98%|█████████▊| 2488/2550 [1:45:11<09:54,  9.59s/pipeline]                                                                                  
Generation 49 - Current Pareto front scores:
Optimization Progress:  98%|█████████▊| 2503/2550 [1:45:11<07:30,  9.59s/pipeline]                                                                                  
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  98%|█████████▊| 2503/2550 [1:45:11<07:30,  9.59s/pipeline]                                                                                  
-2	-0.544239796751251	ExtraTreesRegressor(AdaBoostRegressor(CombineDFs(input_matrix, input_matrix), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  98%|█████████▊| 2503/2550 [1:45:11<07:30,  9.59s/pipeline]                                                                                  
-3	-0.5438867188837971	ExtraTreesRegressor(LassoLarsCV(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LassoLarsCV__normalize=True), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  98%|█████████▊| 2503/2550 [1:45:11<07:30,  9.59s/pipeline]                                                                                  
-4	-0.5434293131791026	ExtraTreesRegressor(StandardScaler(LassoLarsCV(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LassoLarsCV__normalize=True)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  98%|█████████▊| 2503/2550 [1:45:11<07:30,  9.59s/pipeline]                                                                                  
-7	-0.5432400695459018	ExtraTreesRegressor(LassoLarsCV(OneHotEncoder(MinMaxScaler(MinMaxScaler(LinearSVR(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=0.01, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.1))), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LassoLarsCV__normalize=False), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress:  98%|█████████▊| 2503/2550 [1:45:11<07:30,  9.59s/pipeline]Optimization Progress:  98%|█████████▊| 2503/2550 [1:45:18<05:21,  6.85s/pipeline]Optimization Progress:  98%|█████████▊| 2505/2550 [1:45:53<07:30, 10.01s/pipeline]Optimization Progress:  99%|█████████▉| 2521/2550 [1:46:34<03:45,  7.79s/pipeline]Optimization Progress:  99%|█████████▉| 2537/2550 [1:48:18<01:36,  7.40s/pipeline]Optimization Progress: 2553pipeline [1:48:21,  5.24s/pipeline]                                                                                  
Generation 50 - Current Pareto front scores:
Optimization Progress: 2553pipeline [1:48:21,  5.24s/pipeline]                                                              
-1	-0.5497747950984838	ExtraTreesRegressor(CombineDFs(input_matrix, input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress: 2553pipeline [1:48:21,  5.24s/pipeline]                                                              
-2	-0.544239796751251	ExtraTreesRegressor(AdaBoostRegressor(CombineDFs(input_matrix, input_matrix), AdaBoostRegressor__learning_rate=1.0, AdaBoostRegressor__loss=exponential, AdaBoostRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress: 2553pipeline [1:48:21,  5.24s/pipeline]                                                              
-3	-0.5438867188837971	ExtraTreesRegressor(LassoLarsCV(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LassoLarsCV__normalize=True), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.6000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)
Optimization Progress: 2553pipeline [1:48:21,  5.24s/pipeline]                                                              
-4	-0.5434293131791026	ExtraTreesRegressor(StandardScaler(LassoLarsCV(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LassoLarsCV__normalize=True)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.1, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress: 2553pipeline [1:48:21,  5.24s/pipeline]                                                              
-7	-0.5432400695459018	ExtraTreesRegressor(LassoLarsCV(OneHotEncoder(MinMaxScaler(MinMaxScaler(LinearSVR(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=0.01, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.1))), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LassoLarsCV__normalize=False), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress: 2553pipeline [1:48:21,  5.24s/pipeline]                                                              
-8	-0.5422980179962902	ExtraTreesRegressor(LassoLarsCV(LassoLarsCV(OneHotEncoder(MinMaxScaler(MinMaxScaler(LinearSVR(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), LinearSVR__C=0.01, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.1))), OneHotEncoder__minimum_fraction=0.05, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LassoLarsCV__normalize=True), LassoLarsCV__normalize=False), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)
Optimization Progress: 2553pipeline [1:48:21,  5.24s/pipeline]                                                              MODEL SCORE -0.5883298774272301
30 operators have been imported by TPOT.
Optimization Progress:   0%|          | 0/2550 [00:00<?, ?pipeline/s]Optimization Progress:   0%|          | 3/2550 [00:25<5:56:32,  8.40s/pipeline]Optimization Progress:   1%|          | 19/2550 [00:38<4:18:13,  6.12s/pipeline]Optimization Progress:   1%|▏         | 35/2550 [00:49<3:08:42,  4.50s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:   2%|▏         | 50/2550 [00:49<3:07:35,  4.50s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:   2%|▏         | 50/2550 [00:50<3:07:35,  4.50s/pipeline]Optimization Progress:   2%|▏         | 50/2550 [00:50<2:11:56,  3.17s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:   2%|▏         | 50/2550 [00:51<2:11:56,  3.17s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:   2%|▏         | 50/2550 [00:52<2:11:56,  3.17s/pipeline]Optimization Progress:   2%|▏         | 51/2550 [00:52<2:02:42,  2.95s/pipeline]Optimization Progress:   2%|▏         | 52/2550 [01:18<6:50:09,  9.85s/pipeline]Optimization Progress:   3%|▎         | 68/2550 [01:31<4:54:51,  7.13s/pipeline]Optimization Progress:   3%|▎         | 84/2550 [01:44<3:35:34,  5.25s/pipeline]Optimization Progress:   4%|▍         | 100/2550 [01:49<2:33:48,  3.77s/pipeline]                                                                                 
Generation 1 - Current Pareto front scores:
Optimization Progress:   4%|▍         | 100/2550 [01:49<2:33:48,  3.77s/pipeline]                                                                                 
-1	-4.613410047230809	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)
Optimization Progress:   4%|▍         | 100/2550 [01:49<2:33:48,  3.77s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.
Optimization Progress:   4%|▍         | 100/2550 [01:50<2:33:48,  3.77s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:   4%|▍         | 100/2550 [01:52<2:33:48,  3.77s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:   4%|▍         | 100/2550 [01:54<2:33:48,  3.77s/pipeline]Optimization Progress:   4%|▍         | 101/2550 [01:55<2:51:46,  4.21s/pipeline]Optimization Progress:   4%|▍         | 102/2550 [02:07<4:33:30,  6.70s/pipeline]Optimization Progress:   5%|▍         | 118/2550 [02:25<3:23:36,  5.02s/pipeline]Optimization Progress:   5%|▌         | 134/2550 [02:58<2:46:38,  4.14s/pipeline]Optimization Progress:   6%|▌         | 150/2550 [03:03<1:59:35,  2.99s/pipeline]                                                                                 
Generation 2 - Current Pareto front scores:
Optimization Progress:   6%|▌         | 150/2550 [03:03<1:59:35,  2.99s/pipeline]                                                                                 
-1	-4.613410047230809	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)
Optimization Progress:   6%|▌         | 150/2550 [03:03<1:59:35,  2.99s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:   6%|▌         | 150/2550 [03:06<1:59:35,  2.99s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.
Optimization Progress:   6%|▌         | 150/2550 [03:07<1:59:35,  2.99s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:   6%|▌         | 150/2550 [03:08<1:59:35,  2.99s/pipeline]Optimization Progress:   6%|▌         | 151/2550 [03:27<6:16:10,  9.41s/pipeline]Optimization Progress:   7%|▋         | 167/2550 [03:51<4:39:26,  7.04s/pipeline]Optimization Progress:   7%|▋         | 183/2550 [04:16<3:32:45,  5.39s/pipeline]Optimization Progress:   8%|▊         | 199/2550 [04:20<2:30:17,  3.84s/pipeline]                                                                                 
Generation 3 - Current Pareto front scores:
Optimization Progress:   8%|▊         | 200/2550 [04:20<2:30:14,  3.84s/pipeline]                                                                                 
-1	-4.613410047230809	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)
Optimization Progress:   8%|▊         | 200/2550 [04:20<2:30:14,  3.84s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.
Optimization Progress:   8%|▊         | 200/2550 [04:29<2:30:14,  3.84s/pipeline]Optimization Progress:   8%|▊         | 200/2550 [04:29<3:41:25,  5.65s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=1 [06:37:09] ../src/learner.cc:602: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x19c8d9) [0x7fdf6464d8d9]
  [bt] (1) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b1f1b) [0x7fdf64662f1b]
  [bt] (2) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b9b4d) [0x7fdf6466ab4d]
  [bt] (3) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b4559) [0x7fdf64665559]
  [bt] (4) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fdf6454a4e8]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fe03280fff5]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fe03280f40a]
  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x5b6) [0x7fe02f4c5306]
  [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x139dc) [0x7fe02f4c59dc]

.
Optimization Progress:   8%|▊         | 200/2550 [04:29<3:41:25,  5.65s/pipeline]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:   8%|▊         | 200/2550 [04:30<3:41:25,  5.65s/pipeline]Optimization Progress:   8%|▊         | 201/2550 [04:30<2:44:15,  4.20s/pipeline]Optimization Progress:   8%|▊         | 202/2550 [05:05<8:42:39, 13.36s/pipeline]Optimization Progress:   9%|▊         | 218/2550 [05:33<6:23:49,  9.88s/pipeline]Optimization Progress:   9%|▉         | 234/2550 [06:10<4:53:32,  7.60s/pipeline]Optimization Progress:  10%|▉         | 250/2550 [06:14<3:26:51,  5.40s/pipeline]                                                                                 
Generation 4 - Current Pareto front scores:
Optimization Progress:  10%|▉         | 250/2550 [06:14<3:26:51,  5.40s/pipeline]                                                                                 
-1	-4.613410047230809	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)
Optimization Progress:  10%|▉         | 250/2550 [06:14<3:26:51,  5.40s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  10%|▉         | 250/2550 [06:14<3:26:51,  5.40s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 [06:38:54] ../src/learner.cc:602: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x19c8d9) [0x7fdf6464d8d9]
  [bt] (1) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b1f1b) [0x7fdf64662f1b]
  [bt] (2) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b9b4d) [0x7fdf6466ab4d]
  [bt] (3) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b4559) [0x7fdf64665559]
  [bt] (4) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fdf6454a4e8]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fe03280fff5]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fe03280f40a]
  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x5b6) [0x7fe02f4c5306]
  [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x139dc) [0x7fe02f4c59dc]

.
Optimization Progress:  10%|▉         | 250/2550 [06:15<3:26:51,  5.40s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:  10%|▉         | 250/2550 [06:17<3:26:51,  5.40s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 [06:38:58] ../src/learner.cc:602: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x19c8d9) [0x7fdf6464d8d9]
  [bt] (1) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b1f1b) [0x7fdf64662f1b]
  [bt] (2) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b9b4d) [0x7fdf6466ab4d]
  [bt] (3) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b4559) [0x7fdf64665559]
  [bt] (4) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fdf6454a4e8]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fe03280fff5]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fe03280f40a]
  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x5b6) [0x7fe02f4c5306]
  [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x139dc) [0x7fe02f4c59dc]

.
Optimization Progress:  10%|▉         | 250/2550 [06:19<3:26:51,  5.40s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.
Optimization Progress:  10%|▉         | 250/2550 [06:22<3:26:51,  5.40s/pipeline]Optimization Progress:  10%|▉         | 251/2550 [07:02<11:39:27, 18.25s/pipeline]Optimization Progress:  10%|█         | 267/2550 [07:41<8:33:37, 13.50s/pipeline] Optimization Progress:  11%|█         | 283/2550 [08:25<6:28:16, 10.28s/pipeline]Optimization Progress:  12%|█▏        | 299/2550 [08:34<4:36:40,  7.37s/pipeline]                                                                                 
Generation 5 - Current Pareto front scores:
Optimization Progress:  12%|█▏        | 300/2550 [08:34<4:36:33,  7.37s/pipeline]                                                                                 
-1	-4.613410047230809	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)
Optimization Progress:  12%|█▏        | 300/2550 [08:34<4:36:33,  7.37s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  12%|█▏        | 300/2550 [08:39<4:36:33,  7.37s/pipeline]Optimization Progress:  12%|█▏        | 300/2550 [08:39<4:04:08,  6.51s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  12%|█▏        | 300/2550 [08:42<4:04:08,  6.51s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 [06:41:24] ../src/learner.cc:602: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x19c8d9) [0x7fdf6464d8d9]
  [bt] (1) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b1f1b) [0x7fdf64662f1b]
  [bt] (2) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b9b4d) [0x7fdf6466ab4d]
  [bt] (3) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b4559) [0x7fdf64665559]
  [bt] (4) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fdf6454a4e8]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fe03280fff5]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fe03280f40a]
  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x5b6) [0x7fe02f4c5306]
  [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x139dc) [0x7fe02f4c59dc]

.
Optimization Progress:  12%|█▏        | 300/2550 [08:45<4:04:08,  6.51s/pipeline]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  12%|█▏        | 300/2550 [08:48<4:04:08,  6.51s/pipeline]Optimization Progress:  12%|█▏        | 301/2550 [08:48<4:34:20,  7.32s/pipeline]Optimization Progress:  12%|█▏        | 302/2550 [09:22<9:31:07, 15.24s/pipeline]Optimization Progress:  12%|█▏        | 318/2550 [10:29<7:24:04, 11.94s/pipeline]Optimization Progress:  13%|█▎        | 334/2550 [11:11<5:37:19,  9.13s/pipeline]Optimization Progress:  14%|█▎        | 350/2550 [11:20<4:01:03,  6.57s/pipeline]                                                                                 
Generation 6 - Current Pareto front scores:
Optimization Progress:  14%|█▎        | 350/2550 [11:20<4:01:03,  6.57s/pipeline]                                                                                 
-1	-4.613410047230809	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=14, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.9500000000000001, XGBRegressor__verbosity=0)
Optimization Progress:  14%|█▎        | 350/2550 [11:20<4:01:03,  6.57s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:  14%|█▎        | 350/2550 [11:29<4:01:03,  6.57s/pipeline]Optimization Progress:  14%|█▍        | 352/2550 [11:36<4:13:52,  6.93s/pipeline]Optimization Progress:  14%|█▍        | 353/2550 [12:24<11:48:27, 19.35s/pipeline]Optimization Progress:  14%|█▍        | 369/2550 [13:05<8:40:01, 14.31s/pipeline] Optimization Progress:  15%|█▌        | 385/2550 [13:44<6:27:43, 10.75s/pipeline]                                                                                 
Generation 7 - Current Pareto front scores:
Optimization Progress:  16%|█▌        | 400/2550 [13:44<6:25:02, 10.75s/pipeline]                                                                                 
-1	-4.376134425571518	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001, XGBRegressor__verbosity=0)
Optimization Progress:  16%|█▌        | 400/2550 [13:44<6:25:02, 10.75s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.
Optimization Progress:  16%|█▌        | 400/2550 [13:45<6:25:02, 10.75s/pipeline]Optimization Progress:  16%|█▌        | 400/2550 [13:45<4:30:38,  7.55s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..
Optimization Progress:  16%|█▌        | 400/2550 [13:46<4:30:38,  7.55s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  16%|█▌        | 400/2550 [13:50<4:30:38,  7.55s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:  16%|█▌        | 400/2550 [13:52<4:30:38,  7.55s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  16%|█▌        | 400/2550 [13:57<4:30:38,  7.55s/pipeline]Optimization Progress:  16%|█▌        | 401/2550 [13:58<5:19:45,  8.93s/pipeline]Optimization Progress:  16%|█▌        | 402/2550 [14:54<13:53:43, 23.29s/pipeline]Optimization Progress:  16%|█▋        | 418/2550 [15:54<10:18:44, 17.41s/pipeline]Optimization Progress:  17%|█▋        | 434/2550 [16:30<7:34:03, 12.88s/pipeline] Optimization Progress:  18%|█▊        | 450/2550 [16:40<5:21:37,  9.19s/pipeline]                                                                                 
Generation 8 - Current Pareto front scores:
Optimization Progress:  18%|█▊        | 450/2550 [16:40<5:21:37,  9.19s/pipeline]                                                                                 
-1	-4.376134425571518	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001, XGBRegressor__verbosity=0)
Optimization Progress:  18%|█▊        | 450/2550 [16:40<5:21:37,  9.19s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.
Optimization Progress:  18%|█▊        | 450/2550 [16:40<5:21:37,  9.19s/pipeline]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  18%|█▊        | 450/2550 [16:54<5:21:37,  9.19s/pipeline]Optimization Progress:  18%|█▊        | 451/2550 [16:54<6:14:55, 10.72s/pipeline]Optimization Progress:  18%|█▊        | 452/2550 [17:33<11:13:10, 19.25s/pipeline]Optimization Progress:  18%|█▊        | 468/2550 [18:12<8:12:58, 14.21s/pipeline] Optimization Progress:  19%|█▉        | 484/2550 [19:08<6:18:43, 11.00s/pipeline]Optimization Progress:  20%|█▉        | 500/2550 [19:23<4:32:27,  7.97s/pipeline]                                                                                 
Generation 9 - Current Pareto front scores:
Optimization Progress:  20%|█▉        | 500/2550 [19:23<4:32:27,  7.97s/pipeline]                                                                                 
-1	-4.376134425571518	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001, XGBRegressor__verbosity=0)
Optimization Progress:  20%|█▉        | 500/2550 [19:23<4:32:27,  7.97s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..
Optimization Progress:  20%|█▉        | 500/2550 [19:23<4:32:27,  7.97s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:  20%|█▉        | 500/2550 [19:26<4:32:27,  7.97s/pipeline]Optimization Progress:  20%|█▉        | 501/2550 [19:37<5:34:56,  9.81s/pipeline]Optimization Progress:  20%|█▉        | 502/2550 [20:23<11:49:37, 20.79s/pipeline]Optimization Progress:  20%|██        | 518/2550 [21:01<8:36:41, 15.26s/pipeline] Optimization Progress:  21%|██        | 534/2550 [21:41<6:24:11, 11.43s/pipeline]Optimization Progress:  22%|██▏       | 550/2550 [21:46<4:29:41,  8.09s/pipeline]                                                                                 
Generation 10 - Current Pareto front scores:
Optimization Progress:  22%|██▏       | 550/2550 [21:46<4:29:41,  8.09s/pipeline]                                                                                 
-1	-4.376134425571518	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001, XGBRegressor__verbosity=0)
Optimization Progress:  22%|██▏       | 550/2550 [21:46<4:29:41,  8.09s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..
Optimization Progress:  22%|██▏       | 550/2550 [21:46<4:29:41,  8.09s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64.
Optimization Progress:  22%|██▏       | 550/2550 [22:00<4:29:41,  8.09s/pipeline]Optimization Progress:  22%|██▏       | 552/2550 [22:00<4:20:48,  7.83s/pipeline]Optimization Progress:  22%|██▏       | 553/2550 [22:42<9:55:29, 17.89s/pipeline]Optimization Progress:  22%|██▏       | 569/2550 [24:28<7:59:05, 14.51s/pipeline]Optimization Progress:  23%|██▎       | 585/2550 [25:15<6:01:34, 11.04s/pipeline]                                                                                 
Generation 11 - Current Pareto front scores:
Optimization Progress:  24%|██▎       | 600/2550 [25:15<5:58:49, 11.04s/pipeline]                                                                                 
-1	-4.376134425571518	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001, XGBRegressor__verbosity=0)
Optimization Progress:  24%|██▎       | 600/2550 [25:15<5:58:49, 11.04s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.
Optimization Progress:  24%|██▎       | 600/2550 [25:27<5:58:49, 11.04s/pipeline]Optimization Progress:  24%|██▎       | 600/2550 [25:27<4:18:56,  7.97s/pipeline]Optimization Progress:  24%|██▎       | 602/2550 [25:31<3:21:05,  6.19s/pipeline]Optimization Progress:  24%|██▎       | 603/2550 [26:10<8:42:22, 16.10s/pipeline]Optimization Progress:  24%|██▍       | 619/2550 [26:52<6:27:53, 12.05s/pipeline]Optimization Progress:  25%|██▍       | 635/2550 [28:11<5:16:27,  9.92s/pipeline]                                                                                 
Generation 12 - Current Pareto front scores:
Optimization Progress:  25%|██▌       | 650/2550 [28:11<5:13:59,  9.92s/pipeline]                                                                                 
-1	-4.376134425571518	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001, XGBRegressor__verbosity=0)
Optimization Progress:  25%|██▌       | 650/2550 [28:11<5:13:59,  9.92s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress:  25%|██▌       | 650/2550 [28:23<5:13:59,  9.92s/pipeline]Optimization Progress:  25%|██▌       | 650/2550 [28:23<3:47:21,  7.18s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  25%|██▌       | 650/2550 [28:24<3:47:21,  7.18s/pipeline]Optimization Progress:  26%|██▌       | 651/2550 [28:25<3:04:39,  5.83s/pipeline]Optimization Progress:  26%|██▌       | 652/2550 [29:07<8:40:30, 16.45s/pipeline]Optimization Progress:  26%|██▌       | 668/2550 [29:51<6:27:14, 12.35s/pipeline]Optimization Progress:  27%|██▋       | 684/2550 [30:58<5:07:48,  9.90s/pipeline]Optimization Progress:  27%|██▋       | 700/2550 [31:09<3:39:58,  7.13s/pipeline]                                                                                 
Generation 13 - Current Pareto front scores:
Optimization Progress:  27%|██▋       | 700/2550 [31:09<3:39:58,  7.13s/pipeline]                                                                                 
-1	-4.376134425571518	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001, XGBRegressor__verbosity=0)
Optimization Progress:  27%|██▋       | 700/2550 [31:09<3:39:58,  7.13s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  27%|██▋       | 700/2550 [31:14<3:39:58,  7.13s/pipeline]Optimization Progress:  27%|██▋       | 701/2550 [31:23<4:50:07,  9.41s/pipeline]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  27%|██▋       | 701/2550 [31:23<4:50:07,  9.41s/pipeline]Optimization Progress:  28%|██▊       | 703/2550 [32:31<8:35:30, 16.75s/pipeline]Optimization Progress:  28%|██▊       | 719/2550 [33:46<6:40:43, 13.13s/pipeline]Optimization Progress:  29%|██▉       | 735/2550 [34:47<5:12:41, 10.34s/pipeline]                                                                                 
Generation 14 - Current Pareto front scores:
Optimization Progress:  29%|██▉       | 750/2550 [34:47<5:10:06, 10.34s/pipeline]                                                                                 
-1	-4.376134425571518	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001, XGBRegressor__verbosity=0)
Optimization Progress:  29%|██▉       | 750/2550 [34:47<5:10:06, 10.34s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  29%|██▉       | 750/2550 [34:59<5:10:06, 10.34s/pipeline]Optimization Progress:  29%|██▉       | 750/2550 [34:59<3:44:21,  7.48s/pipeline]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  29%|██▉       | 750/2550 [35:03<3:44:21,  7.48s/pipeline]Optimization Progress:  29%|██▉       | 751/2550 [35:03<3:09:30,  6.32s/pipeline]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  29%|██▉       | 751/2550 [35:03<3:09:30,  6.32s/pipeline]Optimization Progress:  30%|██▉       | 753/2550 [35:46<5:24:25, 10.83s/pipeline]Optimization Progress:  30%|███       | 769/2550 [36:29<4:08:59,  8.39s/pipeline]Optimization Progress:  31%|███       | 785/2550 [37:11<3:15:55,  6.66s/pipeline]                                                                                 
Generation 15 - Current Pareto front scores:
Optimization Progress:  31%|███▏      | 800/2550 [37:11<3:14:15,  6.66s/pipeline]                                                                                 
-1	-4.376134425571518	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001, XGBRegressor__verbosity=0)
Optimization Progress:  31%|███▏      | 800/2550 [37:11<3:14:15,  6.66s/pipeline]                                                                                 
-3	-4.369047536035814	XGBRegressor(GradientBoostingRegressor(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.15, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=ls, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.8500000000000001, GradientBoostingRegressor__min_samples_leaf=2, GradientBoostingRegressor__min_samples_split=17, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6000000000000001, XGBRegressor__verbosity=0)
Optimization Progress:  31%|███▏      | 800/2550 [37:11<3:14:15,  6.66s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 [07:09:52] ../src/learner.cc:602: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x19c8d9) [0x7fdf6464d8d9]
  [bt] (1) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b1f1b) [0x7fdf64662f1b]
  [bt] (2) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b9b4d) [0x7fdf6466ab4d]
  [bt] (3) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b4559) [0x7fdf64665559]
  [bt] (4) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fdf6454a4e8]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fe03280fff5]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fe03280f40a]
  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x5b6) [0x7fe02f4c5306]
  [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x139dc) [0x7fe02f4c59dc]

.
Optimization Progress:  31%|███▏      | 800/2550 [37:13<3:14:15,  6.66s/pipeline]Optimization Progress:  31%|███▏      | 800/2550 [37:13<2:16:59,  4.70s/pipeline]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  31%|███▏      | 800/2550 [37:26<2:16:59,  4.70s/pipeline]Optimization Progress:  31%|███▏      | 801/2550 [37:26<3:35:20,  7.39s/pipeline]Optimization Progress:  31%|███▏      | 802/2550 [38:10<8:55:29, 18.38s/pipeline]Optimization Progress:  32%|███▏      | 818/2550 [39:06<6:41:31, 13.91s/pipeline]Optimization Progress:  33%|███▎      | 834/2550 [39:53<5:03:50, 10.62s/pipeline]Optimization Progress:  33%|███▎      | 850/2550 [40:04<3:36:26,  7.64s/pipeline]                                                                                 
Generation 16 - Current Pareto front scores:
Optimization Progress:  33%|███▎      | 850/2550 [40:04<3:36:26,  7.64s/pipeline]                                                                                 
-1	-4.376134425571518	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001, XGBRegressor__verbosity=0)
Optimization Progress:  33%|███▎      | 850/2550 [40:04<3:36:26,  7.64s/pipeline]                                                                                 
-2	-4.373744628793642	XGBRegressor(MinMaxScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001, XGBRegressor__verbosity=0)
Optimization Progress:  33%|███▎      | 850/2550 [40:04<3:36:26,  7.64s/pipeline]                                                                                 
-3	-4.369047536035814	XGBRegressor(GradientBoostingRegressor(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.15, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=ls, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.8500000000000001, GradientBoostingRegressor__min_samples_leaf=2, GradientBoostingRegressor__min_samples_split=17, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6000000000000001, XGBRegressor__verbosity=0)
Optimization Progress:  33%|███▎      | 850/2550 [40:04<3:36:26,  7.64s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 [07:12:46] ../src/learner.cc:602: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x19c8d9) [0x7fdf6464d8d9]
  [bt] (1) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b1f1b) [0x7fdf64662f1b]
  [bt] (2) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b9b4d) [0x7fdf6466ab4d]
  [bt] (3) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b4559) [0x7fdf64665559]
  [bt] (4) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fdf6454a4e8]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fe03280fff5]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fe03280f40a]
  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x5b6) [0x7fe02f4c5306]
  [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x139dc) [0x7fe02f4c59dc]

.
Optimization Progress:  33%|███▎      | 850/2550 [40:06<3:36:26,  7.64s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 [07:12:49] ../src/learner.cc:602: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x19c8d9) [0x7fdf6464d8d9]
  [bt] (1) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b1f1b) [0x7fdf64662f1b]
  [bt] (2) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b9b4d) [0x7fdf6466ab4d]
  [bt] (3) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b4559) [0x7fdf64665559]
  [bt] (4) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fdf6454a4e8]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fe03280fff5]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fe03280f40a]
  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x5b6) [0x7fe02f4c5306]
  [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x139dc) [0x7fe02f4c59dc]

.
Optimization Progress:  33%|███▎      | 850/2550 [40:09<3:36:26,  7.64s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.
Optimization Progress:  33%|███▎      | 850/2550 [40:16<3:36:26,  7.64s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=1 [07:12:56] ../src/learner.cc:602: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x19c8d9) [0x7fdf6464d8d9]
  [bt] (1) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b1f1b) [0x7fdf64662f1b]
  [bt] (2) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b9b4d) [0x7fdf6466ab4d]
  [bt] (3) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b4559) [0x7fdf64665559]
  [bt] (4) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fdf6454a4e8]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fe03280fff5]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fe03280f40a]
  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x5b6) [0x7fe02f4c5306]
  [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x139dc) [0x7fe02f4c59dc]

.
Optimization Progress:  33%|███▎      | 850/2550 [40:16<3:36:26,  7.64s/pipeline]Optimization Progress:  33%|███▎      | 851/2550 [41:04<10:58:36, 23.26s/pipeline]Optimization Progress:  34%|███▍      | 867/2550 [41:43<7:57:35, 17.03s/pipeline] Optimization Progress:  35%|███▍      | 883/2550 [42:41<6:01:15, 13.00s/pipeline]Optimization Progress:  35%|███▌      | 899/2550 [42:51<4:15:36,  9.29s/pipeline]                                                                                 
Generation 17 - Current Pareto front scores:
Optimization Progress:  35%|███▌      | 900/2550 [42:51<4:15:26,  9.29s/pipeline]                                                                                 
-1	-4.376134425571518	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001, XGBRegressor__verbosity=0)
Optimization Progress:  35%|███▌      | 900/2550 [42:51<4:15:26,  9.29s/pipeline]                                                                                 
-2	-4.369047536035814	XGBRegressor(GradientBoostingRegressor(input_matrix, GradientBoostingRegressor__alpha=0.99, GradientBoostingRegressor__learning_rate=0.5, GradientBoostingRegressor__loss=ls, GradientBoostingRegressor__max_depth=1, GradientBoostingRegressor__max_features=0.8500000000000001, GradientBoostingRegressor__min_samples_leaf=2, GradientBoostingRegressor__min_samples_split=17, GradientBoostingRegressor__n_estimators=100, GradientBoostingRegressor__subsample=0.9000000000000001), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6000000000000001, XGBRegressor__verbosity=0)
Optimization Progress:  35%|███▌      | 900/2550 [42:51<4:15:26,  9.29s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  35%|███▌      | 900/2550 [42:56<4:15:26,  9.29s/pipeline]Optimization Progress:  35%|███▌      | 900/2550 [42:56<3:35:13,  7.83s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 [07:15:36] ../src/learner.cc:602: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x19c8d9) [0x7fdf6464d8d9]
  [bt] (1) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b1f1b) [0x7fdf64662f1b]
  [bt] (2) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b9b4d) [0x7fdf6466ab4d]
  [bt] (3) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b4559) [0x7fdf64665559]
  [bt] (4) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fdf6454a4e8]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fe03280fff5]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fe03280f40a]
  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x5b6) [0x7fe02f4c5306]
  [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x139dc) [0x7fe02f4c59dc]

.
Optimization Progress:  35%|███▌      | 900/2550 [42:57<3:35:13,  7.83s/pipeline]Optimization Progress:  35%|███▌      | 901/2550 [43:05<3:46:40,  8.25s/pipeline]Optimization Progress:  35%|███▌      | 902/2550 [43:41<7:34:59, 16.57s/pipeline]Optimization Progress:  36%|███▌      | 918/2550 [44:20<5:35:26, 12.33s/pipeline]Optimization Progress:  37%|███▋      | 934/2550 [45:12<4:18:33,  9.60s/pipeline]                                                                                 
Generation 18 - Current Pareto front scores:
Optimization Progress:  37%|███▋      | 950/2550 [45:12<4:16:00,  9.60s/pipeline]                                                                                 
-1	-4.376134425571518	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001, XGBRegressor__verbosity=0)
Optimization Progress:  37%|███▋      | 950/2550 [45:12<4:16:00,  9.60s/pipeline]                                                                                 
-2	-4.363135334809744	XGBRegressor(MaxAbsScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=7, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  37%|███▋      | 950/2550 [45:12<4:16:00,  9.60s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:  37%|███▋      | 950/2550 [45:15<4:16:00,  9.60s/pipeline]Optimization Progress:  37%|███▋      | 950/2550 [45:15<3:01:04,  6.79s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..
Optimization Progress:  37%|███▋      | 950/2550 [45:16<3:01:04,  6.79s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 [07:17:56] ../src/learner.cc:602: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x19c8d9) [0x7fdf6464d8d9]
  [bt] (1) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b1f1b) [0x7fdf64662f1b]
  [bt] (2) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b9b4d) [0x7fdf6466ab4d]
  [bt] (3) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b4559) [0x7fdf64665559]
  [bt] (4) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fdf6454a4e8]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fe03280fff5]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fe03280f40a]
  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x5b6) [0x7fe02f4c5306]
  [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x139dc) [0x7fe02f4c59dc]

.
Optimization Progress:  37%|███▋      | 950/2550 [45:17<3:01:04,  6.79s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 [07:18:01] ../src/learner.cc:602: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x19c8d9) [0x7fdf6464d8d9]
  [bt] (1) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b1f1b) [0x7fdf64662f1b]
  [bt] (2) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b9b4d) [0x7fdf6466ab4d]
  [bt] (3) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b4559) [0x7fdf64665559]
  [bt] (4) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fdf6454a4e8]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fe03280fff5]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fe03280f40a]
  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x5b6) [0x7fe02f4c5306]
  [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x139dc) [0x7fe02f4c59dc]

.
Optimization Progress:  37%|███▋      | 950/2550 [45:22<3:01:04,  6.79s/pipeline]Optimization Progress:  37%|███▋      | 951/2550 [45:26<3:30:45,  7.91s/pipeline]Optimization Progress:  37%|███▋      | 952/2550 [46:12<8:32:11, 19.23s/pipeline]Optimization Progress:  38%|███▊      | 968/2550 [46:52<6:14:52, 14.22s/pipeline]Optimization Progress:  39%|███▊      | 984/2550 [47:32<4:39:13, 10.70s/pipeline]Optimization Progress:  39%|███▉      | 1000/2550 [47:43<3:18:56,  7.70s/pipeline]                                                                                  
Generation 19 - Current Pareto front scores:
Optimization Progress:  39%|███▉      | 1000/2550 [47:43<3:18:56,  7.70s/pipeline]                                                                                  
-1	-4.33118866294321	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  39%|███▉      | 1000/2550 [47:43<3:18:56,  7.70s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress:  39%|███▉      | 1000/2550 [47:46<3:18:56,  7.70s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  39%|███▉      | 1000/2550 [47:55<3:18:56,  7.70s/pipeline]Optimization Progress:  39%|███▉      | 1001/2550 [47:58<4:15:51,  9.91s/pipeline]Optimization Progress:  39%|███▉      | 1002/2550 [49:12<12:28:18, 29.00s/pipeline]Optimization Progress:  40%|███▉      | 1018/2550 [49:57<9:00:10, 21.16s/pipeline] Optimization Progress:  41%|████      | 1034/2550 [50:45<6:36:58, 15.71s/pipeline]Optimization Progress:  41%|████      | 1050/2550 [50:55<4:39:37, 11.19s/pipeline]                                                                                  
Generation 20 - Current Pareto front scores:
Optimization Progress:  41%|████      | 1050/2550 [50:55<4:39:37, 11.19s/pipeline]                                                                                  
-1	-4.33118866294321	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=8, XGBRegressor__min_child_weight=1, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  41%|████      | 1050/2550 [50:55<4:39:37, 11.19s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  41%|████      | 1050/2550 [51:02<4:39:37, 11.19s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [07:23:43] ../src/learner.cc:602: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x19c8d9) [0x7fdf6464d8d9]
  [bt] (1) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b1f1b) [0x7fdf64662f1b]
  [bt] (2) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b9b4d) [0x7fdf6466ab4d]
  [bt] (3) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b4559) [0x7fdf64665559]
  [bt] (4) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fdf6454a4e8]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fe03280fff5]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fe03280f40a]
  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x5b6) [0x7fe02f4c5306]
  [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x139dc) [0x7fe02f4c59dc]

.
Optimization Progress:  41%|████      | 1050/2550 [51:04<4:39:37, 11.19s/pipeline]Optimization Progress:  41%|████      | 1051/2550 [51:59<11:11:51, 26.89s/pipeline]Optimization Progress:  42%|████▏     | 1067/2550 [52:38<8:03:35, 19.57s/pipeline] Optimization Progress:  42%|████▏     | 1083/2550 [53:47<6:06:10, 14.98s/pipeline]Optimization Progress:  43%|████▎     | 1099/2550 [53:58<4:18:45, 10.70s/pipeline]                                                                                  
Generation 21 - Current Pareto front scores:
Optimization Progress:  43%|████▎     | 1100/2550 [53:58<4:18:34, 10.70s/pipeline]                                                                                  
-1	-4.306398912850523	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  43%|████▎     | 1100/2550 [53:58<4:18:34, 10.70s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56.
Optimization Progress:  43%|████▎     | 1100/2550 [54:02<4:18:34, 10.70s/pipeline]Optimization Progress:  43%|████▎     | 1100/2550 [54:02<3:31:44,  8.76s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  43%|████▎     | 1100/2550 [54:15<3:31:44,  8.76s/pipeline]Optimization Progress:  43%|████▎     | 1101/2550 [54:15<3:55:54,  9.77s/pipeline]Optimization Progress:  43%|████▎     | 1102/2550 [55:03<8:34:43, 21.33s/pipeline]Optimization Progress:  44%|████▍     | 1118/2550 [56:07<6:25:08, 16.14s/pipeline]Optimization Progress:  44%|████▍     | 1134/2550 [57:24<5:00:32, 12.73s/pipeline]Optimization Progress:  45%|████▌     | 1150/2550 [57:34<3:32:34,  9.11s/pipeline]                                                                                  
Generation 22 - Current Pareto front scores:
Optimization Progress:  45%|████▌     | 1150/2550 [57:34<3:32:34,  9.11s/pipeline]                                                                                  
-1	-4.306398912850523	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  45%|████▌     | 1150/2550 [57:34<3:32:34,  9.11s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.
Optimization Progress:  45%|████▌     | 1150/2550 [57:42<3:32:34,  9.11s/pipeline]Optimization Progress:  45%|████▌     | 1151/2550 [57:50<4:14:57, 10.93s/pipeline]Optimization Progress:  45%|████▌     | 1152/2550 [58:29<7:36:54, 19.61s/pipeline]Optimization Progress:  46%|████▌     | 1168/2550 [59:34<5:44:09, 14.94s/pipeline]Optimization Progress:  46%|████▋     | 1184/2550 [1:00:35<4:24:02, 11.60s/pipeline]Optimization Progress:  47%|████▋     | 1200/2550 [1:00:45<3:06:52,  8.31s/pipeline]                                                                                    
Generation 23 - Current Pareto front scores:
Optimization Progress:  47%|████▋     | 1200/2550 [1:00:45<3:06:52,  8.31s/pipeline]                                                                                    
-1	-4.306398912850523	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  47%|████▋     | 1200/2550 [1:00:45<3:06:52,  8.31s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..
Optimization Progress:  47%|████▋     | 1200/2550 [1:00:46<3:06:52,  8.31s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 [07:33:32] ../src/learner.cc:602: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x19c8d9) [0x7fdf6464d8d9]
  [bt] (1) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b1f1b) [0x7fdf64662f1b]
  [bt] (2) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b9b4d) [0x7fdf6466ab4d]
  [bt] (3) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b4559) [0x7fdf64665559]
  [bt] (4) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fdf6454a4e8]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fe03280fff5]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fe03280f40a]
  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x5b6) [0x7fe02f4c5306]
  [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x139dc) [0x7fe02f4c59dc]

.
Optimization Progress:  47%|████▋     | 1200/2550 [1:00:52<3:06:52,  8.31s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 [07:33:33] ../src/learner.cc:602: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x19c8d9) [0x7fdf6464d8d9]
  [bt] (1) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b1f1b) [0x7fdf64662f1b]
  [bt] (2) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b9b4d) [0x7fdf6466ab4d]
  [bt] (3) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b4559) [0x7fdf64665559]
  [bt] (4) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fdf6454a4e8]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fe03280fff5]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fe03280f40a]
  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x5b6) [0x7fe02f4c5306]
  [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x139dc) [0x7fe02f4c59dc]

.
Optimization Progress:  47%|████▋     | 1200/2550 [1:00:53<3:06:52,  8.31s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 [07:33:33] ../src/learner.cc:602: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x19c8d9) [0x7fdf6464d8d9]
  [bt] (1) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b1f1b) [0x7fdf64662f1b]
  [bt] (2) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b9b4d) [0x7fdf6466ab4d]
  [bt] (3) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b4559) [0x7fdf64665559]
  [bt] (4) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fdf6454a4e8]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fe03280fff5]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fe03280f40a]
  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x5b6) [0x7fe02f4c5306]
  [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x139dc) [0x7fe02f4c59dc]

.
Optimization Progress:  47%|████▋     | 1200/2550 [1:00:54<3:06:52,  8.31s/pipeline]Optimization Progress:  47%|████▋     | 1201/2550 [1:01:01<3:57:37, 10.57s/pipeline]                                                                                    Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  47%|████▋     | 1201/2550 [1:01:01<3:57:37, 10.57s/pipeline]                                                                                    Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  47%|████▋     | 1202/2550 [1:01:01<3:57:26, 10.57s/pipeline]Optimization Progress:  47%|████▋     | 1204/2550 [1:01:51<4:37:47, 12.38s/pipeline]Optimization Progress:  48%|████▊     | 1220/2550 [1:02:42<3:33:26,  9.63s/pipeline]Optimization Progress:  48%|████▊     | 1236/2550 [1:03:23<2:44:41,  7.52s/pipeline]                                                                                    
Generation 24 - Current Pareto front scores:
Optimization Progress:  49%|████▉     | 1250/2550 [1:03:23<2:42:55,  7.52s/pipeline]                                                                                    
-1	-4.306398912850523	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  49%|████▉     | 1250/2550 [1:03:23<2:42:55,  7.52s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:  49%|████▉     | 1250/2550 [1:03:24<2:42:55,  7.52s/pipeline]Optimization Progress:  49%|████▉     | 1250/2550 [1:03:24<1:54:10,  5.27s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..
Optimization Progress:  49%|████▉     | 1250/2550 [1:03:28<1:54:10,  5.27s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  49%|████▉     | 1250/2550 [1:03:33<1:54:10,  5.27s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.
Optimization Progress:  49%|████▉     | 1250/2550 [1:03:35<1:54:10,  5.27s/pipeline]Optimization Progress:  49%|████▉     | 1251/2550 [1:03:41<1:54:05,  5.27s/pipeline]Optimization Progress:  49%|████▉     | 1252/2550 [1:04:24<4:36:19, 12.77s/pipeline]Optimization Progress:  50%|████▉     | 1268/2550 [1:05:07<3:28:07,  9.74s/pipeline]Optimization Progress:  50%|█████     | 1284/2550 [1:05:53<2:42:01,  7.68s/pipeline]Optimization Progress:  51%|█████     | 1300/2550 [1:06:04<1:56:14,  5.58s/pipeline]                                                                                    
Generation 25 - Current Pareto front scores:
Optimization Progress:  51%|█████     | 1300/2550 [1:06:04<1:56:14,  5.58s/pipeline]                                                                                    
-1	-4.306398912850523	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  51%|█████     | 1300/2550 [1:06:04<1:56:14,  5.58s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.
Optimization Progress:  51%|█████     | 1300/2550 [1:06:09<1:56:14,  5.58s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 [07:38:48] ../src/learner.cc:602: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x19c8d9) [0x7fdf6464d8d9]
  [bt] (1) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b1f1b) [0x7fdf64662f1b]
  [bt] (2) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b9b4d) [0x7fdf6466ab4d]
  [bt] (3) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b4559) [0x7fdf64665559]
  [bt] (4) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fdf6454a4e8]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fe03280fff5]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fe03280f40a]
  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x5b6) [0x7fe02f4c5306]
  [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x139dc) [0x7fe02f4c59dc]

.
Optimization Progress:  51%|█████     | 1300/2550 [1:06:09<1:56:14,  5.58s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  51%|█████     | 1300/2550 [1:06:17<1:56:14,  5.58s/pipeline]Optimization Progress:  51%|█████     | 1301/2550 [1:06:18<2:53:18,  8.33s/pipeline]Optimization Progress:  51%|█████     | 1302/2550 [1:07:07<7:03:42, 20.37s/pipeline]Optimization Progress:  52%|█████▏    | 1318/2550 [1:07:57<5:11:53, 15.19s/pipeline]Optimization Progress:  52%|█████▏    | 1334/2550 [1:08:39<3:51:43, 11.43s/pipeline]Optimization Progress:  53%|█████▎    | 1350/2550 [1:08:51<2:44:34,  8.23s/pipeline]                                                                                    
Generation 26 - Current Pareto front scores:
Optimization Progress:  53%|█████▎    | 1350/2550 [1:08:51<2:44:34,  8.23s/pipeline]                                                                                    
-1	-4.306398912850523	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  53%|█████▎    | 1350/2550 [1:08:51<2:44:34,  8.23s/pipeline]                                                                                    
-4	-4.296536888464838	XGBRegressor(VarianceThreshold(StandardScaler(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.005)), VarianceThreshold__threshold=0.0001), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  53%|█████▎    | 1350/2550 [1:08:51<2:44:34,  8.23s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  53%|█████▎    | 1350/2550 [1:09:03<2:44:34,  8.23s/pipeline]Optimization Progress:  53%|█████▎    | 1351/2550 [1:09:06<3:25:34, 10.29s/pipeline]                                                                                    Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  53%|█████▎    | 1351/2550 [1:09:06<3:25:34, 10.29s/pipeline]Optimization Progress:  53%|█████▎    | 1353/2550 [1:09:56<4:53:32, 14.71s/pipeline]Optimization Progress:  54%|█████▎    | 1369/2550 [1:10:34<3:36:35, 11.00s/pipeline]Optimization Progress:  54%|█████▍    | 1385/2550 [1:11:30<2:50:02,  8.76s/pipeline]                                                                                    
Generation 27 - Current Pareto front scores:
Optimization Progress:  55%|█████▍    | 1400/2550 [1:11:30<2:47:51,  8.76s/pipeline]                                                                                    
-1	-4.306398912850523	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  55%|█████▍    | 1400/2550 [1:11:30<2:47:51,  8.76s/pipeline]                                                                                    
-3	-4.296536888464838	XGBRegressor(StandardScaler(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.005)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  55%|█████▍    | 1400/2550 [1:11:30<2:47:51,  8.76s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 [07:44:13] ../src/learner.cc:602: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x19c8d9) [0x7fdf6464d8d9]
  [bt] (1) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b1f1b) [0x7fdf64662f1b]
  [bt] (2) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b9b4d) [0x7fdf6466ab4d]
  [bt] (3) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b4559) [0x7fdf64665559]
  [bt] (4) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fdf6454a4e8]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fe03280fff5]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fe03280f40a]
  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x5b6) [0x7fe02f4c5306]
  [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x139dc) [0x7fe02f4c59dc]

.
Optimization Progress:  55%|█████▍    | 1400/2550 [1:11:33<2:47:51,  8.76s/pipeline]Optimization Progress:  55%|█████▍    | 1400/2550 [1:11:33<1:58:41,  6.19s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.
Optimization Progress:  55%|█████▍    | 1400/2550 [1:11:42<1:58:41,  6.19s/pipeline]Optimization Progress:  55%|█████▍    | 1401/2550 [1:12:48<8:29:32, 26.61s/pipeline]Optimization Progress:  56%|█████▌    | 1417/2550 [1:13:36<6:08:51, 19.53s/pipeline]Optimization Progress:  56%|█████▌    | 1433/2550 [1:14:40<4:36:57, 14.88s/pipeline]Optimization Progress:  57%|█████▋    | 1449/2550 [1:14:51<3:14:54, 10.62s/pipeline]                                                                                    
Generation 28 - Current Pareto front scores:
Optimization Progress:  57%|█████▋    | 1450/2550 [1:14:51<3:14:44, 10.62s/pipeline]                                                                                    
-1	-4.306398912850523	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  57%|█████▋    | 1450/2550 [1:14:51<3:14:44, 10.62s/pipeline]                                                                                    
-3	-4.296536888464838	XGBRegressor(StandardScaler(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.005)), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  57%|█████▋    | 1450/2550 [1:14:51<3:14:44, 10.62s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 [07:47:35] ../src/learner.cc:602: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x19c8d9) [0x7fdf6464d8d9]
  [bt] (1) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b1f1b) [0x7fdf64662f1b]
  [bt] (2) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b9b4d) [0x7fdf6466ab4d]
  [bt] (3) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b4559) [0x7fdf64665559]
  [bt] (4) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fdf6454a4e8]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fe03280fff5]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fe03280f40a]
  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x5b6) [0x7fe02f4c5306]
  [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x139dc) [0x7fe02f4c59dc]

.
Optimization Progress:  57%|█████▋    | 1450/2550 [1:14:56<3:14:44, 10.62s/pipeline]Optimization Progress:  57%|█████▋    | 1450/2550 [1:14:56<2:40:13,  8.74s/pipeline]Optimization Progress:  57%|█████▋    | 1453/2550 [1:15:05<2:09:25,  7.08s/pipeline]Optimization Progress:  57%|█████▋    | 1454/2550 [1:15:40<4:39:03, 15.28s/pipeline]Optimization Progress:  58%|█████▊    | 1470/2550 [1:16:24<3:27:21, 11.52s/pipeline]Optimization Progress:  58%|█████▊    | 1486/2550 [1:17:01<2:35:19,  8.76s/pipeline]                                                                                    
Generation 29 - Current Pareto front scores:
Optimization Progress:  59%|█████▉    | 1500/2550 [1:17:01<2:33:17,  8.76s/pipeline]                                                                                    
-1	-4.306398912850523	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  59%|█████▉    | 1500/2550 [1:17:01<2:33:17,  8.76s/pipeline]                                                                                    
-2	-4.296536888464838	XGBRegressor(StandardScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  59%|█████▉    | 1500/2550 [1:17:01<2:33:17,  8.76s/pipeline]                                                                                    
-3	-4.276124509274096	XGBRegressor(SGDRegressor(ZeroCount(input_matrix), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001, XGBRegressor__verbosity=0)
Optimization Progress:  59%|█████▉    | 1500/2550 [1:17:01<2:33:17,  8.76s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  59%|█████▉    | 1500/2550 [1:17:13<2:33:17,  8.76s/pipeline]Optimization Progress:  59%|█████▉    | 1500/2550 [1:17:13<1:51:52,  6.39s/pipeline]Optimization Progress:  59%|█████▉    | 1501/2550 [1:17:17<1:40:57,  5.77s/pipeline]Optimization Progress:  59%|█████▉    | 1502/2550 [1:17:56<4:31:15, 15.53s/pipeline]Optimization Progress:  60%|█████▉    | 1518/2550 [1:19:04<3:28:52, 12.14s/pipeline]Optimization Progress:  60%|██████    | 1534/2550 [1:20:14<2:46:23,  9.83s/pipeline]Optimization Progress:  61%|██████    | 1550/2550 [1:20:26<1:58:15,  7.10s/pipeline]                                                                                    
Generation 30 - Current Pareto front scores:
Optimization Progress:  61%|██████    | 1550/2550 [1:20:26<1:58:15,  7.10s/pipeline]                                                                                    
-1	-4.306398912850523	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  61%|██████    | 1550/2550 [1:20:26<1:58:15,  7.10s/pipeline]                                                                                    
-2	-4.296536888464838	XGBRegressor(StandardScaler(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=3, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  61%|██████    | 1550/2550 [1:20:26<1:58:15,  7.10s/pipeline]                                                                                    
-3	-4.276124509274096	XGBRegressor(SGDRegressor(ZeroCount(input_matrix), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=1.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001, XGBRegressor__verbosity=0)
Optimization Progress:  61%|██████    | 1550/2550 [1:20:26<1:58:15,  7.10s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  61%|██████    | 1550/2550 [1:20:29<1:58:15,  7.10s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=1 [07:53:09] ../src/learner.cc:602: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x19c8d9) [0x7fdf6464d8d9]
  [bt] (1) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b1f1b) [0x7fdf64662f1b]
  [bt] (2) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b9b4d) [0x7fdf6466ab4d]
  [bt] (3) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b4559) [0x7fdf64665559]
  [bt] (4) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fdf6454a4e8]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fe03280fff5]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fe03280f40a]
  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x5b6) [0x7fe02f4c5306]
  [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x139dc) [0x7fe02f4c59dc]

.
Optimization Progress:  61%|██████    | 1550/2550 [1:20:29<1:58:15,  7.10s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.
Optimization Progress:  61%|██████    | 1550/2550 [1:20:39<1:58:15,  7.10s/pipeline]Optimization Progress:  61%|██████    | 1551/2550 [1:21:29<6:40:33, 24.06s/pipeline]Optimization Progress:  61%|██████▏   | 1567/2550 [1:22:47<4:59:48, 18.30s/pipeline]Optimization Progress:  62%|██████▏   | 1583/2550 [1:23:44<3:43:43, 13.88s/pipeline]Optimization Progress:  63%|██████▎   | 1599/2550 [1:23:55<2:37:03,  9.91s/pipeline]                                                                                    
Generation 31 - Current Pareto front scores:
Optimization Progress:  63%|██████▎   | 1600/2550 [1:23:55<2:36:53,  9.91s/pipeline]                                                                                    
-1	-4.275815367546219	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  63%|██████▎   | 1600/2550 [1:23:55<2:36:53,  9.91s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 [07:56:42] ../src/learner.cc:602: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x19c8d9) [0x7fdf6464d8d9]
  [bt] (1) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b1f1b) [0x7fdf64662f1b]
  [bt] (2) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b9b4d) [0x7fdf6466ab4d]
  [bt] (3) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b4559) [0x7fdf64665559]
  [bt] (4) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fdf6454a4e8]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fe03280fff5]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fe03280f40a]
  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x5b6) [0x7fe02f4c5306]
  [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x139dc) [0x7fe02f4c59dc]

.
Optimization Progress:  63%|██████▎   | 1600/2550 [1:24:02<2:36:53,  9.91s/pipeline]Optimization Progress:  63%|██████▎   | 1600/2550 [1:24:02<2:26:17,  9.24s/pipeline]Optimization Progress:  63%|██████▎   | 1601/2550 [1:24:08<2:10:45,  8.27s/pipeline]Optimization Progress:  63%|██████▎   | 1602/2550 [1:24:46<4:30:47, 17.14s/pipeline]Optimization Progress:  63%|██████▎   | 1618/2550 [1:25:54<3:26:03, 13.27s/pipeline]Optimization Progress:  64%|██████▍   | 1634/2550 [1:26:56<2:39:31, 10.45s/pipeline]Optimization Progress:  65%|██████▍   | 1650/2550 [1:27:07<1:52:47,  7.52s/pipeline]                                                                                    
Generation 32 - Current Pareto front scores:
Optimization Progress:  65%|██████▍   | 1650/2550 [1:27:07<1:52:47,  7.52s/pipeline]                                                                                    
-1	-4.275815367546219	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  65%|██████▍   | 1650/2550 [1:27:07<1:52:47,  7.52s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:  65%|██████▍   | 1650/2550 [1:27:09<1:52:47,  7.52s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 [07:59:51] ../src/learner.cc:602: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x19c8d9) [0x7fdf6464d8d9]
  [bt] (1) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b1f1b) [0x7fdf64662f1b]
  [bt] (2) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b9b4d) [0x7fdf6466ab4d]
  [bt] (3) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b4559) [0x7fdf64665559]
  [bt] (4) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fdf6454a4e8]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fe03280fff5]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fe03280f40a]
  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x5b6) [0x7fe02f4c5306]
  [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x139dc) [0x7fe02f4c59dc]

.
Optimization Progress:  65%|██████▍   | 1650/2550 [1:27:11<1:52:47,  7.52s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  65%|██████▍   | 1650/2550 [1:27:16<1:52:47,  7.52s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  65%|██████▍   | 1650/2550 [1:27:17<1:52:47,  7.52s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 [07:59:58] ../src/learner.cc:602: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x19c8d9) [0x7fdf6464d8d9]
  [bt] (1) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b1f1b) [0x7fdf64662f1b]
  [bt] (2) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b9b4d) [0x7fdf6466ab4d]
  [bt] (3) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b4559) [0x7fdf64665559]
  [bt] (4) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fdf6454a4e8]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fe03280fff5]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fe03280f40a]
  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x5b6) [0x7fe02f4c5306]
  [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x139dc) [0x7fe02f4c59dc]

.
Optimization Progress:  65%|██████▍   | 1650/2550 [1:27:19<1:52:47,  7.52s/pipeline]                                                                                    Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  65%|██████▍   | 1650/2550 [1:27:23<1:52:47,  7.52s/pipeline]Optimization Progress:  65%|██████▍   | 1651/2550 [1:27:23<2:32:47, 10.20s/pipeline]Optimization Progress:  65%|██████▍   | 1652/2550 [1:28:17<5:47:41, 23.23s/pipeline]Optimization Progress:  65%|██████▌   | 1668/2550 [1:29:01<4:11:10, 17.09s/pipeline]Optimization Progress:  66%|██████▌   | 1684/2550 [1:29:48<3:05:24, 12.85s/pipeline]Optimization Progress:  67%|██████▋   | 1700/2550 [1:29:59<2:10:21,  9.20s/pipeline]                                                                                    
Generation 33 - Current Pareto front scores:
Optimization Progress:  67%|██████▋   | 1700/2550 [1:29:59<2:10:21,  9.20s/pipeline]                                                                                    
-1	-4.275815367546219	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  67%|██████▋   | 1700/2550 [1:29:59<2:10:21,  9.20s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.
Optimization Progress:  67%|██████▋   | 1700/2550 [1:30:02<2:10:21,  9.20s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.00500.
Optimization Progress:  67%|██████▋   | 1700/2550 [1:30:06<2:10:21,  9.20s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.00100.
Optimization Progress:  67%|██████▋   | 1700/2550 [1:30:14<2:10:21,  9.20s/pipeline]Optimization Progress:  67%|██████▋   | 1702/2550 [1:30:15<2:03:25,  8.73s/pipeline]Optimization Progress:  67%|██████▋   | 1703/2550 [1:31:00<4:36:48, 19.61s/pipeline]Optimization Progress:  67%|██████▋   | 1719/2550 [1:31:43<3:21:25, 14.54s/pipeline]Optimization Progress:  68%|██████▊   | 1735/2550 [1:32:23<2:28:33, 10.94s/pipeline]                                                                                    
Generation 34 - Current Pareto front scores:
Optimization Progress:  69%|██████▊   | 1750/2550 [1:32:23<2:25:49, 10.94s/pipeline]                                                                                    
-1	-4.275815367546219	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  69%|██████▊   | 1750/2550 [1:32:23<2:25:49, 10.94s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.
Optimization Progress:  69%|██████▊   | 1750/2550 [1:32:29<2:25:49, 10.94s/pipeline]Optimization Progress:  69%|██████▊   | 1750/2550 [1:32:29<1:43:29,  7.76s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 [08:05:12] ../src/learner.cc:602: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x19c8d9) [0x7fdf6464d8d9]
  [bt] (1) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b1f1b) [0x7fdf64662f1b]
  [bt] (2) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b9b4d) [0x7fdf6466ab4d]
  [bt] (3) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b4559) [0x7fdf64665559]
  [bt] (4) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fdf6454a4e8]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fe03280fff5]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fe03280f40a]
  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x5b6) [0x7fe02f4c5306]
  [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x139dc) [0x7fe02f4c59dc]

.
Optimization Progress:  69%|██████▊   | 1750/2550 [1:32:32<1:43:29,  7.76s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.
Optimization Progress:  69%|██████▊   | 1750/2550 [1:32:38<1:43:29,  7.76s/pipeline]                                                                                    Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  69%|██████▊   | 1750/2550 [1:32:40<1:43:29,  7.76s/pipeline]Optimization Progress:  69%|██████▊   | 1751/2550 [1:32:40<1:57:45,  8.84s/pipeline]Optimization Progress:  69%|██████▊   | 1752/2550 [1:33:23<4:11:23, 18.90s/pipeline]Optimization Progress:  69%|██████▉   | 1768/2550 [1:34:04<3:02:28, 14.00s/pipeline]Optimization Progress:  70%|██████▉   | 1784/2550 [1:35:11<2:21:18, 11.07s/pipeline]Optimization Progress:  71%|███████   | 1800/2550 [1:35:20<1:39:00,  7.92s/pipeline]                                                                                    
Generation 35 - Current Pareto front scores:
Optimization Progress:  71%|███████   | 1800/2550 [1:35:20<1:39:00,  7.92s/pipeline]                                                                                    
-1	-4.275815367546219	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  71%|███████   | 1800/2550 [1:35:20<1:39:00,  7.92s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..
Optimization Progress:  71%|███████   | 1800/2550 [1:35:22<1:39:00,  7.92s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.
Optimization Progress:  71%|███████   | 1800/2550 [1:35:23<1:39:00,  7.92s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 [08:08:06] ../src/learner.cc:602: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x19c8d9) [0x7fdf6464d8d9]
  [bt] (1) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b1f1b) [0x7fdf64662f1b]
  [bt] (2) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b9b4d) [0x7fdf6466ab4d]
  [bt] (3) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b4559) [0x7fdf64665559]
  [bt] (4) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fdf6454a4e8]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fe03280fff5]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fe03280f40a]
  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x5b6) [0x7fe02f4c5306]
  [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x139dc) [0x7fe02f4c59dc]

.
Optimization Progress:  71%|███████   | 1800/2550 [1:35:27<1:39:00,  7.92s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  71%|███████   | 1800/2550 [1:35:35<1:39:00,  7.92s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 [08:08:16] ../src/learner.cc:602: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x19c8d9) [0x7fdf6464d8d9]
  [bt] (1) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b1f1b) [0x7fdf64662f1b]
  [bt] (2) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b9b4d) [0x7fdf6466ab4d]
  [bt] (3) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b4559) [0x7fdf64665559]
  [bt] (4) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fdf6454a4e8]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fe03280fff5]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fe03280f40a]
  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x5b6) [0x7fe02f4c5306]
  [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x139dc) [0x7fe02f4c59dc]

.
Optimization Progress:  71%|███████   | 1800/2550 [1:35:36<1:39:00,  7.92s/pipeline]Optimization Progress:  71%|███████   | 1801/2550 [1:36:22<5:00:01, 24.03s/pipeline]Optimization Progress:  71%|███████▏  | 1817/2550 [1:37:13<3:37:14, 17.78s/pipeline]Optimization Progress:  72%|███████▏  | 1833/2550 [1:38:16<2:42:46, 13.62s/pipeline]Optimization Progress:  73%|███████▎  | 1849/2550 [1:38:27<1:53:45,  9.74s/pipeline]                                                                                    
Generation 36 - Current Pareto front scores:
Optimization Progress:  73%|███████▎  | 1850/2550 [1:38:27<1:53:35,  9.74s/pipeline]                                                                                    
-1	-4.275815367546219	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  73%|███████▎  | 1850/2550 [1:38:27<1:53:35,  9.74s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  73%|███████▎  | 1850/2550 [1:38:30<1:53:35,  9.74s/pipeline]Optimization Progress:  73%|███████▎  | 1850/2550 [1:38:30<1:31:40,  7.86s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..
Optimization Progress:  73%|███████▎  | 1850/2550 [1:38:32<1:31:40,  7.86s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55.
Optimization Progress:  73%|███████▎  | 1850/2550 [1:38:36<1:31:40,  7.86s/pipeline]Optimization Progress:  73%|███████▎  | 1851/2550 [1:39:25<4:16:00, 21.98s/pipeline]Optimization Progress:  73%|███████▎  | 1867/2550 [1:39:59<3:02:18, 16.01s/pipeline]Optimization Progress:  74%|███████▍  | 1883/2550 [1:41:05<2:18:23, 12.45s/pipeline]Optimization Progress:  74%|███████▍  | 1899/2550 [1:41:19<1:37:31,  8.99s/pipeline]                                                                                    
Generation 37 - Current Pareto front scores:
Optimization Progress:  75%|███████▍  | 1900/2550 [1:41:19<1:37:22,  8.99s/pipeline]                                                                                    
-1	-4.275815367546219	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  75%|███████▍  | 1900/2550 [1:41:19<1:37:22,  8.99s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  75%|███████▍  | 1900/2550 [1:41:24<1:37:22,  8.99s/pipeline]Optimization Progress:  75%|███████▍  | 1900/2550 [1:41:24<1:22:03,  7.57s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:  75%|███████▍  | 1900/2550 [1:41:28<1:22:03,  7.57s/pipeline]Optimization Progress:  75%|███████▍  | 1902/2550 [1:41:36<1:16:56,  7.12s/pipeline]                                                                                    Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  75%|███████▍  | 1902/2550 [1:41:36<1:16:56,  7.12s/pipeline]Optimization Progress:  75%|███████▍  | 1904/2550 [1:42:41<2:39:10, 14.78s/pipeline]Optimization Progress:  75%|███████▌  | 1920/2550 [1:43:35<1:59:15, 11.36s/pipeline]Optimization Progress:  76%|███████▌  | 1936/2550 [1:44:50<1:35:51,  9.37s/pipeline]                                                                                    
Generation 38 - Current Pareto front scores:
Optimization Progress:  76%|███████▋  | 1950/2550 [1:44:50<1:33:40,  9.37s/pipeline]                                                                                    
-1	-4.275815367546219	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  76%|███████▋  | 1950/2550 [1:44:50<1:33:40,  9.37s/pipeline]                                                                                    
-3	-4.26828901164139	XGBRegressor(SGDRegressor(CombineDFs(ZeroCount(input_matrix), input_matrix), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001, XGBRegressor__verbosity=0)
Optimization Progress:  76%|███████▋  | 1950/2550 [1:44:50<1:33:40,  9.37s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:  76%|███████▋  | 1950/2550 [1:44:57<1:33:40,  9.37s/pipeline]Optimization Progress:  76%|███████▋  | 1950/2550 [1:44:57<1:06:56,  6.69s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  76%|███████▋  | 1950/2550 [1:44:57<1:06:56,  6.69s/pipeline]                                                                                    Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  76%|███████▋  | 1950/2550 [1:45:06<1:06:56,  6.69s/pipeline]Optimization Progress:  77%|███████▋  | 1951/2550 [1:45:06<1:14:47,  7.49s/pipeline]                                                                                    Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  77%|███████▋  | 1951/2550 [1:45:06<1:14:47,  7.49s/pipeline]Optimization Progress:  77%|███████▋  | 1953/2550 [1:46:03<2:17:08, 13.78s/pipeline]Optimization Progress:  77%|███████▋  | 1969/2550 [1:47:05<1:44:34, 10.80s/pipeline]Optimization Progress:  78%|███████▊  | 1985/2550 [1:48:00<1:21:02,  8.61s/pipeline]                                                                                    
Generation 39 - Current Pareto front scores:
Optimization Progress:  78%|███████▊  | 2000/2550 [1:48:00<1:18:53,  8.61s/pipeline]                                                                                    
-1	-4.275815367546219	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  78%|███████▊  | 2000/2550 [1:48:00<1:18:53,  8.61s/pipeline]                                                                                    
-3	-4.26828901164139	XGBRegressor(SGDRegressor(CombineDFs(ZeroCount(input_matrix), input_matrix), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001, XGBRegressor__verbosity=0)
Optimization Progress:  78%|███████▊  | 2000/2550 [1:48:00<1:18:53,  8.61s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  78%|███████▊  | 2000/2550 [1:48:09<1:18:53,  8.61s/pipeline]Optimization Progress:  78%|███████▊  | 2000/2550 [1:48:09<56:45,  6.19s/pipeline]  Optimization Progress:  78%|███████▊  | 2001/2550 [1:49:06<3:16:20, 21.46s/pipeline]Optimization Progress:  79%|███████▉  | 2017/2550 [1:50:02<2:22:47, 16.07s/pipeline]Optimization Progress:  80%|███████▉  | 2033/2550 [1:50:52<1:45:03, 12.19s/pipeline]Optimization Progress:  80%|████████  | 2049/2550 [1:51:04<1:13:05,  8.75s/pipeline]                                                                                    
Generation 40 - Current Pareto front scores:
Optimization Progress:  80%|████████  | 2050/2550 [1:51:04<1:12:56,  8.75s/pipeline]                                                                                    
-1	-4.275815367546219	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  80%|████████  | 2050/2550 [1:51:04<1:12:56,  8.75s/pipeline]                                                                                    
-3	-4.26828901164139	XGBRegressor(SGDRegressor(CombineDFs(ZeroCount(input_matrix), input_matrix), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001, XGBRegressor__verbosity=0)
Optimization Progress:  80%|████████  | 2050/2550 [1:51:04<1:12:56,  8.75s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  80%|████████  | 2050/2550 [1:51:11<1:12:56,  8.75s/pipeline]Optimization Progress:  80%|████████  | 2050/2550 [1:51:11<1:09:01,  8.28s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:  80%|████████  | 2050/2550 [1:51:11<1:09:01,  8.28s/pipeline]                                                                                    _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  80%|████████  | 2050/2550 [1:51:14<1:09:01,  8.28s/pipeline]Optimization Progress:  80%|████████  | 2051/2550 [1:51:18<1:05:51,  7.92s/pipeline]Optimization Progress:  80%|████████  | 2052/2550 [1:51:56<2:20:29, 16.93s/pipeline]Optimization Progress:  81%|████████  | 2068/2550 [1:52:47<1:42:48, 12.80s/pipeline]Optimization Progress:  82%|████████▏ | 2084/2550 [1:53:32<1:16:07,  9.80s/pipeline]Optimization Progress:  82%|████████▏ | 2100/2550 [1:53:33<51:37,  6.88s/pipeline]                                                                                    
Generation 41 - Current Pareto front scores:
Optimization Progress:  82%|████████▏ | 2100/2550 [1:53:33<51:37,  6.88s/pipeline]                                                                                  
-1	-4.275815367546219	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  82%|████████▏ | 2100/2550 [1:53:33<51:37,  6.88s/pipeline]                                                                                  
-3	-4.26828901164139	XGBRegressor(SGDRegressor(CombineDFs(ZeroCount(input_matrix), input_matrix), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001, XGBRegressor__verbosity=0)
Optimization Progress:  82%|████████▏ | 2100/2550 [1:53:33<51:37,  6.88s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  82%|████████▏ | 2100/2550 [1:53:36<51:37,  6.88s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [08:26:21] ../src/learner.cc:602: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x19c8d9) [0x7fdf6464d8d9]
  [bt] (1) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b1f1b) [0x7fdf64662f1b]
  [bt] (2) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b9b4d) [0x7fdf6466ab4d]
  [bt] (3) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b4559) [0x7fdf64665559]
  [bt] (4) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fdf6454a4e8]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fe03280fff5]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fe03280f40a]
  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x5b6) [0x7fe02f4c5306]
  [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x139dc) [0x7fe02f4c59dc]

.
Optimization Progress:  82%|████████▏ | 2100/2550 [1:53:42<51:37,  6.88s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.
Optimization Progress:  82%|████████▏ | 2100/2550 [1:53:46<51:37,  6.88s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  82%|████████▏ | 2100/2550 [1:53:49<51:37,  6.88s/pipeline]Optimization Progress:  82%|████████▏ | 2101/2550 [1:53:49<1:12:04,  9.63s/pipeline]Optimization Progress:  82%|████████▏ | 2102/2550 [1:54:31<2:24:47, 19.39s/pipeline]Optimization Progress:  83%|████████▎ | 2118/2550 [1:55:20<1:44:19, 14.49s/pipeline]Optimization Progress:  84%|████████▎ | 2134/2550 [1:56:08<1:16:36, 11.05s/pipeline]Optimization Progress:  84%|████████▍ | 2150/2550 [1:56:18<52:49,  7.92s/pipeline]                                                                                    
Generation 42 - Current Pareto front scores:
Optimization Progress:  84%|████████▍ | 2150/2550 [1:56:18<52:49,  7.92s/pipeline]                                                                                  
-1	-4.275815367546219	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  84%|████████▍ | 2150/2550 [1:56:18<52:49,  7.92s/pipeline]                                                                                  
-3	-4.26828901164139	XGBRegressor(SGDRegressor(CombineDFs(ZeroCount(input_matrix), input_matrix), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001, XGBRegressor__verbosity=0)
Optimization Progress:  84%|████████▍ | 2150/2550 [1:56:18<52:49,  7.92s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [08:29:08] ../src/learner.cc:602: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x19c8d9) [0x7fdf6464d8d9]
  [bt] (1) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b1f1b) [0x7fdf64662f1b]
  [bt] (2) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b9b4d) [0x7fdf6466ab4d]
  [bt] (3) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b4559) [0x7fdf64665559]
  [bt] (4) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fdf6454a4e8]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fe03280fff5]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fe03280f40a]
  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x5b6) [0x7fe02f4c5306]
  [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x139dc) [0x7fe02f4c59dc]

.
Optimization Progress:  84%|████████▍ | 2150/2550 [1:56:29<52:49,  7.92s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  84%|████████▍ | 2150/2550 [1:56:32<52:49,  7.92s/pipeline]Optimization Progress:  84%|████████▍ | 2151/2550 [1:56:32<1:03:53,  9.61s/pipeline]                                                                                    Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  84%|████████▍ | 2151/2550 [1:56:32<1:03:53,  9.61s/pipeline]                                                                                    Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  84%|████████▍ | 2152/2550 [1:56:32<1:03:43,  9.61s/pipeline]Optimization Progress:  84%|████████▍ | 2154/2550 [1:57:28<1:21:40, 12.38s/pipeline]Optimization Progress:  85%|████████▌ | 2170/2550 [1:58:08<59:33,  9.40s/pipeline]  Optimization Progress:  86%|████████▌ | 2186/2550 [1:58:51<44:53,  7.40s/pipeline]                                                                                  
Generation 43 - Current Pareto front scores:
Optimization Progress:  86%|████████▋ | 2200/2550 [1:58:51<43:10,  7.40s/pipeline]                                                                                  
-1	-4.275815367546219	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  86%|████████▋ | 2200/2550 [1:58:51<43:10,  7.40s/pipeline]                                                                                  
-3	-4.26828901164139	XGBRegressor(SGDRegressor(CombineDFs(ZeroCount(input_matrix), input_matrix), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001, XGBRegressor__verbosity=0)
Optimization Progress:  86%|████████▋ | 2200/2550 [1:58:51<43:10,  7.40s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.
Optimization Progress:  86%|████████▋ | 2200/2550 [1:59:04<43:10,  7.40s/pipeline]Optimization Progress:  86%|████████▋ | 2200/2550 [1:59:04<31:45,  5.44s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  86%|████████▋ | 2200/2550 [1:59:07<31:45,  5.44s/pipeline]Optimization Progress:  86%|████████▋ | 2201/2550 [1:59:07<28:07,  4.83s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  86%|████████▋ | 2201/2550 [1:59:07<28:07,  4.83s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  86%|████████▋ | 2202/2550 [1:59:07<28:02,  4.83s/pipeline]Optimization Progress:  86%|████████▋ | 2204/2550 [2:00:03<51:41,  8.96s/pipeline]Optimization Progress:  87%|████████▋ | 2220/2550 [2:00:50<39:22,  7.16s/pipeline]Optimization Progress:  88%|████████▊ | 2236/2550 [2:01:52<32:18,  6.17s/pipeline]                                                                                  
Generation 44 - Current Pareto front scores:
Optimization Progress:  88%|████████▊ | 2250/2550 [2:01:52<30:51,  6.17s/pipeline]                                                                                  
-1	-4.275815367546219	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  88%|████████▊ | 2250/2550 [2:01:52<30:51,  6.17s/pipeline]                                                                                  
-3	-4.26828901164139	XGBRegressor(SGDRegressor(CombineDFs(ZeroCount(input_matrix), input_matrix), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001, XGBRegressor__verbosity=0)
Optimization Progress:  88%|████████▊ | 2250/2550 [2:01:52<30:51,  6.17s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [08:34:37] ../src/learner.cc:602: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x19c8d9) [0x7fdf6464d8d9]
  [bt] (1) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b1f1b) [0x7fdf64662f1b]
  [bt] (2) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b9b4d) [0x7fdf6466ab4d]
  [bt] (3) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b4559) [0x7fdf64665559]
  [bt] (4) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fdf6454a4e8]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fe03280fff5]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fe03280f40a]
  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x5b6) [0x7fe02f4c5306]
  [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x139dc) [0x7fe02f4c59dc]

.
Optimization Progress:  88%|████████▊ | 2250/2550 [2:01:57<30:51,  6.17s/pipeline]Optimization Progress:  88%|████████▊ | 2250/2550 [2:01:57<22:11,  4.44s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:  88%|████████▊ | 2250/2550 [2:01:57<22:11,  4.44s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.
Optimization Progress:  88%|████████▊ | 2250/2550 [2:02:04<22:11,  4.44s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  88%|████████▊ | 2250/2550 [2:02:07<22:11,  4.44s/pipeline]Optimization Progress:  88%|████████▊ | 2251/2550 [2:02:07<29:31,  5.93s/pipeline]Optimization Progress:  88%|████████▊ | 2252/2550 [2:03:17<2:05:47, 25.33s/pipeline]Optimization Progress:  89%|████████▉ | 2268/2550 [2:04:06<1:27:36, 18.64s/pipeline]Optimization Progress:  90%|████████▉ | 2284/2550 [2:04:54<1:01:51, 13.95s/pipeline]Optimization Progress:  90%|█████████ | 2300/2550 [2:05:07<41:44, 10.02s/pipeline]                                                                                    
Generation 45 - Current Pareto front scores:
Optimization Progress:  90%|█████████ | 2300/2550 [2:05:07<41:44, 10.02s/pipeline]                                                                                  
-1	-4.275815367546219	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  90%|█████████ | 2300/2550 [2:05:07<41:44, 10.02s/pipeline]                                                                                  
-2	-4.272794361813041	XGBRegressor(ZeroCount(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  90%|█████████ | 2300/2550 [2:05:07<41:44, 10.02s/pipeline]                                                                                  
-3	-4.26828901164139	XGBRegressor(SGDRegressor(CombineDFs(ZeroCount(input_matrix), input_matrix), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001, XGBRegressor__verbosity=0)
Optimization Progress:  90%|█████████ | 2300/2550 [2:05:07<41:44, 10.02s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [08:37:47] ../src/learner.cc:602: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x19c8d9) [0x7fdf6464d8d9]
  [bt] (1) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b1f1b) [0x7fdf64662f1b]
  [bt] (2) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b9b4d) [0x7fdf6466ab4d]
  [bt] (3) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b4559) [0x7fdf64665559]
  [bt] (4) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fdf6454a4e8]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fe03280fff5]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fe03280f40a]
  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x5b6) [0x7fe02f4c5306]
  [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x139dc) [0x7fe02f4c59dc]

.
Optimization Progress:  90%|█████████ | 2300/2550 [2:05:08<41:44, 10.02s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.
Optimization Progress:  90%|█████████ | 2300/2550 [2:05:15<41:44, 10.02s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  90%|█████████ | 2300/2550 [2:05:16<41:44, 10.02s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  90%|█████████ | 2300/2550 [2:05:21<41:44, 10.02s/pipeline]Optimization Progress:  90%|█████████ | 2301/2550 [2:05:21<45:48, 11.04s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  90%|█████████ | 2301/2550 [2:05:21<45:48, 11.04s/pipeline]Optimization Progress:  90%|█████████ | 2303/2550 [2:06:25<1:11:05, 17.27s/pipeline]Optimization Progress:  91%|█████████ | 2319/2550 [2:07:12<49:57, 12.98s/pipeline]  Optimization Progress:  92%|█████████▏| 2335/2550 [2:07:53<35:17,  9.85s/pipeline]                                                                                  
Generation 46 - Current Pareto front scores:
Optimization Progress:  92%|█████████▏| 2350/2550 [2:07:53<32:49,  9.85s/pipeline]                                                                                  
-1	-4.275815367546219	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  92%|█████████▏| 2350/2550 [2:07:53<32:49,  9.85s/pipeline]                                                                                  
-2	-4.272794361813041	XGBRegressor(ZeroCount(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  92%|█████████▏| 2350/2550 [2:07:53<32:49,  9.85s/pipeline]                                                                                  
-3	-4.26828901164139	XGBRegressor(SGDRegressor(CombineDFs(ZeroCount(input_matrix), input_matrix), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001, XGBRegressor__verbosity=0)
Optimization Progress:  92%|█████████▏| 2350/2550 [2:07:53<32:49,  9.85s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  92%|█████████▏| 2350/2550 [2:07:56<32:49,  9.85s/pipeline]Optimization Progress:  92%|█████████▏| 2350/2550 [2:07:56<23:10,  6.95s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress:  92%|█████████▏| 2350/2550 [2:07:56<23:10,  6.95s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  92%|█████████▏| 2350/2550 [2:07:58<23:10,  6.95s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [08:40:43] ../src/learner.cc:602: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x19c8d9) [0x7fdf6464d8d9]
  [bt] (1) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b1f1b) [0x7fdf64662f1b]
  [bt] (2) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b9b4d) [0x7fdf6466ab4d]
  [bt] (3) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b4559) [0x7fdf64665559]
  [bt] (4) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fdf6454a4e8]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fe03280fff5]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fe03280f40a]
  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x5b6) [0x7fe02f4c5306]
  [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x139dc) [0x7fe02f4c59dc]

.
Optimization Progress:  92%|█████████▏| 2350/2550 [2:08:04<23:10,  6.95s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  92%|█████████▏| 2350/2550 [2:08:04<23:10,  6.95s/pipeline]Optimization Progress:  92%|█████████▏| 2351/2550 [2:08:07<27:52,  8.40s/pipeline]Optimization Progress:  92%|█████████▏| 2352/2550 [2:09:10<1:21:45, 24.77s/pipeline]Optimization Progress:  93%|█████████▎| 2368/2550 [2:09:57<55:14, 18.21s/pipeline]  Optimization Progress:  93%|█████████▎| 2384/2550 [2:10:41<37:34, 13.58s/pipeline]Optimization Progress:  94%|█████████▍| 2400/2550 [2:10:52<24:16,  9.71s/pipeline]                                                                                  
Generation 47 - Current Pareto front scores:
Optimization Progress:  94%|█████████▍| 2400/2550 [2:10:52<24:16,  9.71s/pipeline]                                                                                  
-1	-4.275815367546219	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  94%|█████████▍| 2400/2550 [2:10:52<24:16,  9.71s/pipeline]                                                                                  
-2	-4.272794361813041	XGBRegressor(ZeroCount(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  94%|█████████▍| 2400/2550 [2:10:52<24:16,  9.71s/pipeline]                                                                                  
-3	-4.26828901164139	XGBRegressor(SGDRegressor(CombineDFs(ZeroCount(input_matrix), input_matrix), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.0, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001, XGBRegressor__verbosity=0)
Optimization Progress:  94%|█████████▍| 2400/2550 [2:10:52<24:16,  9.71s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 [08:43:36] ../src/learner.cc:602: Check failed: mparam_.num_feature != 0 (0 vs. 0) : 0 feature is supplied.  Are you using raw Booster interface?
Stack trace:
  [bt] (0) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x19c8d9) [0x7fdf6464d8d9]
  [bt] (1) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b1f1b) [0x7fdf64662f1b]
  [bt] (2) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b9b4d) [0x7fdf6466ab4d]
  [bt] (3) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x1b4559) [0x7fdf64665559]
  [bt] (4) /home/bartek/.local/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7fdf6454a4e8]
  [bt] (5) /lib/x86_64-linux-gnu/libffi.so.7(+0x6ff5) [0x7fe03280fff5]
  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.7(+0x640a) [0x7fe03280f40a]
  [bt] (7) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x5b6) [0x7fe02f4c5306]
  [bt] (8) /usr/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x139dc) [0x7fe02f4c59dc]

.
Optimization Progress:  94%|█████████▍| 2400/2550 [2:10:56<24:16,  9.71s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  94%|█████████▍| 2400/2550 [2:11:01<24:16,  9.71s/pipeline]Optimization Progress:  94%|█████████▍| 2401/2550 [2:11:07<28:13, 11.37s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  94%|█████████▍| 2401/2550 [2:11:07<28:13, 11.37s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  94%|█████████▍| 2402/2550 [2:11:08<28:02, 11.37s/pipeline]Optimization Progress:  94%|█████████▍| 2404/2550 [2:12:24<37:57, 15.60s/pipeline]Optimization Progress:  95%|█████████▍| 2420/2550 [2:13:06<25:20, 11.70s/pipeline]Optimization Progress:  96%|█████████▌| 2436/2550 [2:13:53<17:15,  9.09s/pipeline]                                                                                  
Generation 48 - Current Pareto front scores:
Optimization Progress:  96%|█████████▌| 2450/2550 [2:13:53<15:08,  9.09s/pipeline]                                                                                  
-1	-4.275815367546219	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  96%|█████████▌| 2450/2550 [2:13:53<15:08,  9.09s/pipeline]                                                                                  
-2	-4.272794361813041	XGBRegressor(ZeroCount(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  96%|█████████▌| 2450/2550 [2:13:53<15:08,  9.09s/pipeline]                                                                                  
-3	-4.2447372565614305	XGBRegressor(SGDRegressor(ZeroCount(input_matrix), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001, XGBRegressor__verbosity=0)
Optimization Progress:  96%|█████████▌| 2450/2550 [2:13:53<15:08,  9.09s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  96%|█████████▌| 2450/2550 [2:13:57<15:08,  9.09s/pipeline]Optimization Progress:  96%|█████████▌| 2450/2550 [2:13:57<10:44,  6.45s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.
Optimization Progress:  96%|█████████▌| 2450/2550 [2:14:01<10:44,  6.45s/pipeline]Optimization Progress:  96%|█████████▌| 2451/2550 [2:14:08<12:39,  7.67s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▌| 2451/2550 [2:14:08<12:39,  7.67s/pipeline]Optimization Progress:  96%|█████████▌| 2453/2550 [2:14:53<19:34, 12.10s/pipeline]Optimization Progress:  97%|█████████▋| 2469/2550 [2:15:55<13:00,  9.63s/pipeline]Optimization Progress:  97%|█████████▋| 2485/2550 [2:16:37<08:10,  7.54s/pipeline]                                                                                  
Generation 49 - Current Pareto front scores:
Optimization Progress:  98%|█████████▊| 2500/2550 [2:16:37<06:16,  7.54s/pipeline]                                                                                  
-1	-4.275815367546219	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  98%|█████████▊| 2500/2550 [2:16:37<06:16,  7.54s/pipeline]                                                                                  
-2	-4.272794361813041	XGBRegressor(ZeroCount(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress:  98%|█████████▊| 2500/2550 [2:16:37<06:16,  7.54s/pipeline]                                                                                  
-3	-4.2447372565614305	XGBRegressor(SGDRegressor(ZeroCount(input_matrix), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001, XGBRegressor__verbosity=0)
Optimization Progress:  98%|█████████▊| 2500/2550 [2:16:37<06:16,  7.54s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  98%|█████████▊| 2500/2550 [2:16:44<06:16,  7.54s/pipeline]Optimization Progress:  98%|█████████▊| 2500/2550 [2:16:44<04:30,  5.41s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 2500/2550 [2:16:52<04:30,  5.41s/pipeline]Optimization Progress:  98%|█████████▊| 2501/2550 [2:16:52<05:05,  6.24s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 2501/2550 [2:16:52<05:05,  6.24s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 2502/2550 [2:16:52<04:59,  6.24s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  98%|█████████▊| 2503/2550 [2:16:52<04:53,  6.24s/pipeline]Optimization Progress:  98%|█████████▊| 2505/2550 [2:18:01<07:09,  9.55s/pipeline]Optimization Progress:  99%|█████████▉| 2521/2550 [2:18:42<03:35,  7.44s/pipeline]Optimization Progress:  99%|█████████▉| 2537/2550 [2:19:26<01:18,  6.05s/pipeline]                                                                                  
Generation 50 - Current Pareto front scores:
Optimization Progress: 100%|██████████| 2550/2550 [2:19:26<00:00,  6.05s/pipeline]                                                                                  
-1	-4.275815367546219	XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress: 100%|██████████| 2550/2550 [2:19:26<00:00,  6.05s/pipeline]                                                                                  
-2	-4.272794361813041	XGBRegressor(ZeroCount(input_matrix), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=9, XGBRegressor__min_child_weight=2, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.45, XGBRegressor__verbosity=0)
Optimization Progress: 100%|██████████| 2550/2550 [2:19:26<00:00,  6.05s/pipeline]                                                                                  
-3	-4.2447372565614305	XGBRegressor(SGDRegressor(ZeroCount(input_matrix), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=invscaling, SGDRegressor__loss=squared_loss, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=10.0), XGBRegressor__learning_rate=0.1, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=4, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.6500000000000001, XGBRegressor__verbosity=0)
Optimization Progress: 100%|██████████| 2550/2550 [2:19:26<00:00,  6.05s/pipeline]                                                                                  MODEL SCORE -4.906288037680716
30 operators have been imported by TPOT.
Optimization Progress:   0%|          | 0/2550 [00:00<?, ?pipeline/s]Optimization Progress:   0%|          | 4/2550 [00:12<2:12:05,  3.11s/pipeline]Optimization Progress:   1%|          | 20/2550 [00:25<1:42:33,  2.43s/pipeline]Optimization Progress:   1%|▏         | 36/2550 [00:37<1:20:27,  1.92s/pipeline]Optimization Progress:   2%|▏         | 50/2550 [00:42<1:00:23,  1.45s/pipeline]Optimization Progress:   2%|▏         | 52/2550 [00:53<1:51:19,  2.67s/pipeline]Optimization Progress:   3%|▎         | 68/2550 [01:05<1:26:57,  2.10s/pipeline]Optimization Progress:   3%|▎         | 84/2550 [01:20<1:11:40,  1.74s/pipeline]Optimization Progress:   4%|▍         | 100/2550 [01:27<55:22,  1.36s/pipeline]                                                                                
Generation 1 - Current Pareto front scores:
Optimization Progress:   4%|▍         | 100/2550 [01:27<55:22,  1.36s/pipeline]                                                                               
-1	-1.08533880270445	LinearSVR(CombineDFs(input_matrix, input_matrix), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05)
Optimization Progress:   4%|▍         | 100/2550 [01:27<55:22,  1.36s/pipeline]                                                                               _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress:   4%|▍         | 100/2550 [01:32<55:22,  1.36s/pipeline]                                                                               _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.
Optimization Progress:   4%|▍         | 100/2550 [01:32<55:22,  1.36s/pipeline]Optimization Progress:   4%|▍         | 101/2550 [01:33<1:54:28,  2.80s/pipeline]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:   4%|▍         | 101/2550 [01:33<1:54:28,  2.80s/pipeline]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:   4%|▍         | 102/2550 [01:33<1:54:25,  2.80s/pipeline]Optimization Progress:   4%|▍         | 104/2550 [01:45<2:07:26,  3.13s/pipeline]Optimization Progress:   5%|▍         | 120/2550 [02:02<1:41:33,  2.51s/pipeline]Optimization Progress:   5%|▌         | 136/2550 [02:16<1:21:07,  2.02s/pipeline]                                                                                 
Generation 2 - Current Pareto front scores:
Optimization Progress:   6%|▌         | 150/2550 [02:16<1:20:39,  2.02s/pipeline]                                                                                 
-1	-1.08533880270445	LinearSVR(CombineDFs(input_matrix, input_matrix), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05)
Optimization Progress:   6%|▌         | 150/2550 [02:16<1:20:39,  2.02s/pipeline]Optimization Progress:   6%|▌         | 150/2550 [02:22<1:01:29,  1.54s/pipeline]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:   6%|▌         | 151/2550 [02:22<1:01:27,  1.54s/pipeline]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:   6%|▌         | 152/2550 [02:22<1:01:26,  1.54s/pipeline]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:   6%|▌         | 153/2550 [02:22<1:01:24,  1.54s/pipeline]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:   6%|▌         | 154/2550 [02:22<1:01:23,  1.54s/pipeline]Optimization Progress:   6%|▌         | 156/2550 [02:34<1:06:43,  1.67s/pipeline]Optimization Progress:   7%|▋         | 172/2550 [02:52<1:00:21,  1.52s/pipeline]Optimization Progress:   7%|▋         | 188/2550 [03:00<47:52,  1.22s/pipeline]                                                                                 
Generation 3 - Current Pareto front scores:
Optimization Progress:   8%|▊         | 200/2550 [03:00<47:37,  1.22s/pipeline]                                                                               
-1	-1.08533880270445	LinearSVR(CombineDFs(input_matrix, input_matrix), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05)
Optimization Progress:   8%|▊         | 200/2550 [03:00<47:37,  1.22s/pipeline]                                                                               _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:   8%|▊         | 200/2550 [03:01<47:37,  1.22s/pipeline]Optimization Progress:   8%|▊         | 200/2550 [03:01<34:15,  1.14pipeline/s]                                                                               _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:   8%|▊         | 200/2550 [03:02<34:15,  1.14pipeline/s]                                                                               _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:   8%|▊         | 200/2550 [03:02<34:15,  1.14pipeline/s]                                                                               _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:   8%|▊         | 200/2550 [03:02<34:15,  1.14pipeline/s]                                                                               _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:   8%|▊         | 200/2550 [03:03<34:15,  1.14pipeline/s]                                                                               Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:   8%|▊         | 200/2550 [03:04<34:15,  1.14pipeline/s]Optimization Progress:   8%|▊         | 201/2550 [03:04<53:42,  1.37s/pipeline]Optimization Progress:   8%|▊         | 202/2550 [03:14<2:39:00,  4.06s/pipeline]Optimization Progress:   9%|▊         | 218/2550 [03:23<1:56:37,  3.00s/pipeline]Optimization Progress:   9%|▉         | 234/2550 [03:32<1:27:50,  2.28s/pipeline]Optimization Progress:  10%|▉         | 250/2550 [03:33<1:01:51,  1.61s/pipeline]                                                                                 
Generation 4 - Current Pareto front scores:
Optimization Progress:  10%|▉         | 250/2550 [03:33<1:01:51,  1.61s/pipeline]                                                                                 
-1	-1.08533880270445	LinearSVR(CombineDFs(input_matrix, input_matrix), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05)
Optimization Progress:  10%|▉         | 250/2550 [03:33<1:01:51,  1.61s/pipeline]Optimization Progress:  10%|▉         | 251/2550 [03:36<1:16:59,  2.01s/pipeline]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  10%|▉         | 251/2550 [03:36<1:16:59,  2.01s/pipeline]Optimization Progress:  10%|▉         | 253/2550 [03:45<1:43:17,  2.70s/pipeline]Optimization Progress:  11%|█         | 269/2550 [03:53<1:18:09,  2.06s/pipeline]Optimization Progress:  11%|█         | 285/2550 [03:56<56:26,  1.49s/pipeline]                                                                                 
Generation 5 - Current Pareto front scores:
Optimization Progress:  12%|█▏        | 300/2550 [03:56<56:03,  1.49s/pipeline]                                                                               
-1	-1.08533880270445	LinearSVR(CombineDFs(input_matrix, input_matrix), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05)
Optimization Progress:  12%|█▏        | 300/2550 [03:56<56:03,  1.49s/pipeline]                                                                               _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.
Optimization Progress:  12%|█▏        | 300/2550 [03:57<56:03,  1.49s/pipeline]Optimization Progress:  12%|█▏        | 300/2550 [03:57<39:26,  1.05s/pipeline]Optimization Progress:  12%|█▏        | 303/2550 [04:00<40:51,  1.09s/pipeline]                                                                               Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  12%|█▏        | 303/2550 [04:00<40:51,  1.09s/pipeline]Optimization Progress:  12%|█▏        | 305/2550 [04:03<45:52,  1.23s/pipeline]Optimization Progress:  13%|█▎        | 321/2550 [04:15<39:43,  1.07s/pipeline]Optimization Progress:  13%|█▎        | 337/2550 [04:30<38:06,  1.03s/pipeline]                                                                               
Generation 6 - Current Pareto front scores:
Optimization Progress:  14%|█▎        | 350/2550 [04:30<37:53,  1.03s/pipeline]                                                                               
-1	-1.08533880270445	LinearSVR(CombineDFs(input_matrix, input_matrix), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05)
Optimization Progress:  14%|█▎        | 350/2550 [04:30<37:53,  1.03s/pipeline]                                                                               
-3	-1.0852852202958132	LassoLarsCV(ZeroCount(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.01)), LassoLarsCV__normalize=True)
Optimization Progress:  14%|█▎        | 350/2550 [04:30<37:53,  1.03s/pipeline]                                                                               _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.
Optimization Progress:  14%|█▎        | 350/2550 [04:30<37:53,  1.03s/pipeline]Optimization Progress:  14%|█▎        | 350/2550 [04:30<26:41,  1.37pipeline/s]Optimization Progress:  14%|█▍        | 352/2550 [04:33<37:17,  1.02s/pipeline]Optimization Progress:  14%|█▍        | 354/2550 [04:38<52:13,  1.43s/pipeline]Optimization Progress:  14%|█▍        | 369/2550 [04:47<42:52,  1.18s/pipeline]Optimization Progress:  15%|█▌        | 385/2550 [04:58<37:07,  1.03s/pipeline]                                                                               
Generation 7 - Current Pareto front scores:
Optimization Progress:  16%|█▌        | 400/2550 [04:58<36:52,  1.03s/pipeline]                                                                               
-1	-1.08533880270445	LinearSVR(CombineDFs(input_matrix, input_matrix), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05)
Optimization Progress:  16%|█▌        | 400/2550 [04:58<36:52,  1.03s/pipeline]                                                                               
-2	-1.0842604216095497	LinearSVR(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  16%|█▌        | 400/2550 [04:58<36:52,  1.03s/pipeline]                                                                               
-3	-1.0814601475798489	RidgeCV(SGDRegressor(ZeroCount(input_matrix), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=1.0))
Optimization Progress:  16%|█▌        | 400/2550 [04:58<36:52,  1.03s/pipeline]                                                                               _pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:  16%|█▌        | 400/2550 [04:59<36:52,  1.03s/pipeline]Optimization Progress:  16%|█▌        | 400/2550 [04:59<26:25,  1.36pipeline/s]                                                                               _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  16%|█▌        | 400/2550 [05:00<26:25,  1.36pipeline/s]Optimization Progress:  16%|█▌        | 401/2550 [05:03<59:24,  1.66s/pipeline]                                                                               Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  16%|█▌        | 401/2550 [05:03<59:24,  1.66s/pipeline]Optimization Progress:  16%|█▌        | 403/2550 [05:12<1:29:11,  2.49s/pipeline]Optimization Progress:  16%|█▋        | 419/2550 [05:20<1:07:29,  1.90s/pipeline]Optimization Progress:  17%|█▋        | 435/2550 [05:36<57:32,  1.63s/pipeline]                                                                                 
Generation 8 - Current Pareto front scores:
Optimization Progress:  18%|█▊        | 450/2550 [05:36<57:07,  1.63s/pipeline]                                                                               
-1	-1.08533880270445	LinearSVR(CombineDFs(input_matrix, input_matrix), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05)
Optimization Progress:  18%|█▊        | 450/2550 [05:36<57:07,  1.63s/pipeline]                                                                               
-2	-1.0842604216095497	LinearSVR(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  18%|█▊        | 450/2550 [05:36<57:07,  1.63s/pipeline]                                                                               
-3	-1.0814601475798489	RidgeCV(SGDRegressor(ZeroCount(input_matrix), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=1.0))
Optimization Progress:  18%|█▊        | 450/2550 [05:36<57:07,  1.63s/pipeline]                                                                               _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  18%|█▊        | 450/2550 [05:36<57:07,  1.63s/pipeline]                                                                               _pre_test decorator: _random_mutation_operator: num_test=0 x and y arrays must have at least 2 entries.
Optimization Progress:  18%|█▊        | 450/2550 [05:37<57:07,  1.63s/pipeline]Optimization Progress:  18%|█▊        | 450/2550 [05:37<40:27,  1.16s/pipeline]                                                                               _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.
Optimization Progress:  18%|█▊        | 450/2550 [05:38<40:27,  1.16s/pipeline]Optimization Progress:  18%|█▊        | 451/2550 [05:39<48:35,  1.39s/pipeline]Optimization Progress:  18%|█▊        | 452/2550 [05:47<1:59:08,  3.41s/pipeline]Optimization Progress:  18%|█▊        | 468/2550 [05:56<1:28:51,  2.56s/pipeline]Optimization Progress:  19%|█▉        | 484/2550 [05:59<1:03:36,  1.85s/pipeline]Optimization Progress:  20%|█▉        | 500/2550 [05:59<44:25,  1.30s/pipeline]                                                                                 
Generation 9 - Current Pareto front scores:
Optimization Progress:  20%|█▉        | 500/2550 [05:59<44:25,  1.30s/pipeline]                                                                               
-1	-1.08533880270445	LinearSVR(CombineDFs(input_matrix, input_matrix), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05)
Optimization Progress:  20%|█▉        | 500/2550 [05:59<44:25,  1.30s/pipeline]                                                                               
-2	-1.0842604216095497	LinearSVR(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  20%|█▉        | 500/2550 [05:59<44:25,  1.30s/pipeline]                                                                               
-3	-1.0814601475798489	RidgeCV(SGDRegressor(ZeroCount(input_matrix), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=1.0))
Optimization Progress:  20%|█▉        | 500/2550 [05:59<44:25,  1.30s/pipeline]                                                                               _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress:  20%|█▉        | 500/2550 [05:59<44:25,  1.30s/pipeline]                                                                               _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  20%|█▉        | 500/2550 [06:03<44:25,  1.30s/pipeline]Optimization Progress:  20%|█▉        | 505/2550 [06:03<38:46,  1.14s/pipeline]Optimization Progress:  20%|█▉        | 507/2550 [06:14<1:24:29,  2.48s/pipeline]Optimization Progress:  20%|██        | 522/2550 [06:29<1:08:44,  2.03s/pipeline]Optimization Progress:  21%|██        | 538/2550 [06:39<53:34,  1.60s/pipeline]                                                                                 
Generation 10 - Current Pareto front scores:
Optimization Progress:  22%|██▏       | 550/2550 [06:39<53:14,  1.60s/pipeline]                                                                               
-1	-1.08533880270445	LinearSVR(CombineDFs(input_matrix, input_matrix), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05)
Optimization Progress:  22%|██▏       | 550/2550 [06:39<53:14,  1.60s/pipeline]                                                                               
-2	-1.0842604216095497	LinearSVR(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  22%|██▏       | 550/2550 [06:39<53:14,  1.60s/pipeline]                                                                               
-3	-1.0814601475798489	RidgeCV(SGDRegressor(ZeroCount(input_matrix), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=1.0))
Optimization Progress:  22%|██▏       | 550/2550 [06:39<53:14,  1.60s/pipeline]                                                                               _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:  22%|██▏       | 550/2550 [06:39<53:14,  1.60s/pipeline]Optimization Progress:  22%|██▏       | 550/2550 [06:39<37:56,  1.14s/pipeline]Optimization Progress:  22%|██▏       | 551/2550 [06:42<50:12,  1.51s/pipeline]Optimization Progress:  22%|██▏       | 552/2550 [06:52<2:21:46,  4.26s/pipeline]Optimization Progress:  22%|██▏       | 568/2550 [07:00<1:43:24,  3.13s/pipeline]Optimization Progress:  23%|██▎       | 584/2550 [07:11<1:18:17,  2.39s/pipeline]Optimization Progress:  24%|██▎       | 600/2550 [07:12<54:59,  1.69s/pipeline]                                                                                 
Generation 11 - Current Pareto front scores:
Optimization Progress:  24%|██▎       | 600/2550 [07:12<54:59,  1.69s/pipeline]                                                                               
-1	-1.08533880270445	LinearSVR(CombineDFs(input_matrix, input_matrix), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05)
Optimization Progress:  24%|██▎       | 600/2550 [07:12<54:59,  1.69s/pipeline]                                                                               
-2	-1.0842604216095497	LinearSVR(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  24%|██▎       | 600/2550 [07:12<54:59,  1.69s/pipeline]                                                                               
-3	-1.0814601475798489	RidgeCV(SGDRegressor(ZeroCount(input_matrix), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=1.0))
Optimization Progress:  24%|██▎       | 600/2550 [07:12<54:59,  1.69s/pipeline]                                                                               _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  24%|██▎       | 600/2550 [07:17<54:59,  1.69s/pipeline]Optimization Progress:  24%|██▎       | 601/2550 [07:18<1:34:56,  2.92s/pipeline]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  24%|██▎       | 601/2550 [07:18<1:34:56,  2.92s/pipeline]Optimization Progress:  24%|██▎       | 603/2550 [07:37<2:38:05,  4.87s/pipeline]Optimization Progress:  24%|██▍       | 619/2550 [07:49<1:57:02,  3.64s/pipeline]Optimization Progress:  25%|██▍       | 635/2550 [08:00<1:28:03,  2.76s/pipeline]                                                                                 
Generation 12 - Current Pareto front scores:
Optimization Progress:  25%|██▌       | 650/2550 [08:00<1:27:21,  2.76s/pipeline]                                                                                 
-1	-1.0850899939437213	LinearSVR(input_matrix, LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  25%|██▌       | 650/2550 [08:00<1:27:21,  2.76s/pipeline]                                                                                 
-2	-1.0842604216095497	LinearSVR(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  25%|██▌       | 650/2550 [08:00<1:27:21,  2.76s/pipeline]                                                                                 
-3	-1.0814601475798489	RidgeCV(SGDRegressor(ZeroCount(input_matrix), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=1.0))
Optimization Progress:  25%|██▌       | 650/2550 [08:00<1:27:21,  2.76s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  25%|██▌       | 650/2550 [08:03<1:27:21,  2.76s/pipeline]Optimization Progress:  25%|██▌       | 650/2550 [08:03<1:02:50,  1.98s/pipeline]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  25%|██▌       | 650/2550 [08:06<1:02:50,  1.98s/pipeline]Optimization Progress:  26%|██▌       | 651/2550 [08:06<1:11:47,  2.27s/pipeline]Optimization Progress:  26%|██▌       | 652/2550 [08:17<2:42:16,  5.13s/pipeline]Optimization Progress:  26%|██▌       | 668/2550 [08:37<2:04:20,  3.96s/pipeline]Optimization Progress:  27%|██▋       | 684/2550 [08:52<1:34:50,  3.05s/pipeline]Optimization Progress:  27%|██▋       | 700/2550 [08:55<1:07:17,  2.18s/pipeline]                                                                                 
Generation 13 - Current Pareto front scores:
Optimization Progress:  27%|██▋       | 700/2550 [08:55<1:07:17,  2.18s/pipeline]                                                                                 
-1	-1.0850899939437213	LinearSVR(input_matrix, LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  27%|██▋       | 700/2550 [08:55<1:07:17,  2.18s/pipeline]                                                                                 
-2	-1.0842604216095497	LinearSVR(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  27%|██▋       | 700/2550 [08:55<1:07:17,  2.18s/pipeline]                                                                                 
-3	-1.0814601475798489	RidgeCV(SGDRegressor(ZeroCount(input_matrix), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=1.0))
Optimization Progress:  27%|██▋       | 700/2550 [08:55<1:07:17,  2.18s/pipeline]Optimization Progress:  27%|██▋       | 701/2550 [08:59<1:27:12,  2.83s/pipeline]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  27%|██▋       | 701/2550 [08:59<1:27:12,  2.83s/pipeline]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  28%|██▊       | 702/2550 [08:59<1:27:09,  2.83s/pipeline]Optimization Progress:  28%|██▊       | 704/2550 [09:07<1:26:49,  2.82s/pipeline]Optimization Progress:  28%|██▊       | 720/2550 [09:19<1:07:13,  2.20s/pipeline]Optimization Progress:  29%|██▉       | 736/2550 [09:30<52:30,  1.74s/pipeline]                                                                                 
Generation 14 - Current Pareto front scores:
Optimization Progress:  29%|██▉       | 750/2550 [09:30<52:05,  1.74s/pipeline]                                                                               
-1	-1.0850899939437213	LinearSVR(input_matrix, LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  29%|██▉       | 750/2550 [09:30<52:05,  1.74s/pipeline]                                                                               
-2	-1.0842604216095497	LinearSVR(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  29%|██▉       | 750/2550 [09:30<52:05,  1.74s/pipeline]                                                                               
-3	-1.0814601475798489	RidgeCV(SGDRegressor(ZeroCount(input_matrix), SGDRegressor__alpha=0.0, SGDRegressor__eta0=1.0, SGDRegressor__fit_intercept=False, SGDRegressor__l1_ratio=0.5, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=1.0))
Optimization Progress:  29%|██▉       | 750/2550 [09:30<52:05,  1.74s/pipeline]                                                                               _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  29%|██▉       | 750/2550 [09:31<52:05,  1.74s/pipeline]Optimization Progress:  29%|██▉       | 750/2550 [09:31<37:09,  1.24s/pipeline]                                                                               _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  29%|██▉       | 750/2550 [09:35<37:09,  1.24s/pipeline]                                                                               _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59.
Optimization Progress:  29%|██▉       | 750/2550 [09:35<37:09,  1.24s/pipeline]Optimization Progress:  29%|██▉       | 751/2550 [09:49<3:05:29,  6.19s/pipeline]Optimization Progress:  30%|███       | 767/2550 [10:09<2:19:58,  4.71s/pipeline]Optimization Progress:  31%|███       | 783/2550 [10:18<1:42:14,  3.47s/pipeline]Optimization Progress:  31%|███▏      | 799/2550 [10:26<1:15:13,  2.58s/pipeline]                                                                                 
Generation 15 - Current Pareto front scores:
Optimization Progress:  31%|███▏      | 800/2550 [10:26<1:15:11,  2.58s/pipeline]                                                                                 
-1	-1.0850899939437213	LinearSVR(input_matrix, LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  31%|███▏      | 800/2550 [10:26<1:15:11,  2.58s/pipeline]                                                                                 
-2	-1.0842604216095497	LinearSVR(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  31%|███▏      | 800/2550 [10:26<1:15:11,  2.58s/pipeline]                                                                                 
-3	-1.0799269147525186	ElasticNetCV(MaxAbsScaler(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0))), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  31%|███▏      | 800/2550 [10:26<1:15:11,  2.58s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:  31%|███▏      | 800/2550 [10:28<1:15:11,  2.58s/pipeline]Optimization Progress:  31%|███▏      | 800/2550 [10:28<1:07:43,  2.32s/pipeline]                                                                                 _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  31%|███▏      | 800/2550 [10:29<1:07:43,  2.32s/pipeline]Optimization Progress:  31%|███▏      | 801/2550 [10:32<1:26:52,  2.98s/pipeline]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  31%|███▏      | 801/2550 [10:32<1:26:52,  2.98s/pipeline]Optimization Progress:  31%|███▏      | 803/2550 [10:44<1:53:41,  3.90s/pipeline]Optimization Progress:  32%|███▏      | 819/2550 [10:55<1:24:47,  2.94s/pipeline]Optimization Progress:  33%|███▎      | 835/2550 [11:16<1:10:02,  2.45s/pipeline]                                                                                 
Generation 16 - Current Pareto front scores:
Optimization Progress:  33%|███▎      | 850/2550 [11:16<1:09:25,  2.45s/pipeline]                                                                                 
-1	-1.0850899939437213	LinearSVR(input_matrix, LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  33%|███▎      | 850/2550 [11:16<1:09:25,  2.45s/pipeline]                                                                                 
-2	-1.0842604216095497	LinearSVR(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  33%|███▎      | 850/2550 [11:16<1:09:25,  2.45s/pipeline]                                                                                 
-3	-1.0799269147525186	ElasticNetCV(MaxAbsScaler(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0))), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  33%|███▎      | 850/2550 [11:16<1:09:25,  2.45s/pipeline]Optimization Progress:  33%|███▎      | 850/2550 [11:21<51:06,  1.80s/pipeline]                                                                                 Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  33%|███▎      | 852/2550 [11:21<51:03,  1.80s/pipeline]Optimization Progress:  33%|███▎      | 854/2550 [11:30<55:16,  1.96s/pipeline]Optimization Progress:  34%|███▍      | 870/2550 [11:45<46:09,  1.65s/pipeline]Optimization Progress:  35%|███▍      | 886/2550 [11:55<37:07,  1.34s/pipeline]                                                                               
Generation 17 - Current Pareto front scores:
Optimization Progress:  35%|███▌      | 900/2550 [11:55<36:48,  1.34s/pipeline]                                                                               
-1	-1.0850899939437213	LinearSVR(input_matrix, LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  35%|███▌      | 900/2550 [11:55<36:48,  1.34s/pipeline]                                                                               
-2	-1.0842604216095497	LinearSVR(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  35%|███▌      | 900/2550 [11:55<36:48,  1.34s/pipeline]                                                                               
-3	-1.0799269147525186	ElasticNetCV(MaxAbsScaler(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0))), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  35%|███▌      | 900/2550 [11:55<36:48,  1.34s/pipeline]Optimization Progress:  35%|███▌      | 900/2550 [11:59<28:03,  1.02s/pipeline]Optimization Progress:  35%|███▌      | 901/2550 [12:08<1:40:00,  3.64s/pipeline]Optimization Progress:  36%|███▌      | 917/2550 [12:19<1:14:42,  2.74s/pipeline]Optimization Progress:  37%|███▋      | 933/2550 [12:34<59:15,  2.20s/pipeline]  Optimization Progress:  37%|███▋      | 949/2550 [12:35<41:36,  1.56s/pipeline]                                                                               
Generation 18 - Current Pareto front scores:
Optimization Progress:  37%|███▋      | 950/2550 [12:35<41:34,  1.56s/pipeline]                                                                               
-1	-1.0850899939437213	LinearSVR(input_matrix, LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  37%|███▋      | 950/2550 [12:35<41:34,  1.56s/pipeline]                                                                               
-2	-1.0842604216095497	LinearSVR(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  37%|███▋      | 950/2550 [12:35<41:34,  1.56s/pipeline]                                                                               
-3	-1.0799269147525186	ElasticNetCV(MaxAbsScaler(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0))), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  37%|███▋      | 950/2550 [12:35<41:34,  1.56s/pipeline]                                                                               _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress:  37%|███▋      | 950/2550 [12:35<41:34,  1.56s/pipeline]                                                                               _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  37%|███▋      | 950/2550 [12:37<41:34,  1.56s/pipeline]Optimization Progress:  37%|███▋      | 950/2550 [12:37<46:15,  1.73s/pipeline]Optimization Progress:  37%|███▋      | 951/2550 [12:40<54:24,  2.04s/pipeline]                                                                               Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  37%|███▋      | 951/2550 [12:40<54:24,  2.04s/pipeline]                                                                               Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  37%|███▋      | 952/2550 [12:40<54:22,  2.04s/pipeline]Optimization Progress:  37%|███▋      | 954/2550 [12:51<1:07:13,  2.53s/pipeline]Optimization Progress:  38%|███▊      | 970/2550 [13:01<51:27,  1.95s/pipeline]  Optimization Progress:  39%|███▊      | 986/2550 [13:11<40:36,  1.56s/pipeline]                                                                               
Generation 19 - Current Pareto front scores:
Optimization Progress:  39%|███▉      | 1000/2550 [13:11<40:14,  1.56s/pipeline]                                                                                
-1	-1.0850899939437213	LinearSVR(input_matrix, LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  39%|███▉      | 1000/2550 [13:11<40:14,  1.56s/pipeline]                                                                                
-2	-1.0842604216095497	LinearSVR(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  39%|███▉      | 1000/2550 [13:11<40:14,  1.56s/pipeline]                                                                                
-3	-1.0799269147525186	ElasticNetCV(MaxAbsScaler(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0))), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  39%|███▉      | 1000/2550 [13:11<40:14,  1.56s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.
Optimization Progress:  39%|███▉      | 1000/2550 [13:12<40:14,  1.56s/pipeline]Optimization Progress:  39%|███▉      | 1000/2550 [13:12<28:50,  1.12s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  39%|███▉      | 1000/2550 [13:12<28:50,  1.12s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..
Optimization Progress:  39%|███▉      | 1000/2550 [13:15<28:50,  1.12s/pipeline]Optimization Progress:  39%|███▉      | 1001/2550 [13:33<3:02:01,  7.05s/pipeline]Optimization Progress:  40%|███▉      | 1017/2550 [13:49<2:14:03,  5.25s/pipeline]Optimization Progress:  41%|████      | 1033/2550 [14:02<1:39:02,  3.92s/pipeline]Optimization Progress:  41%|████      | 1049/2550 [14:03<1:08:40,  2.74s/pipeline]                                                                                  
Generation 20 - Current Pareto front scores:
Optimization Progress:  41%|████      | 1050/2550 [14:03<1:08:37,  2.74s/pipeline]                                                                                  
-1	-1.0850899939437213	LinearSVR(input_matrix, LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  41%|████      | 1050/2550 [14:03<1:08:37,  2.74s/pipeline]                                                                                  
-2	-1.0842604216095497	LinearSVR(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  41%|████      | 1050/2550 [14:03<1:08:37,  2.74s/pipeline]                                                                                  
-3	-1.0799269147525186	ElasticNetCV(MaxAbsScaler(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0))), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  41%|████      | 1050/2550 [14:03<1:08:37,  2.74s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:  41%|████      | 1050/2550 [14:03<1:08:37,  2.74s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  41%|████      | 1050/2550 [14:07<1:08:37,  2.74s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:  41%|████      | 1050/2550 [14:07<1:08:37,  2.74s/pipeline]Optimization Progress:  41%|████▏     | 1052/2550 [14:19<1:08:31,  2.74s/pipeline]Optimization Progress:  41%|████▏     | 1053/2550 [14:20<1:21:14,  3.26s/pipeline]Optimization Progress:  42%|████▏     | 1069/2550 [14:37<1:04:03,  2.60s/pipeline]Optimization Progress:  43%|████▎     | 1085/2550 [14:52<51:18,  2.10s/pipeline]                                                                                  
Generation 21 - Current Pareto front scores:
Optimization Progress:  43%|████▎     | 1100/2550 [14:52<50:46,  2.10s/pipeline]                                                                                
-1	-1.0850899939437213	LinearSVR(input_matrix, LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  43%|████▎     | 1100/2550 [14:52<50:46,  2.10s/pipeline]                                                                                
-2	-1.0842604216095497	LinearSVR(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  43%|████▎     | 1100/2550 [14:52<50:46,  2.10s/pipeline]                                                                                
-3	-1.0799269147525186	ElasticNetCV(MaxAbsScaler(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0))), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  43%|████▎     | 1100/2550 [14:52<50:46,  2.10s/pipeline]Optimization Progress:  43%|████▎     | 1100/2550 [14:58<38:06,  1.58s/pipeline]Optimization Progress:  43%|████▎     | 1101/2550 [15:07<1:33:36,  3.88s/pipeline]Optimization Progress:  44%|████▍     | 1117/2550 [15:17<1:09:26,  2.91s/pipeline]Optimization Progress:  44%|████▍     | 1133/2550 [15:33<54:53,  2.32s/pipeline]  Optimization Progress:  45%|████▌     | 1149/2550 [15:40<41:11,  1.76s/pipeline]                                                                                
Generation 22 - Current Pareto front scores:
Optimization Progress:  45%|████▌     | 1150/2550 [15:40<41:09,  1.76s/pipeline]                                                                                
-1	-1.0850899939437213	LinearSVR(input_matrix, LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  45%|████▌     | 1150/2550 [15:40<41:09,  1.76s/pipeline]                                                                                
-2	-1.0842604216095497	LinearSVR(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  45%|████▌     | 1150/2550 [15:40<41:09,  1.76s/pipeline]                                                                                
-3	-1.0799269147525186	ElasticNetCV(MaxAbsScaler(CombineDFs(input_matrix, XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0))), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  45%|████▌     | 1150/2550 [15:40<41:09,  1.76s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  45%|████▌     | 1150/2550 [15:40<41:09,  1.76s/pipeline]Optimization Progress:  45%|████▌     | 1150/2550 [15:40<31:17,  1.34s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 98.
Optimization Progress:  45%|████▌     | 1150/2550 [15:42<31:17,  1.34s/pipeline]Optimization Progress:  45%|████▌     | 1151/2550 [15:44<49:14,  2.11s/pipeline]Optimization Progress:  45%|████▌     | 1152/2550 [15:55<1:48:36,  4.66s/pipeline]Optimization Progress:  46%|████▌     | 1168/2550 [16:04<1:18:59,  3.43s/pipeline]Optimization Progress:  46%|████▋     | 1184/2550 [16:17<1:00:21,  2.65s/pipeline]Optimization Progress:  47%|████▋     | 1200/2550 [16:25<44:59,  2.00s/pipeline]                                                                                  
Generation 23 - Current Pareto front scores:
Optimization Progress:  47%|████▋     | 1200/2550 [16:25<44:59,  2.00s/pipeline]                                                                                
-1	-1.0850899939437213	LinearSVR(input_matrix, LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  47%|████▋     | 1200/2550 [16:25<44:59,  2.00s/pipeline]                                                                                
-2	-1.0842604216095497	LinearSVR(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  47%|████▋     | 1200/2550 [16:25<44:59,  2.00s/pipeline]                                                                                
-3	-1.0700377270358012	ElasticNetCV(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  47%|████▋     | 1200/2550 [16:25<44:59,  2.00s/pipeline]Optimization Progress:  47%|████▋     | 1202/2550 [16:30<47:39,  2.12s/pipeline]Optimization Progress:  47%|████▋     | 1203/2550 [16:46<2:23:56,  6.41s/pipeline]Optimization Progress:  48%|████▊     | 1219/2550 [16:55<1:43:23,  4.66s/pipeline]Optimization Progress:  48%|████▊     | 1235/2550 [17:09<1:16:56,  3.51s/pipeline]                                                                                  
Generation 24 - Current Pareto front scores:
Optimization Progress:  49%|████▉     | 1250/2550 [17:09<1:16:03,  3.51s/pipeline]                                                                                  
-1	-1.0850899939437213	LinearSVR(input_matrix, LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  49%|████▉     | 1250/2550 [17:09<1:16:03,  3.51s/pipeline]                                                                                  
-2	-1.0842604216095497	LinearSVR(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  49%|████▉     | 1250/2550 [17:09<1:16:03,  3.51s/pipeline]                                                                                  
-3	-1.0700377270358012	ElasticNetCV(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  49%|████▉     | 1250/2550 [17:09<1:16:03,  3.51s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:  49%|████▉     | 1250/2550 [17:11<1:16:03,  3.51s/pipeline]Optimization Progress:  49%|████▉     | 1250/2550 [17:11<54:29,  2.52s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  49%|████▉     | 1250/2550 [17:11<54:29,  2.52s/pipeline]Optimization Progress:  49%|████▉     | 1251/2550 [17:13<49:06,  2.27s/pipeline]Optimization Progress:  49%|████▉     | 1252/2550 [17:27<2:05:19,  5.79s/pipeline]Optimization Progress:  50%|████▉     | 1268/2550 [17:40<1:31:50,  4.30s/pipeline]Optimization Progress:  50%|█████     | 1284/2550 [17:55<1:09:32,  3.30s/pipeline]Optimization Progress:  51%|█████     | 1300/2550 [17:56<48:07,  2.31s/pipeline]                                                                                  
Generation 25 - Current Pareto front scores:
Optimization Progress:  51%|█████     | 1300/2550 [17:56<48:07,  2.31s/pipeline]                                                                                
-1	-1.0850899939437213	LinearSVR(input_matrix, LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  51%|█████     | 1300/2550 [17:56<48:07,  2.31s/pipeline]                                                                                
-2	-1.0842604216095497	LinearSVR(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  51%|█████     | 1300/2550 [17:56<48:07,  2.31s/pipeline]                                                                                
-3	-1.0700377270358012	ElasticNetCV(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  51%|█████     | 1300/2550 [17:56<48:07,  2.31s/pipeline]Optimization Progress:  51%|█████     | 1300/2550 [18:09<48:07,  2.31s/pipeline]Optimization Progress:  51%|█████     | 1301/2550 [18:30<4:11:04, 12.06s/pipeline]Optimization Progress:  52%|█████▏    | 1317/2550 [18:45<2:59:01,  8.71s/pipeline]Optimization Progress:  52%|█████▏    | 1333/2550 [19:12<2:14:09,  6.61s/pipeline]Optimization Progress:  53%|█████▎    | 1349/2550 [19:23<1:36:39,  4.83s/pipeline]                                                                                  
Generation 26 - Current Pareto front scores:
Optimization Progress:  53%|█████▎    | 1350/2550 [19:23<1:36:34,  4.83s/pipeline]                                                                                  
-1	-1.0850899939437213	LinearSVR(input_matrix, LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  53%|█████▎    | 1350/2550 [19:23<1:36:34,  4.83s/pipeline]                                                                                  
-2	-1.0842604216095497	LinearSVR(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  53%|█████▎    | 1350/2550 [19:23<1:36:34,  4.83s/pipeline]                                                                                  
-3	-1.0700377270358012	ElasticNetCV(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  53%|█████▎    | 1350/2550 [19:23<1:36:34,  4.83s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:  53%|█████▎    | 1350/2550 [19:28<1:36:34,  4.83s/pipeline]Optimization Progress:  53%|█████▎    | 1350/2550 [19:28<1:37:04,  4.85s/pipeline]Optimization Progress:  53%|█████▎    | 1351/2550 [19:31<1:28:26,  4.43s/pipeline]Optimization Progress:  53%|█████▎    | 1352/2550 [19:51<2:58:43,  8.95s/pipeline]Optimization Progress:  54%|█████▎    | 1368/2550 [20:18<2:13:24,  6.77s/pipeline]Optimization Progress:  54%|█████▍    | 1384/2550 [20:32<1:37:23,  5.01s/pipeline]Optimization Progress:  55%|█████▍    | 1400/2550 [20:35<1:08:09,  3.56s/pipeline]                                                                                  
Generation 27 - Current Pareto front scores:
Optimization Progress:  55%|█████▍    | 1400/2550 [20:35<1:08:09,  3.56s/pipeline]                                                                                  
-1	-1.0850899939437213	LinearSVR(input_matrix, LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  55%|█████▍    | 1400/2550 [20:35<1:08:09,  3.56s/pipeline]                                                                                  
-2	-1.0842604216095497	LinearSVR(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  55%|█████▍    | 1400/2550 [20:35<1:08:09,  3.56s/pipeline]                                                                                  
-3	-1.0700377270358012	ElasticNetCV(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  55%|█████▍    | 1400/2550 [20:35<1:08:09,  3.56s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.
Optimization Progress:  55%|█████▍    | 1400/2550 [20:37<1:08:09,  3.56s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  55%|█████▍    | 1400/2550 [20:39<1:08:09,  3.56s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  55%|█████▍    | 1400/2550 [20:41<1:08:09,  3.56s/pipeline]Optimization Progress:  55%|█████▍    | 1401/2550 [21:03<3:28:16, 10.88s/pipeline]Optimization Progress:  56%|█████▌    | 1417/2550 [21:25<2:31:36,  8.03s/pipeline]Optimization Progress:  56%|█████▌    | 1433/2550 [21:41<1:50:14,  5.92s/pipeline]Optimization Progress:  57%|█████▋    | 1449/2550 [21:43<1:16:56,  4.19s/pipeline]                                                                                  
Generation 28 - Current Pareto front scores:
Optimization Progress:  57%|█████▋    | 1450/2550 [21:43<1:16:52,  4.19s/pipeline]                                                                                  
-1	-1.0850899939437213	LinearSVR(input_matrix, LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  57%|█████▋    | 1450/2550 [21:43<1:16:52,  4.19s/pipeline]                                                                                  
-2	-1.0842604216095497	LinearSVR(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  57%|█████▋    | 1450/2550 [21:43<1:16:52,  4.19s/pipeline]                                                                                  
-3	-1.0700377270358012	ElasticNetCV(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  57%|█████▋    | 1450/2550 [21:43<1:16:52,  4.19s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress:  57%|█████▋    | 1450/2550 [21:47<1:16:52,  4.19s/pipeline]Optimization Progress:  57%|█████▋    | 1450/2550 [21:47<1:11:07,  3.88s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.
Optimization Progress:  57%|█████▋    | 1450/2550 [21:49<1:11:07,  3.88s/pipeline]                                                                                  Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  57%|█████▋    | 1450/2550 [21:52<1:11:07,  3.88s/pipeline]Optimization Progress:  57%|█████▋    | 1451/2550 [21:52<1:18:30,  4.29s/pipeline]Optimization Progress:  57%|█████▋    | 1452/2550 [22:06<2:12:05,  7.22s/pipeline]Optimization Progress:  58%|█████▊    | 1468/2550 [22:30<1:39:08,  5.50s/pipeline]Optimization Progress:  58%|█████▊    | 1484/2550 [22:52<1:15:54,  4.27s/pipeline]Optimization Progress:  59%|█████▉    | 1500/2550 [22:52<52:24,  2.99s/pipeline]                                                                                  
Generation 29 - Current Pareto front scores:
Optimization Progress:  59%|█████▉    | 1500/2550 [22:52<52:24,  2.99s/pipeline]                                                                                
-1	-1.0850899939437213	LinearSVR(input_matrix, LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  59%|█████▉    | 1500/2550 [22:52<52:24,  2.99s/pipeline]                                                                                
-2	-1.0842604216095497	LinearSVR(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  59%|█████▉    | 1500/2550 [22:52<52:24,  2.99s/pipeline]                                                                                
-3	-1.0700377270358012	ElasticNetCV(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  59%|█████▉    | 1500/2550 [22:52<52:24,  2.99s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress:  59%|█████▉    | 1500/2550 [23:00<52:24,  2.99s/pipeline]Optimization Progress:  59%|█████▉    | 1500/2550 [23:09<52:24,  2.99s/pipeline]Optimization Progress:  59%|█████▉    | 1501/2550 [23:24<3:21:30, 11.53s/pipeline]Optimization Progress:  59%|█████▉    | 1517/2550 [23:51<2:27:34,  8.57s/pipeline]Optimization Progress:  60%|██████    | 1533/2550 [24:09<1:47:28,  6.34s/pipeline]Optimization Progress:  61%|██████    | 1549/2550 [24:11<1:14:48,  4.48s/pipeline]                                                                                  
Generation 30 - Current Pareto front scores:
Optimization Progress:  61%|██████    | 1550/2550 [24:11<1:14:43,  4.48s/pipeline]                                                                                  
-1	-1.0850899939437213	LinearSVR(input_matrix, LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  61%|██████    | 1550/2550 [24:11<1:14:43,  4.48s/pipeline]                                                                                  
-2	-1.0842604216095497	LinearSVR(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  61%|██████    | 1550/2550 [24:11<1:14:43,  4.48s/pipeline]                                                                                  
-3	-1.0700377270358012	ElasticNetCV(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  61%|██████    | 1550/2550 [24:11<1:14:43,  4.48s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.
Optimization Progress:  61%|██████    | 1550/2550 [24:15<1:14:43,  4.48s/pipeline]Optimization Progress:  61%|██████    | 1550/2550 [24:15<1:10:38,  4.24s/pipeline]Optimization Progress:  61%|██████    | 1551/2550 [24:36<2:33:04,  9.19s/pipeline]Optimization Progress:  61%|██████▏   | 1567/2550 [24:49<1:49:34,  6.69s/pipeline]Optimization Progress:  62%|██████▏   | 1583/2550 [24:58<1:18:11,  4.85s/pipeline]Optimization Progress:  63%|██████▎   | 1599/2550 [25:09<57:06,  3.60s/pipeline]                                                                                  
Generation 31 - Current Pareto front scores:
Optimization Progress:  63%|██████▎   | 1600/2550 [25:09<57:03,  3.60s/pipeline]                                                                                
-1	-1.0850899939437213	LinearSVR(input_matrix, LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  63%|██████▎   | 1600/2550 [25:09<57:03,  3.60s/pipeline]                                                                                
-2	-1.0842604216095497	LinearSVR(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  63%|██████▎   | 1600/2550 [25:09<57:03,  3.60s/pipeline]                                                                                
-3	-1.0700377270358012	ElasticNetCV(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  63%|██████▎   | 1600/2550 [25:09<57:03,  3.60s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:  63%|██████▎   | 1600/2550 [25:16<57:03,  3.60s/pipeline]Optimization Progress:  63%|██████▎   | 1600/2550 [25:16<1:10:01,  4.42s/pipeline]Optimization Progress:  63%|██████▎   | 1601/2550 [25:17<55:18,  3.50s/pipeline]  Optimization Progress:  63%|██████▎   | 1602/2550 [25:41<2:32:07,  9.63s/pipeline]Optimization Progress:  63%|██████▎   | 1618/2550 [25:58<1:49:42,  7.06s/pipeline]Optimization Progress:  64%|██████▍   | 1634/2550 [26:19<1:21:31,  5.34s/pipeline]Optimization Progress:  65%|██████▍   | 1650/2550 [26:27<58:19,  3.89s/pipeline]                                                                                  
Generation 32 - Current Pareto front scores:
Optimization Progress:  65%|██████▍   | 1650/2550 [26:27<58:19,  3.89s/pipeline]                                                                                
-1	-1.0850899939437213	LinearSVR(input_matrix, LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  65%|██████▍   | 1650/2550 [26:27<58:19,  3.89s/pipeline]                                                                                
-2	-1.0842604216095497	LinearSVR(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  65%|██████▍   | 1650/2550 [26:27<58:19,  3.89s/pipeline]                                                                                
-3	-1.0700377270358012	ElasticNetCV(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  65%|██████▍   | 1650/2550 [26:27<58:19,  3.89s/pipeline]Optimization Progress:  65%|██████▍   | 1651/2550 [26:55<2:45:21, 11.04s/pipeline]Optimization Progress:  65%|██████▌   | 1667/2550 [27:12<1:58:29,  8.05s/pipeline]Optimization Progress:  66%|██████▌   | 1683/2550 [27:28<1:25:34,  5.92s/pipeline]Optimization Progress:  67%|██████▋   | 1699/2550 [27:36<1:00:54,  4.29s/pipeline]                                                                                  
Generation 33 - Current Pareto front scores:
Optimization Progress:  67%|██████▋   | 1700/2550 [27:36<1:00:49,  4.29s/pipeline]                                                                                  
-1	-1.0850899939437213	LinearSVR(input_matrix, LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  67%|██████▋   | 1700/2550 [27:36<1:00:49,  4.29s/pipeline]                                                                                  
-2	-1.0842604216095497	LinearSVR(OneHotEncoder(input_matrix, OneHotEncoder__minimum_fraction=0.25, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  67%|██████▋   | 1700/2550 [27:36<1:00:49,  4.29s/pipeline]                                                                                  
-3	-1.0700377270358012	ElasticNetCV(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  67%|██████▋   | 1700/2550 [27:36<1:00:49,  4.29s/pipeline]Optimization Progress:  67%|██████▋   | 1700/2550 [27:43<1:12:26,  5.11s/pipeline]Optimization Progress:  67%|██████▋   | 1701/2550 [28:05<2:26:04, 10.32s/pipeline]Optimization Progress:  67%|██████▋   | 1717/2550 [28:25<1:45:32,  7.60s/pipeline]Optimization Progress:  68%|██████▊   | 1733/2550 [28:35<1:15:02,  5.51s/pipeline]Optimization Progress:  69%|██████▊   | 1749/2550 [28:43<53:30,  4.01s/pipeline]                                                                                  
Generation 34 - Current Pareto front scores:
Optimization Progress:  69%|██████▊   | 1750/2550 [28:43<53:26,  4.01s/pipeline]                                                                                
-1	-1.0832991495501172	LinearSVR(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  69%|██████▊   | 1750/2550 [28:43<53:26,  4.01s/pipeline]                                                                                
-3	-1.0700377270358012	ElasticNetCV(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  69%|██████▊   | 1750/2550 [28:43<53:26,  4.01s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:  69%|██████▊   | 1750/2550 [28:50<53:26,  4.01s/pipeline]Optimization Progress:  69%|██████▊   | 1750/2550 [28:50<1:02:43,  4.70s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  69%|██████▊   | 1750/2550 [28:50<1:02:43,  4.70s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:  69%|██████▊   | 1750/2550 [28:51<1:02:43,  4.70s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.
Optimization Progress:  69%|██████▊   | 1750/2550 [28:51<1:02:43,  4.70s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress:  69%|██████▊   | 1750/2550 [28:52<1:02:43,  4.70s/pipeline]Optimization Progress:  69%|██████▊   | 1751/2550 [29:19<2:40:39, 12.06s/pipeline]Optimization Progress:  69%|██████▉   | 1767/2550 [29:42<1:55:52,  8.88s/pipeline]Optimization Progress:  70%|██████▉   | 1783/2550 [30:00<1:23:52,  6.56s/pipeline]Optimization Progress:  71%|███████   | 1799/2550 [30:01<57:32,  4.60s/pipeline]                                                                                  
Generation 35 - Current Pareto front scores:
Optimization Progress:  71%|███████   | 1800/2550 [30:01<57:27,  4.60s/pipeline]                                                                                
-1	-1.0832991495501172	LinearSVR(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  71%|███████   | 1800/2550 [30:01<57:27,  4.60s/pipeline]                                                                                
-3	-1.0700377270358012	ElasticNetCV(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  71%|███████   | 1800/2550 [30:01<57:27,  4.60s/pipeline]                                                                                
-4	-1.0672179337963286	ElasticNetCV(CombineDFs(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  71%|███████   | 1800/2550 [30:01<57:27,  4.60s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:  71%|███████   | 1800/2550 [30:04<57:27,  4.60s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.
Optimization Progress:  71%|███████   | 1800/2550 [30:05<57:27,  4.60s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 98.
Optimization Progress:  71%|███████   | 1800/2550 [30:08<57:27,  4.60s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress:  71%|███████   | 1800/2550 [30:08<57:27,  4.60s/pipeline]Optimization Progress:  71%|███████   | 1801/2550 [30:19<57:23,  4.60s/pipeline]Optimization Progress:  71%|███████   | 1802/2550 [30:29<1:15:02,  6.02s/pipeline]Optimization Progress:  71%|███████▏  | 1818/2550 [30:53<57:00,  4.67s/pipeline]  Optimization Progress:  72%|███████▏  | 1834/2550 [31:31<47:32,  3.98s/pipeline]Optimization Progress:  73%|███████▎  | 1850/2550 [31:46<35:41,  3.06s/pipeline]                                                                                
Generation 36 - Current Pareto front scores:
Optimization Progress:  73%|███████▎  | 1850/2550 [31:46<35:41,  3.06s/pipeline]                                                                                
-1	-1.0832991495501172	LinearSVR(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  73%|███████▎  | 1850/2550 [31:46<35:41,  3.06s/pipeline]                                                                                
-3	-1.0700377270358012	ElasticNetCV(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  73%|███████▎  | 1850/2550 [31:46<35:41,  3.06s/pipeline]                                                                                
-4	-1.0672179337963286	ElasticNetCV(CombineDFs(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  73%|███████▎  | 1850/2550 [31:46<35:41,  3.06s/pipeline]Optimization Progress:  73%|███████▎  | 1851/2550 [31:53<50:06,  4.30s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  73%|███████▎  | 1851/2550 [31:53<50:06,  4.30s/pipeline]Optimization Progress:  73%|███████▎  | 1853/2550 [32:12<1:08:34,  5.90s/pipeline]Optimization Progress:  73%|███████▎  | 1869/2550 [32:32<51:03,  4.50s/pipeline]  Optimization Progress:  74%|███████▍  | 1885/2550 [32:46<37:49,  3.41s/pipeline]                                                                                
Generation 37 - Current Pareto front scores:
Optimization Progress:  75%|███████▍  | 1900/2550 [32:46<36:57,  3.41s/pipeline]                                                                                
-1	-1.0832991495501172	LinearSVR(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  75%|███████▍  | 1900/2550 [32:46<36:57,  3.41s/pipeline]                                                                                
-3	-1.0700377270358012	ElasticNetCV(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  75%|███████▍  | 1900/2550 [32:46<36:57,  3.41s/pipeline]                                                                                
-4	-1.0672179337963286	ElasticNetCV(CombineDFs(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  75%|███████▍  | 1900/2550 [32:46<36:57,  3.41s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  75%|███████▍  | 1900/2550 [32:53<36:57,  3.41s/pipeline]Optimization Progress:  75%|███████▍  | 1900/2550 [32:53<27:29,  2.54s/pipeline]Optimization Progress:  75%|███████▍  | 1901/2550 [32:56<29:41,  2.74s/pipeline]                                                                                Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:  75%|███████▍  | 1901/2550 [32:56<29:41,  2.74s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  75%|███████▍  | 1902/2550 [32:56<29:38,  2.74s/pipeline]Optimization Progress:  75%|███████▍  | 1904/2550 [33:25<51:34,  4.79s/pipeline]Optimization Progress:  75%|███████▌  | 1920/2550 [33:50<40:02,  3.81s/pipeline]Optimization Progress:  76%|███████▌  | 1936/2550 [34:16<32:25,  3.17s/pipeline]                                                                                
Generation 38 - Current Pareto front scores:
Optimization Progress:  76%|███████▋  | 1950/2550 [34:16<31:40,  3.17s/pipeline]                                                                                
-1	-1.0832991495501172	LinearSVR(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  76%|███████▋  | 1950/2550 [34:16<31:40,  3.17s/pipeline]                                                                                
-2	-1.0832767688784881	LinearSVR(CombineDFs(FastICA(input_matrix, FastICA__tol=0.2), input_matrix), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05)
Optimization Progress:  76%|███████▋  | 1950/2550 [34:16<31:40,  3.17s/pipeline]                                                                                
-3	-1.0700377270358012	ElasticNetCV(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  76%|███████▋  | 1950/2550 [34:16<31:40,  3.17s/pipeline]                                                                                
-4	-1.0672179337963286	ElasticNetCV(CombineDFs(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  76%|███████▋  | 1950/2550 [34:16<31:40,  3.17s/pipeline]Optimization Progress:  76%|███████▋  | 1950/2550 [34:23<23:43,  2.37s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  76%|███████▋  | 1950/2550 [34:23<23:43,  2.37s/pipeline]Optimization Progress:  77%|███████▋  | 1952/2550 [34:39<40:04,  4.02s/pipeline]Optimization Progress:  77%|███████▋  | 1968/2550 [35:01<31:19,  3.23s/pipeline]Optimization Progress:  78%|███████▊  | 1984/2550 [35:20<24:43,  2.62s/pipeline]Optimization Progress:  78%|███████▊  | 2000/2550 [35:22<17:00,  1.86s/pipeline]                                                                                
Generation 39 - Current Pareto front scores:
Optimization Progress:  78%|███████▊  | 2000/2550 [35:22<17:00,  1.86s/pipeline]                                                                                
-1	-1.0832991495501172	LinearSVR(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  78%|███████▊  | 2000/2550 [35:22<17:00,  1.86s/pipeline]                                                                                
-2	-1.0832767688784881	LinearSVR(CombineDFs(FastICA(input_matrix, FastICA__tol=0.2), input_matrix), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05)
Optimization Progress:  78%|███████▊  | 2000/2550 [35:22<17:00,  1.86s/pipeline]                                                                                
-3	-1.0700377270358012	ElasticNetCV(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  78%|███████▊  | 2000/2550 [35:22<17:00,  1.86s/pipeline]                                                                                
-4	-1.0672179337963286	ElasticNetCV(CombineDFs(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  78%|███████▊  | 2000/2550 [35:22<17:00,  1.86s/pipeline]Optimization Progress:  79%|███████▊  | 2002/2550 [35:35<30:47,  3.37s/pipeline]Optimization Progress:  79%|███████▊  | 2003/2550 [36:12<2:00:42, 13.24s/pipeline]Optimization Progress:  79%|███████▉  | 2019/2550 [36:36<1:26:07,  9.73s/pipeline]Optimization Progress:  80%|███████▉  | 2035/2550 [37:05<1:03:08,  7.36s/pipeline]                                                                                  
Generation 40 - Current Pareto front scores:
Optimization Progress:  80%|████████  | 2050/2550 [37:05<1:01:17,  7.36s/pipeline]                                                                                  
-1	-1.0832991495501172	LinearSVR(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  80%|████████  | 2050/2550 [37:05<1:01:17,  7.36s/pipeline]                                                                                  
-2	-1.0832767688784881	LinearSVR(CombineDFs(FastICA(input_matrix, FastICA__tol=0.2), input_matrix), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05)
Optimization Progress:  80%|████████  | 2050/2550 [37:05<1:01:17,  7.36s/pipeline]                                                                                  
-3	-1.0700377270358012	ElasticNetCV(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  80%|████████  | 2050/2550 [37:05<1:01:17,  7.36s/pipeline]                                                                                  
-4	-1.0672179337963286	ElasticNetCV(CombineDFs(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  80%|████████  | 2050/2550 [37:05<1:01:17,  7.36s/pipeline]                                                                                  _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  80%|████████  | 2050/2550 [37:11<1:01:17,  7.36s/pipeline]Optimization Progress:  80%|████████  | 2050/2550 [37:11<43:48,  5.26s/pipeline]  Optimization Progress:  80%|████████  | 2051/2550 [37:16<44:27,  5.35s/pipeline]Optimization Progress:  80%|████████  | 2052/2550 [37:48<1:49:16, 13.17s/pipeline]Optimization Progress:  81%|████████  | 2068/2550 [38:04<1:16:30,  9.52s/pipeline]Optimization Progress:  82%|████████▏ | 2084/2550 [38:32<55:50,  7.19s/pipeline]                                                                                  
Generation 41 - Current Pareto front scores:
Optimization Progress:  82%|████████▏ | 2100/2550 [38:32<53:55,  7.19s/pipeline]                                                                                
-1	-1.0832991495501172	LinearSVR(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  82%|████████▏ | 2100/2550 [38:32<53:55,  7.19s/pipeline]                                                                                
-2	-1.0832767688784881	LinearSVR(CombineDFs(FastICA(input_matrix, FastICA__tol=0.2), input_matrix), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05)
Optimization Progress:  82%|████████▏ | 2100/2550 [38:32<53:55,  7.19s/pipeline]                                                                                
-3	-1.0700377270358012	ElasticNetCV(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  82%|████████▏ | 2100/2550 [38:32<53:55,  7.19s/pipeline]                                                                                
-4	-1.0672179337963286	ElasticNetCV(CombineDFs(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  82%|████████▏ | 2100/2550 [38:32<53:55,  7.19s/pipeline]Optimization Progress:  82%|████████▏ | 2100/2550 [38:42<39:10,  5.22s/pipeline]Optimization Progress:  82%|████████▏ | 2103/2550 [39:04<43:35,  5.85s/pipeline]Optimization Progress:  83%|████████▎ | 2119/2550 [39:37<33:52,  4.71s/pipeline]Optimization Progress:  84%|████████▎ | 2135/2550 [40:01<25:53,  3.74s/pipeline]                                                                                
Generation 42 - Current Pareto front scores:
Optimization Progress:  84%|████████▍ | 2150/2550 [40:01<24:57,  3.74s/pipeline]                                                                                
-1	-1.0832991495501172	LinearSVR(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  84%|████████▍ | 2150/2550 [40:01<24:57,  3.74s/pipeline]                                                                                
-2	-1.0832767688784881	LinearSVR(CombineDFs(FastICA(input_matrix, FastICA__tol=0.2), input_matrix), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05)
Optimization Progress:  84%|████████▍ | 2150/2550 [40:01<24:57,  3.74s/pipeline]                                                                                
-3	-1.0700377270358012	ElasticNetCV(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  84%|████████▍ | 2150/2550 [40:01<24:57,  3.74s/pipeline]                                                                                
-4	-1.0672179337963286	ElasticNetCV(CombineDFs(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  84%|████████▍ | 2150/2550 [40:01<24:57,  3.74s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.
Optimization Progress:  84%|████████▍ | 2150/2550 [40:06<24:57,  3.74s/pipeline]Optimization Progress:  84%|████████▍ | 2150/2550 [40:06<18:10,  2.73s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  84%|████████▍ | 2150/2550 [40:06<18:10,  2.73s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:  84%|████████▍ | 2150/2550 [40:12<18:10,  2.73s/pipeline]Optimization Progress:  84%|████████▍ | 2151/2550 [40:53<1:46:28, 16.01s/pipeline]Optimization Progress:  85%|████████▍ | 2167/2550 [41:15<1:14:09, 11.62s/pipeline]Optimization Progress:  86%|████████▌ | 2183/2550 [41:39<52:26,  8.57s/pipeline]  Optimization Progress:  86%|████████▌ | 2199/2550 [41:54<36:46,  6.28s/pipeline]                                                                                
Generation 43 - Current Pareto front scores:
Optimization Progress:  86%|████████▋ | 2200/2550 [41:54<36:39,  6.28s/pipeline]                                                                                
-1	-1.0832991495501172	LinearSVR(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  86%|████████▋ | 2200/2550 [41:54<36:39,  6.28s/pipeline]                                                                                
-2	-1.0832767688784881	LinearSVR(CombineDFs(FastICA(input_matrix, FastICA__tol=0.2), input_matrix), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05)
Optimization Progress:  86%|████████▋ | 2200/2550 [41:54<36:39,  6.28s/pipeline]                                                                                
-3	-1.0700377270358012	ElasticNetCV(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  86%|████████▋ | 2200/2550 [41:54<36:39,  6.28s/pipeline]                                                                                
-4	-1.0672179337963286	ElasticNetCV(CombineDFs(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  86%|████████▋ | 2200/2550 [41:54<36:39,  6.28s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  86%|████████▋ | 2200/2550 [41:58<36:39,  6.28s/pipeline]Optimization Progress:  86%|████████▋ | 2200/2550 [41:58<33:02,  5.67s/pipeline]Optimization Progress:  86%|████████▋ | 2201/2550 [42:35<1:27:08, 14.98s/pipeline]Optimization Progress:  87%|████████▋ | 2217/2550 [42:58<1:00:35, 10.92s/pipeline]Optimization Progress:  88%|████████▊ | 2233/2550 [43:35<44:06,  8.35s/pipeline]  Optimization Progress:  88%|████████▊ | 2249/2550 [43:43<30:04,  5.99s/pipeline]                                                                                
Generation 44 - Current Pareto front scores:
Optimization Progress:  88%|████████▊ | 2250/2550 [43:43<29:58,  5.99s/pipeline]                                                                                
-1	-1.0832991495501172	LinearSVR(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  88%|████████▊ | 2250/2550 [43:43<29:58,  5.99s/pipeline]                                                                                
-2	-1.0832767688784881	LinearSVR(CombineDFs(FastICA(input_matrix, FastICA__tol=0.2), input_matrix), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05)
Optimization Progress:  88%|████████▊ | 2250/2550 [43:43<29:58,  5.99s/pipeline]                                                                                
-3	-1.0700377270358012	ElasticNetCV(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  88%|████████▊ | 2250/2550 [43:43<29:58,  5.99s/pipeline]                                                                                
-4	-1.0672179337963286	ElasticNetCV(CombineDFs(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  88%|████████▊ | 2250/2550 [43:43<29:58,  5.99s/pipeline]                                                                                
-5	-1.0670666014555976	ElasticNetCV(CombineDFs(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), XGBRegressor(ZeroCount(input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  88%|████████▊ | 2250/2550 [43:43<29:58,  5.99s/pipeline]                                                                                
-6	-1.0665676793324481	ElasticNetCV(CombineDFs(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), CombineDFs(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), Binarizer(input_matrix, Binarizer__threshold=0.4))), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  88%|████████▊ | 2250/2550 [43:43<29:58,  5.99s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  88%|████████▊ | 2250/2550 [43:51<29:58,  5.99s/pipeline]Optimization Progress:  88%|████████▊ | 2250/2550 [43:51<32:18,  6.46s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
Optimization Progress:  88%|████████▊ | 2250/2550 [43:53<32:18,  6.46s/pipeline]Optimization Progress:  88%|████████▊ | 2251/2550 [43:57<31:32,  6.33s/pipeline]Optimization Progress:  88%|████████▊ | 2252/2550 [44:34<1:17:58, 15.70s/pipeline]Optimization Progress:  89%|████████▉ | 2268/2550 [45:00<53:55, 11.47s/pipeline]  Optimization Progress:  90%|████████▉ | 2284/2550 [45:26<37:46,  8.52s/pipeline]Optimization Progress:  90%|█████████ | 2300/2550 [45:34<25:28,  6.11s/pipeline]                                                                                
Generation 45 - Current Pareto front scores:
Optimization Progress:  90%|█████████ | 2300/2550 [45:34<25:28,  6.11s/pipeline]                                                                                
-1	-1.0832991495501172	LinearSVR(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  90%|█████████ | 2300/2550 [45:34<25:28,  6.11s/pipeline]                                                                                
-2	-1.0832767688784881	LinearSVR(CombineDFs(FastICA(input_matrix, FastICA__tol=0.2), input_matrix), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05)
Optimization Progress:  90%|█████████ | 2300/2550 [45:34<25:28,  6.11s/pipeline]                                                                                
-3	-1.0700377270358012	ElasticNetCV(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  90%|█████████ | 2300/2550 [45:34<25:28,  6.11s/pipeline]                                                                                
-4	-1.0672179337963286	ElasticNetCV(CombineDFs(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  90%|█████████ | 2300/2550 [45:34<25:28,  6.11s/pipeline]                                                                                
-5	-1.0670666014555976	ElasticNetCV(CombineDFs(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), XGBRegressor(ZeroCount(input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  90%|█████████ | 2300/2550 [45:34<25:28,  6.11s/pipeline]                                                                                
-6	-1.0665676793324481	ElasticNetCV(CombineDFs(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), CombineDFs(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), Binarizer(input_matrix, Binarizer__threshold=0.4))), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  90%|█████████ | 2300/2550 [45:34<25:28,  6.11s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress:  90%|█████████ | 2300/2550 [45:44<25:28,  6.11s/pipeline]Optimization Progress:  90%|█████████ | 2301/2550 [45:48<34:14,  8.25s/pipeline]Optimization Progress:  90%|█████████ | 2302/2550 [46:14<57:13, 13.85s/pipeline]Optimization Progress:  91%|█████████ | 2318/2550 [46:51<40:09, 10.38s/pipeline]Optimization Progress:  92%|█████████▏| 2334/2550 [47:24<28:23,  7.89s/pipeline]Optimization Progress:  92%|█████████▏| 2350/2550 [47:39<19:20,  5.80s/pipeline]                                                                                
Generation 46 - Current Pareto front scores:
Optimization Progress:  92%|█████████▏| 2350/2550 [47:39<19:20,  5.80s/pipeline]                                                                                
-1	-1.0832991495501172	LinearSVR(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  92%|█████████▏| 2350/2550 [47:39<19:20,  5.80s/pipeline]                                                                                
-2	-1.0832767688784881	LinearSVR(CombineDFs(FastICA(input_matrix, FastICA__tol=0.2), input_matrix), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05)
Optimization Progress:  92%|█████████▏| 2350/2550 [47:39<19:20,  5.80s/pipeline]                                                                                
-3	-1.0700377270358012	ElasticNetCV(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  92%|█████████▏| 2350/2550 [47:39<19:20,  5.80s/pipeline]                                                                                
-4	-1.0672179337963286	ElasticNetCV(CombineDFs(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  92%|█████████▏| 2350/2550 [47:39<19:20,  5.80s/pipeline]                                                                                
-5	-1.0670666014555976	ElasticNetCV(CombineDFs(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), XGBRegressor(ZeroCount(input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  92%|█████████▏| 2350/2550 [47:39<19:20,  5.80s/pipeline]                                                                                
-6	-1.0665676793324481	ElasticNetCV(CombineDFs(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), CombineDFs(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), Binarizer(input_matrix, Binarizer__threshold=0.4))), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  92%|█████████▏| 2350/2550 [47:39<19:20,  5.80s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..
Optimization Progress:  92%|█████████▏| 2350/2550 [47:42<19:20,  5.80s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  92%|█████████▏| 2350/2550 [47:46<19:20,  5.80s/pipeline]Optimization Progress:  92%|█████████▏| 2351/2550 [48:15<49:14, 14.85s/pipeline]Optimization Progress:  93%|█████████▎| 2367/2550 [48:40<33:06, 10.86s/pipeline]Optimization Progress:  93%|█████████▎| 2383/2550 [49:25<23:30,  8.45s/pipeline]Optimization Progress:  94%|█████████▍| 2399/2550 [49:34<15:16,  6.07s/pipeline]                                                                                
Generation 47 - Current Pareto front scores:
Optimization Progress:  94%|█████████▍| 2400/2550 [49:34<15:10,  6.07s/pipeline]                                                                                
-1	-1.0832991495501172	LinearSVR(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  94%|█████████▍| 2400/2550 [49:34<15:10,  6.07s/pipeline]                                                                                
-2	-1.0832767688784881	LinearSVR(CombineDFs(FastICA(input_matrix, FastICA__tol=0.2), input_matrix), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05)
Optimization Progress:  94%|█████████▍| 2400/2550 [49:34<15:10,  6.07s/pipeline]                                                                                
-3	-1.0700377270358012	ElasticNetCV(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  94%|█████████▍| 2400/2550 [49:34<15:10,  6.07s/pipeline]                                                                                
-4	-1.0672179337963286	ElasticNetCV(CombineDFs(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  94%|█████████▍| 2400/2550 [49:34<15:10,  6.07s/pipeline]                                                                                
-5	-1.0670666014555976	ElasticNetCV(CombineDFs(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), XGBRegressor(ZeroCount(input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  94%|█████████▍| 2400/2550 [49:34<15:10,  6.07s/pipeline]                                                                                
-6	-1.0665493145737346	ElasticNetCV(CombineDFs(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), CombineDFs(CombineDFs(LassoLarsCV(input_matrix, LassoLarsCV__normalize=True), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), Binarizer(input_matrix, Binarizer__threshold=0.4))), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  94%|█████████▍| 2400/2550 [49:34<15:10,  6.07s/pipeline]                                                                                _pre_test decorator: _mate_operator: num_test=0 array must not contain infs or NaNs.
Optimization Progress:  94%|█████████▍| 2400/2550 [49:40<15:10,  6.07s/pipeline]Optimization Progress:  94%|█████████▍| 2400/2550 [49:40<15:36,  6.25s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..
Optimization Progress:  94%|█████████▍| 2400/2550 [49:42<15:36,  6.25s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.
Optimization Progress:  94%|█████████▍| 2400/2550 [49:43<15:36,  6.25s/pipeline]Optimization Progress:  94%|█████████▍| 2401/2550 [50:35<51:19, 20.66s/pipeline]Optimization Progress:  95%|█████████▍| 2417/2550 [51:14<33:41, 15.20s/pipeline]Optimization Progress:  95%|█████████▌| 2433/2550 [51:56<22:16, 11.42s/pipeline]Optimization Progress:  96%|█████████▌| 2449/2550 [52:11<13:56,  8.29s/pipeline]                                                                                
Generation 48 - Current Pareto front scores:
Optimization Progress:  96%|█████████▌| 2450/2550 [52:11<13:48,  8.29s/pipeline]                                                                                
-1	-1.0832991495501172	LinearSVR(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  96%|█████████▌| 2450/2550 [52:11<13:48,  8.29s/pipeline]                                                                                
-2	-1.0832767688784881	LinearSVR(CombineDFs(FastICA(input_matrix, FastICA__tol=0.2), input_matrix), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05)
Optimization Progress:  96%|█████████▌| 2450/2550 [52:11<13:48,  8.29s/pipeline]                                                                                
-3	-1.0700377270358012	ElasticNetCV(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  96%|█████████▌| 2450/2550 [52:11<13:48,  8.29s/pipeline]                                                                                
-4	-1.0672179337963286	ElasticNetCV(CombineDFs(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  96%|█████████▌| 2450/2550 [52:11<13:48,  8.29s/pipeline]                                                                                
-5	-1.0670666014555976	ElasticNetCV(CombineDFs(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), XGBRegressor(ZeroCount(input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  96%|█████████▌| 2450/2550 [52:11<13:48,  8.29s/pipeline]                                                                                
-6	-1.0665493145737346	ElasticNetCV(CombineDFs(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), CombineDFs(CombineDFs(LassoLarsCV(input_matrix, LassoLarsCV__normalize=True), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), Binarizer(input_matrix, Binarizer__threshold=0.4))), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  96%|█████████▌| 2450/2550 [52:11<13:48,  8.29s/pipeline]Optimization Progress:  96%|█████████▌| 2450/2550 [52:31<19:40, 11.80s/pipeline]                                                                                Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.
Optimization Progress:  96%|█████████▌| 2452/2550 [52:31<19:16, 11.80s/pipeline]Optimization Progress:  96%|█████████▌| 2454/2550 [53:17<18:40, 11.67s/pipeline]Optimization Progress:  97%|█████████▋| 2470/2550 [53:56<11:52,  8.90s/pipeline]Optimization Progress:  97%|█████████▋| 2486/2550 [54:45<07:37,  7.15s/pipeline]                                                                                
Generation 49 - Current Pareto front scores:
Optimization Progress:  98%|█████████▊| 2500/2550 [54:45<05:57,  7.15s/pipeline]                                                                                
-1	-1.0832991495501172	LinearSVR(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress:  98%|█████████▊| 2500/2550 [54:45<05:57,  7.15s/pipeline]                                                                                
-2	-1.0832767688784881	LinearSVR(CombineDFs(FastICA(input_matrix, FastICA__tol=0.2), input_matrix), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05)
Optimization Progress:  98%|█████████▊| 2500/2550 [54:45<05:57,  7.15s/pipeline]                                                                                
-3	-1.067607305705394	ElasticNetCV(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=0.9500000000000001, ElasticNetCV__tol=0.01)
Optimization Progress:  98%|█████████▊| 2500/2550 [54:45<05:57,  7.15s/pipeline]                                                                                
-4	-1.0672179337963286	ElasticNetCV(CombineDFs(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  98%|█████████▊| 2500/2550 [54:45<05:57,  7.15s/pipeline]                                                                                
-5	-1.0670666014555976	ElasticNetCV(CombineDFs(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), XGBRegressor(ZeroCount(input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  98%|█████████▊| 2500/2550 [54:45<05:57,  7.15s/pipeline]                                                                                
-6	-1.0665493145737346	ElasticNetCV(CombineDFs(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), CombineDFs(CombineDFs(LassoLarsCV(input_matrix, LassoLarsCV__normalize=True), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), Binarizer(input_matrix, Binarizer__threshold=0.4))), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress:  98%|█████████▊| 2500/2550 [54:45<05:57,  7.15s/pipeline]                                                                                _pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..
Optimization Progress:  98%|█████████▊| 2500/2550 [54:54<05:57,  7.15s/pipeline]Optimization Progress:  98%|█████████▊| 2500/2550 [54:54<04:20,  5.21s/pipeline]Optimization Progress:  98%|█████████▊| 2501/2550 [55:40<14:13, 17.42s/pipeline]Optimization Progress:  99%|█████████▊| 2517/2550 [56:18<07:05, 12.90s/pipeline]Optimization Progress:  99%|█████████▉| 2533/2550 [57:07<02:49,  9.96s/pipeline]Optimization Progress: 100%|█████████▉| 2549/2550 [57:16<00:07,  7.13s/pipeline]                                                                                
Generation 50 - Current Pareto front scores:
Optimization Progress: 100%|██████████| 2550/2550 [57:16<00:00,  7.13s/pipeline]                                                                                
-1	-1.0832991495501172	LinearSVR(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), LinearSVR__C=20.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.01)
Optimization Progress: 100%|██████████| 2550/2550 [57:16<00:00,  7.13s/pipeline]                                                                                
-2	-1.0832767688784881	LinearSVR(CombineDFs(FastICA(input_matrix, FastICA__tol=0.2), input_matrix), LinearSVR__C=25.0, LinearSVR__dual=True, LinearSVR__epsilon=0.0001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=1e-05)
Optimization Progress: 100%|██████████| 2550/2550 [57:16<00:00,  7.13s/pipeline]                                                                                
-3	-1.067607305705394	ElasticNetCV(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=10, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=0.9500000000000001, ElasticNetCV__tol=0.01)
Optimization Progress: 100%|██████████| 2550/2550 [57:16<00:00,  7.13s/pipeline]                                                                                
-4	-1.0672179337963286	ElasticNetCV(CombineDFs(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress: 100%|██████████| 2550/2550 [57:16<00:00,  7.13s/pipeline]                                                                                
-5	-1.0670666014555976	ElasticNetCV(CombineDFs(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), XGBRegressor(ZeroCount(input_matrix), XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=6, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress: 100%|██████████| 2550/2550 [57:16<00:00,  7.13s/pipeline]                                                                                
-6	-1.0665493145737346	ElasticNetCV(CombineDFs(CombineDFs(CombineDFs(input_matrix, LassoLarsCV(input_matrix, LassoLarsCV__normalize=True)), CombineDFs(CombineDFs(LassoLarsCV(input_matrix, LassoLarsCV__normalize=True), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=15, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), Binarizer(input_matrix, Binarizer__threshold=0.4))), XGBRegressor(input_matrix, XGBRegressor__learning_rate=0.5, XGBRegressor__max_depth=5, XGBRegressor__min_child_weight=17, XGBRegressor__n_estimators=100, XGBRegressor__n_jobs=1, XGBRegressor__objective=reg:squarederror, XGBRegressor__subsample=0.4, XGBRegressor__verbosity=0)), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
Optimization Progress: 100%|██████████| 2550/2550 [57:16<00:00,  7.13s/pipeline]                                                                                MODEL SCORE -1.1248306781433597
[ERROR] [2021-12-13 09:49:32,990:Client-AutoML(1):Wine] Dummy prediction failed with run state StatusType.CRASHED and additional output: {'error': 'Result queue is empty', 'exit_status': "<class 'pynisher.limit_function_call.AnythingException'>", 'subprocess_stdout': '', 'subprocess_stderr': 'Process pynisher function call:\nTraceback (most recent call last):\n  File "/usr/lib/python3.8/multiprocessing/process.py", line 315, in _bootstrap\n    self.run()\n  File "/usr/lib/python3.8/multiprocessing/process.py", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File "/home/bartek/.local/lib/python3.8/site-packages/pynisher/limit_function_call.py", line 133, in subprocess_func\n    return_value = ((func(*args, **kwargs), 0))\n  File "/home/bartek/.local/lib/python3.8/site-packages/autosklearn/evaluation/__init__.py", line 41, in fit_predict_try_except_decorator\n    return ta(queue=queue, **kwargs)\n  File "/home/bartek/.local/lib/python3.8/site-packages/autosklearn/evaluation/train_evaluator.py", line 1361, in eval_cv\n    evaluator = TrainEvaluator(\n  File "/home/bartek/.local/lib/python3.8/site-packages/autosklearn/evaluation/train_evaluator.py", line 182, in __init__\n    super().__init__(\n  File "/home/bartek/.local/lib/python3.8/site-packages/autosklearn/evaluation/abstract_evaluator.py", line 201, in __init__\n    threadpool_limits(limits=1)\n  File "/home/bartek/.local/lib/python3.8/site-packages/threadpoolctl.py", line 354, in __init__\n    super().__init__(ThreadpoolController(), limits=limits, user_api=user_api)\n  File "/home/bartek/.local/lib/python3.8/site-packages/threadpoolctl.py", line 159, in __init__\n    self._set_threadpool_limits()\n  File "/home/bartek/.local/lib/python3.8/site-packages/threadpoolctl.py", line 285, in _set_threadpool_limits\n    lib_controller.set_num_threads(num_threads)\n  File "/home/bartek/.local/lib/python3.8/site-packages/threadpoolctl.py", line 809, in set_num_threads\n    return set_func(num_threads)\nKeyboardInterrupt\n', 'exitcode': 1, 'configuration_origin': 'DUMMY'}.
